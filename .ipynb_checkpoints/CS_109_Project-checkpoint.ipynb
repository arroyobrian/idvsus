{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109 Project - The Court Rules In Favor Of...\n",
    "## Aidi Adnan Brian John (Team AABJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "The purpose of this project is to predict votes of Supreme Court justices using oral argument transcripts. Studies in linguistics and psychology, as well as common sense, dictates that the word choices that people make convey crucial information about their beliefs and intentions with regard to issues. Rather than use precedents or formal analysis of the law to predict Supreme Court decisions, we attempt to extract essential emotional features of oral arguments made by justices and advocates in the court. Using aggregate data from 1946 to present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Oral Argument Transcripts - obtained from http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx. Transcripts are made available on the day of court hearing.\n",
    "Justice Vote Counts/Case Information - obtained from the Supreme Court Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Brian/Adnan can you fill this in with a description of what you did in parser.py/convert.py\n",
    "First, we took the all the PDF files of ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reads in text file, replace path of \"wut.txt\" to relevant txt; only processes one text file currently\n",
    "text_file = open(\"wut.txt\", \"r\")\n",
    "text = text_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a parser to extract the names of the petitioner and respondant attorneys from the first 2 pages of the converted text document. An example of list of petitioner and respondant speakers, taken from the example case in 2014 of Johnson v United States (docket number 13-7120) which shall be henceforth used as the recurring example in this process book, is:\n",
    "\n",
    "Katherine M. Menendez, ESQ., Minneapolis, Minn.; on behalf of Petitioner\n",
    "Michael R. Dreeben, ESQ., Deputy Solicitor General, Department of Justice, Washington D.C.; on behalf of Respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_petitioners_and_respondents(text):\n",
    "    '''\n",
    "    This function takes in input text file as string and outputs 2 lists of speakers speaking for petitioners and\n",
    "    respondents sides.\n",
    "    '''\n",
    "    #get portion of transcript between APPEARANCES and CONTENTS that specifies speakers for petitioners/respondents\n",
    "    start = text.find('APPEARANCES:') + len('APPEARNACES')\n",
    "    end = text.find('C O N T E N T S')\n",
    "    speakers_text = text[start:end]\n",
    "    split_speakers_text = re.split('\\.[ ]*\\n', speakers_text)\n",
    "    #for each speaker, get name (capitalized) and side (Pet/Res) he/she is speaking for\n",
    "    pet_speakers, res_speakers, other_speakers = [], [], []\n",
    "    for speaker in split_speakers_text:\n",
    "        name = speaker.strip().split(',')[0]\n",
    "        #search for first index of capitalized word (which will be start of speaker name)\n",
    "        start = 0\n",
    "        for idx, char in enumerate(name):\n",
    "            if str.isupper(char):\n",
    "                start = idx\n",
    "                break\n",
    "        #actual name to be appended to correct list\n",
    "        name = name[start:]\n",
    "        #print name\n",
    "        \n",
    "        #if words Petition, Plaintiff, etc occur in speaker blurb, speaker belongs to Pet\n",
    "        if any(x in speaker for x in ['etition' , 'ppellant', 'emand', 'evers', 'laintiff']):\n",
    "            pet_speakers.append(name)\n",
    "        #otherwise if words Respondent, Defendant, etc occur, speaker belongs to Res\n",
    "        elif any(x in speaker for x in ['espond' , 'ppellee', 'efendant']):\n",
    "            res_speakers.append(name)\n",
    "        #otherwise if neither side is specified in blurb, speaking belongs to Other\n",
    "        elif 'neither' in speaker:\n",
    "            other_speakers.append(name)\n",
    "    return pet_speakers, res_speakers, other_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MR. H. BARTOW FARR'],\n",
       " ['MR. ROY L. REARDON', 'MS. BARBARA D. UNDERWOOD'],\n",
       " [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, for wut.txt, there's 1 petitioner and 2 respondents\n",
    "pet_speakers, res_speakers, other_speakers = get_petitioners_and_respondents(text)\n",
    "pet_speakers, res_speakers, other_speakers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general flow of court proceedings is that the Petitioner attornies make their oral argument, followed by the Respondent attornies, before we hear the rebuttal argument of the Petitioners again. Throughout all proceedings, Justices are free to interject with questions and statements of their own. The below function extracts the main argument portion of the oral transcripts, which is the meat of the proceedings that we are interested in conducting analysis on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_argument_portion(text):\n",
    "    '''\n",
    "    This function gets just the argument portion of the text.\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('P R O C E E D')\n",
    "    end = text.rfind('Whereupon')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P R O C E E D I N G S\\n\\n2\\n\\n[10:13 a.m.]\\n\\n3\\n4\\n\\nCHIEF JUSTICE REHNQUIST:\\n\\nWe'll hear argument on\\n\\nNumber 00-24, PGA Tour, Inc. vs. Casey Martin.\\n\\n5\\n\\nORAL ARGUMENT OF H. BARTOW FARR, III\\n\\n6\\n\\nON BEHALF OF THE PETITIONER\\n\\n7\\n\\nMR. FARR:\\n\\nMr. Farr?\\n\\nMr. Chief Justice and may it please\\n\\n8\\n\\nthe Court:\\n\\nThe Ninth Circuit in our view made two\\n\\n9\\n\\ncritical mistakes in applying the Disabilities Act to this\\n\\n10\\n\\ntype of claim by a professional athlete. First it failed\\n\\n11\\n\\nto recognize that Title 3 of the act, \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argument_portion = get_argument_portion(text)\n",
    "argument_portion[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(s):\n",
    "    '''\n",
    "    This function counts number of proper English words in a string s (not non-words like - or --)\n",
    "    '''\n",
    "    s = s.split()\n",
    "    non_words = ['-', '--']\n",
    "    return sum([x not in non_words for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_speaker_names(speakers):\n",
    "    '''\n",
    "    This function modifies speaker names like 'QUESTION' to 'QUESTION: ', for word count parsing later on\n",
    "    '''\n",
    "    return map(lambda x: x+': ', speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    This function takes in the portions of text, and gets rid of the \\n and the line numbers. \n",
    "    '''\n",
    "    text_arr=text.splitlines()\n",
    "    text_arr.remove('')\n",
    "    text_clean=[]\n",
    "    for each in text_arr:\n",
    "        if each != '':\n",
    "            try:\n",
    "                int(each)\n",
    "            except ValueError: #assummption: if the item only has integers, it is a line number.\n",
    "                text_clean.append(each)\n",
    "    out_text=' '.join(text_clean)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P R O C E E D I N G S [10:13 a.m.] CHIEF JUSTICE REHNQUIST: We'll hear argument on Number 00-24, PGA Tour, Inc. vs. Casey Martin. ORAL ARGUMENT OF H. BARTOW FARR, III ON BEHALF OF THE PETITIONER MR. FARR: Mr. Farr? Mr. Chief Justice and may it please the Court: The Ninth Circuit in our view made two critical mistakes in applying the Disabilities Act to this type of claim by a professional athlete. First it failed to recognize that Title 3 of the act, the public accommodations provision, apply on\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_argument=clean_text(argument_portion)\n",
    "clean_argument[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_wordcount(text):\n",
    "    '''\n",
    "    POSSIBLE FEATURE 1:\n",
    "    This function returns a dictionary with key: name of speaker/justice and value: total number of words they\n",
    "    spoke in total throughout argument.\n",
    "    '''\n",
    "    arg_text = get_argument_portion(text)\n",
    "    #keeps track of current speaker\n",
    "    current_speaker = 'N/A'\n",
    "    clean_argument = clean_text(arg_text)\n",
    "    \n",
    "    #clean argument text split by instances where speakers change\n",
    "    #TODO: cleanup - these should not be hardcorded and instead be result of \n",
    "    #modify_speaker_names(pet_speakers + res_speakers + other_speakers)!!!\n",
    "    #this is currently kept this way cuz of QUESTION: ..........ugh\n",
    "    split_argument = re.split('(MR. FARR: |QUESTION: |MR. REARDON: |CHIEF JUSTICE REHNQUIST: )', clean_argument)\n",
    "    all_speakers = ['MR. FARR: ', 'QUESTION: ', 'MR. REARDON: ', 'CHIEF JUSTICE REHNQUIST: ']\n",
    "    \n",
    "    #num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    num_words = dict(zip(all_speakers + [current_speaker], [0] * (len(all_speakers)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in split_argument:\n",
    "        #if split chunk signifies change in speaker\n",
    "        if s in all_speakers:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            num_words[current_speaker] = num_words[current_speaker] + count_words(s)\n",
    "    \n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHIEF JUSTICE REHNQUIST: ': 24,\n",
       " 'MR. FARR: ': 3433,\n",
       " 'MR. REARDON: ': 1480,\n",
       " 'N/A': 13,\n",
       " 'QUESTION: ': 5170}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for example, this gives us total number of words uttered by each speaker\n",
    "#we just need to find list of all speakers in the form they're referred to in the argument, \"JUSTICE SCALIA: \" for ex.\n",
    "total_wordcount(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a natural first choice for a model since our target value can be viewed as a probability between 0 or 1 for any individual justice to vote For or Against, with a higher probability representing a higher confidence of that justice voting in favor of the arguing party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(penalty='l2',C=1.0, fit_intercept=True, class_weight='auto')\n",
    "log_model = LR.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in SCDB data from file\n",
    "bigdf=pd.read_csv(\"supremeCourtDb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'caseId', u'docketId', u'caseIssuesId', u'voteId', u'dateDecision',\n",
       "       u'decisionType', u'usCite', u'sctCite', u'ledCite', u'lexisCite',\n",
       "       u'term', u'naturalCourt', u'chief', u'docket', u'caseName',\n",
       "       u'dateArgument', u'dateRearg', u'petitioner', u'petitionerState',\n",
       "       u'respondent', u'respondentState', u'jurisdiction', u'adminAction',\n",
       "       u'adminActionState', u'threeJudgeFdc', u'caseOrigin',\n",
       "       u'caseOriginState', u'caseSource', u'caseSourceState',\n",
       "       u'lcDisagreement', u'certReason', u'lcDisposition',\n",
       "       u'lcDispositionDirection', u'declarationUncon', u'caseDisposition',\n",
       "       u'caseDispositionUnusual', u'partyWinning', u'precedentAlteration',\n",
       "       u'voteUnclear', u'issue', u'issueArea', u'decisionDirection',\n",
       "       u'decisionDirectionDissent', u'authorityDecision1',\n",
       "       u'authorityDecision2', u'lawType', u'lawSupp', u'lawMinor',\n",
       "       u'majOpinWriter', u'majOpinAssigner', u'splitVote', u'majVotes',\n",
       "       u'minVotes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = bigdf[\"docketId\", \"dateDecision\", \"case\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C=1.0, kernel='linear', probability=True, class_weight='auto')\n",
    "svm_model = my_svm.fit(X, y)\n",
    "svm_pred = svm_fit.predict(W)\n",
    "# Class probabilities, based on log regression on distance to hyperplane.\n",
    "svm_prob = svm_fit.predict_proba(W)\n",
    "svm_dist = svm_fit.decision_function(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Justice Ruling Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a different dataset in a slightly different approach to making Supreme Court ruling predictions. This method is motivated by the fact that usually, only 2 justices tend to be swing votes and justice decisions are highly influenced by factors outside of what transpires in court proceedings, such as background information about the case itself. The Supreme Court website contains a Justice-centered database which contains extensive information about each case; in particular, the most pertinent fields we are interested in analyzing are:\n",
    "\n",
    "1. Decision Year\n",
    "2. Natural Court\n",
    "3. Petitioner\n",
    "4. Respondent\n",
    "5. Case Origin\n",
    "6. Case Source\n",
    "7. Lower Court Disposition Direction\n",
    "8. Issue Area\n",
    "\n",
    "Our target value to predict is the field called winningParty (petitioner or respondent), which using our justice-centered approach involves aggregating predicted votes for each individual justice and taking majority vote. The associated confidence of our entire prediction is obtained by averaging individual confidences of our models for each justice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (6,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read in justice-centered SCDB data from file\n",
    "newdf=pd.read_csv(\"SCDB_justice_centered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maybe lcDispositionDirection? choose features with continuous/numerical features\n",
    "# do the numbers mean anything though?\n",
    "newsmalldf = newdf[[\"term\", \"naturalCourt\", \"petitioner\", \"respondent\", \"caseOrigin\", \"caseSource\", \"lcDisposition\", \"issueArea\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>naturalCourt</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>caseOrigin</th>\n",
       "      <th>caseSource</th>\n",
       "      <th>lcDisposition</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   term  naturalCourt  petitioner  respondent  caseOrigin  caseSource  \\\n",
       "0  1946          1301         198         172          51          29   \n",
       "1  1946          1301         198         172          51          29   \n",
       "2  1946          1301         198         172          51          29   \n",
       "3  1946          1301         198         172          51          29   \n",
       "4  1946          1301         198         172          51          29   \n",
       "\n",
       "   lcDisposition  issueArea  \n",
       "0              2          8  \n",
       "1              2          8  \n",
       "2              2          8  \n",
       "3              2          8  \n",
       "4              2          8  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsmalldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an intuitive understanding of the features above, check out the documentation here: http://scdb.wustl.edu/documentation.php?var=petitioner. All the above features are categorical instead of continuous (which means the numbers specify a category instead of having a numerical meaning). For an illustrative example, the \"petitioner\" variable includes:\n",
    "\n",
    "1. attorney general of the United States, or his office\n",
    "2. specified state board or department of education\n",
    "7. state department or agency\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Using Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an intuitive understanding of the meanings behind the variables is important and leads us to our idea of usign the decision tree classifier. A distinct advantage of using decisiontrees is that the decision at each node has an intuitive meaning and corresponds to querying along one feature axis at a time (e.g. is the petitioner an attorney general of the United States?). \n",
    "\n",
    "Furthermore, trees are easy to understand and interpret. We can look at the top node and figure out which feature it corresponds to, and conclude that this feature contributes the most information gain, i.e. is the most important/predictive feature. This makes it easy to verify whether our results make intuitive sense.\n",
    "\n",
    "We will show the process of running decision trees on each justice, before aggregating the votes now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Justice-Centered Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, the feature that we want to predict is the vote for each justice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "5         1\n",
       "6         1\n",
       "7         1\n",
       "8         1\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "12        0\n",
       "13        0\n",
       "14        0\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        0\n",
       "20        0\n",
       "21        0\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "         ..\n",
       "114865    1\n",
       "114866    1\n",
       "114867    1\n",
       "114868    0\n",
       "114869    0\n",
       "114870    0\n",
       "114871    0\n",
       "114872    0\n",
       "114873    0\n",
       "114874    0\n",
       "114875    0\n",
       "114876    0\n",
       "114877    1\n",
       "114878    1\n",
       "114879    1\n",
       "114880    1\n",
       "114881    1\n",
       "114882    1\n",
       "114883    1\n",
       "114884    1\n",
       "114885    1\n",
       "114886    0\n",
       "114887    0\n",
       "114888    0\n",
       "114889    0\n",
       "114890    0\n",
       "114891    0\n",
       "114892    0\n",
       "114893    0\n",
       "114894    0\n",
       "Name: partyWinning, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newdf.majority refers to whether justice voted with the majority (1 for dissent, 2 for majority)\n",
    "# newdf.partyWinning indicates winning party (0 for responding party, 1 for petitioning party, 2 for unclear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
