{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109 Project - The Court Rules In Favor Of...\n",
    "## Aidi Adnan Brian John (Team AABJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "The purpose of this project is to predict votes of Supreme Court justices using oral argument transcripts. Studies in linguistics and psychology, as well as common sense, dictates that the word choices that people make convey crucial information about their beliefs and intentions with regard to issues. Rather than use precedents or formal analysis of the law to predict Supreme Court decisions, we attempt to extract essential emotional features of oral arguments made by justices and advocates in the court. Using aggregate data from 1946 to present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Oral Argument Transcripts - obtained from http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx. Transcripts are made available on the day of court hearing.\n",
    "Justice Vote Counts/Case Information - obtained from the Supreme Court Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a python script (scraper.py) to first scrape the pdfs from the Supreme Court Justice Website (but didn't upload those to the repository, because we ultimately wanted to use text files in our process). We then used a script to convert the pdf files to text files, but not before removing the last 10 pages which were reserved as an index for certain words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gather all txt files, first get the path to the data directory\n",
    "# then list the files and filter out all non-txt files\n",
    "curPath = os.getcwd()\n",
    "dataPath = curPath + '/data/'\n",
    "fileList = os.listdir(dataPath)\n",
    "fileExt = \".txt\"\n",
    "txtFiles = filter(lambda f : f[-4:] == fileExt, fileList)\n",
    "txtFiles = map(lambda f : dataPath + f, txtFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a parser to extract the names of the petitioner and respondant attorneys from the first 2 pages of the converted text document. An example of list of petitioner and respondant speakers, taken from the example case in 2014 of Johnson v United States (docket number 13-7120) is:\n",
    "\n",
    "Katherine M. Menendez, ESQ., Minneapolis, Minn.; on behalf of Petitioner\n",
    "Michael R. Dreeben, ESQ., Deputy Solicitor General, Department of Justice, Washington D.C.; on behalf of Respondent\n",
    "\n",
    "To get these speakers, we write a function that uses a regular expression to split the lines based on new line and checks whether there is a name in the line. If we find a name, then we check if that name was listed as a petitoner or respondent at the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPetitionersAndRespondents(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text : a transcript in its raw form, without having run cleanTextMaker\n",
    "\n",
    "    Returns:\n",
    "    pet_speakers, res_speakers, other_speakers\n",
    "    the petitoner speakers, the respondent speakers, and any other speakers as a list\n",
    "    '''\n",
    "    #get portion of transcript between APPEARANCES and CONTENTS that specifies speakers for petitioners/respondents\n",
    "    start = text.find('APPEARANCES:') + len('APPEARNACES')\n",
    "    end = text.find('C O N T E N T S')\n",
    "    speakers_text = text[start:end]\n",
    "    split_speakers_text = re.split('\\.[ ]*\\n', speakers_text)\n",
    "    #for each speaker, get name (capitalized) and side (Pet/Res) he/she is speaking for\n",
    "    pet_speakers, res_speakers, other_speakers = [], [], []\n",
    "    for speaker in split_speakers_text:\n",
    "        name = speaker.strip().split(',')[0]\n",
    "        #search for first index of capitalized word (which will be start of speaker name)\n",
    "        start = 0\n",
    "        for idx, char in enumerate(name):\n",
    "            if str.isupper(char):\n",
    "                start = idx\n",
    "                break\n",
    "        #actual name to be appended to correct list\n",
    "        name = name[start:]\n",
    "        \n",
    "        #if words Petition, Plaintiff, etc occur in speaker blurb, speaker belongs to Pet\n",
    "        if any(x in speaker for x in ['etition' , 'ppellant', 'emand', 'evers', 'laintiff']):\n",
    "            pet_speakers.append(name)\n",
    "        #otherwise if words Respondent, Defendant, etc occur, speaker belongs to Res\n",
    "        elif any(x in speaker for x in ['espond' , 'ppellee', 'efendant']):\n",
    "            res_speakers.append(name)\n",
    "        #otherwise if neither side is specified in blurb, speaking belongs to Other\n",
    "        elif 'neither' in speaker:\n",
    "            other_speakers.append(name)\n",
    "    return pet_speakers, res_speakers, other_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function used to generate a regular expression for later use based on \"$TITLE. $LASTNAME\"; however, that pattern ultimately ended up not being used in some of the cases, and we abandonded this format in favor of just using the last name to generate the regular expression that we would ultimately use to separate the text into which speaker was responsible for a portion of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a list of regular expressions to split the text on\n",
    "def generateRES(nameList, plebe):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    nameList: a list of strings of the names\n",
    "    plebe: a boolean determining whether or not the list is of justices or not\n",
    "    \n",
    "    Returns:\n",
    "    A list of regular expressions for each of the names that work for supreme\n",
    "    court case transcripts\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for name in nameList:\n",
    "        address = \"\"\n",
    "        if plebe:\n",
    "            words = name.split(' ')\n",
    "            # first term is the title, last\n",
    "            # word is the last name\n",
    "            address = words[-1]\n",
    "            retList.append(address)\n",
    "        else:\n",
    "            address = \"JUSTICE %s\" % name\n",
    "            address2 = \"CHIEF JUSTICE %s\" % name\n",
    "            retList.append(address)\n",
    "            retList.append(address2)\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getJusticeNames(text):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    text is the raw text of a transcript\n",
    "    \n",
    "    Returns:\n",
    "    A set of the names of Justices mentioned by name in the transcript\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    retList = []\n",
    "    while index < len(text):\n",
    "        index = text.find(\"JUSTICE\", index)\n",
    "        if index == -1:\n",
    "            break\n",
    "        index += 8 # because length of JUSTICE is 7, plus length of the space\n",
    "        prevIndex = index\n",
    "        while text[index] != ':':\n",
    "            index +=1\n",
    "        retList.append(text[prevIndex:index])\n",
    "    return list(set(retList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general flow of court proceedings is that the Petitioner attornies make their oral argument, followed by the Respondent attornies, before we hear the rebuttal argument of the Petitioners again. Throughout all proceedings, Justices are free to interject with questions and statements of their own. The below function extracts the main argument portion of the oral transcripts, which is the meat of the proceedings that we are interested in conducting analysis on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_argument_portion(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    Raw text of a transcript as a string\n",
    "    \n",
    "    Returns:\n",
    "    The argument portion of the case as a string\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('P R O C E E D I N G S')\n",
    "    end = text.rfind('Whereupon')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords(s):\n",
    "    '''\n",
    "    Inputs:\n",
    "    s: string of words\n",
    "    \n",
    "    Returns:\n",
    "    An integer counting the number of the words in s\n",
    "    '''\n",
    "    s = s.split()\n",
    "    non_words = ['-', '--']\n",
    "    return sum([x not in non_words for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_speaker_names(speakers):\n",
    "    '''\n",
    "    Inputs:\n",
    "    speakers: a list of speakers as strings\n",
    "    \n",
    "    Returns:\n",
    "    A list of the speaker with a colon appended onto them,\n",
    "    corresponding to how they appear in the transcripts\n",
    "    '''\n",
    "    return map(lambda x: x+': ', speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcripts contain a lot of line numbers as well as linebreaks in between sentences, so we want to remove those before we try and do any analysis on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTextMaker(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text: a raw text with newlines and numbers, as is usual in the transcripts\n",
    "    \n",
    "    Returns:\n",
    "    A file with newlines and numbers scrubbed\n",
    "    '''\n",
    "    text_arr=text.splitlines()\n",
    "    text_clean=[]\n",
    "    for each in text_arr:\n",
    "        if each != '':\n",
    "            try:\n",
    "                int(each)\n",
    "            except ValueError: #assummption: if the item only has integers, it is a line number.\n",
    "                text_clean.append(each)\n",
    "    out_text=' '.join(text_clean)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_wordcount(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text: some raw text of a transcript\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary of the the number of words each speaker in the text spoke\n",
    "    '''\n",
    "    \n",
    "    arg_text = get_argument_portion(text)\n",
    "    #keeps track of current speaker\n",
    "    current_speaker = 'N/A'\n",
    "    #clean argument text split by instances where speakers change\n",
    "    clean_argument = cleanTextMaker(arg_text)\n",
    "    \n",
    "\n",
    "    # first get the names of the judges and speakers\n",
    "    pet_speakers, res_speakers, other_speakers = get_petitioners_and_respondents(text)\n",
    "    justiceLeague = getJusticeNames(clean_argument)\n",
    "    # create the regular expression for the justices and the plebes\n",
    "    # need to also add the justice speaker\n",
    "    JLList = generateRES(justiceLeague, False)\n",
    "    plebeList = pet_speakers + res_speakers + other_speakers\n",
    "    plebeRE = generateRES(plebeList, True)\n",
    "    finREList = [\"QUESTION\"]\n",
    "    finREList += plebeRE + JLList\n",
    "    \n",
    "    # speakers are indicated by their name with a colon appended to the name\n",
    "    finREList = map(lambda name : name + \":\", finREList)\n",
    "    RE = '('  + '|'.join(finREList) + ')'\n",
    "    \n",
    "    split_argument = re.split(RE, clean_argument)\n",
    "    all_speakers = finREList\n",
    "    \n",
    "    #num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    num_words = dict(zip(all_speakers + [current_speaker], [0] * (len(all_speakers)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in split_argument:\n",
    "        #if split chunk signifies change in speaker\n",
    "        if s in all_speakers:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            num_words[current_speaker] = num_words[current_speaker] + count_words(s)\n",
    "    \n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordCounter(text):\n",
    "    \"\"\"\n",
    "    counts number of times each word appears in a file\n",
    "    \n",
    "    Inputs:\n",
    "    text: raw text of transcript\n",
    "    \n",
    "    Returns:\n",
    "    a dictionary of (word : times) it appears\n",
    "    \"\"\"\n",
    "    wordCount={}\n",
    "    for word in text.split():\n",
    "        # unfortunately, isalpha does discount some real words\n",
    "        # like those with apostrophes, and words with question\n",
    "        # marks at the end of them\n",
    "        if word.lower() not in wordCount and word.isalpha():\n",
    "            wordCount[word.lower()] = 1\n",
    "        elif word.isalpha():\n",
    "            wordCount[word.lower()] += 1\n",
    "    return wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topWords(diction, num, verbose=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    dictionary: of the format word : count for each entry\n",
    "    num: an integer\n",
    "    verbose: whether printing of the words is desired\n",
    "    \n",
    "    returns: \n",
    "    the top num words in a dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    d = collections.Counter(diction)\n",
    "    if verbose:\n",
    "        for k, v in d.most_common(numTop):\n",
    "            print '%s: %i' % (k, v)\n",
    "    return d.most_common(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitData(X, fraction_train=9.0 / 10.0):\n",
    "    \"\"\"\n",
    "    Deterministically splits a vector\n",
    "    \n",
    "    Inputs:\n",
    "    X: a one dimensional vector\n",
    "    fraction_train: the fraction of data that is desired to be train\n",
    "    \n",
    "    Returns:\n",
    "    the train portion and test portions of the vector\n",
    "    \"\"\"\n",
    "    end_train = int(len(X) * fraction_train)\n",
    "    X_train = X[0:end_train]\n",
    "    X_test = X[end_train:]\n",
    "    return X_train, X_test\n",
    "\n",
    "def splitTrainTest(X, Y, fraction_train = 9.0 / 10.0):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X, Y : vectors to be split\n",
    "    \n",
    "    Returns:\n",
    "    Each vector split into train and test\n",
    "    \"\"\"\n",
    "    X_train, X_test = splitData(X, fraction_train)\n",
    "    Y_train, Y_test = splitData(Y, fraction_train)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textToMat(vizer, docList):\n",
    "    \"\"\"\n",
    "    turns documents into a tfidf matrix\n",
    "    \n",
    "    Inputs:\n",
    "    vizer: a vectorizer on documents\n",
    "    docList: a list of preprocessed documents\n",
    "    \n",
    "    Returns:\n",
    "    a list of matricies that contain the frequencies of words in\n",
    "    the documents based on the vectorizer passed\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for doc in docList:\n",
    "        resMat = vizer.transform(doc).todense()\n",
    "        retList.append(resMat)\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document representation is the first step of our analysis since there are a variety of ways to represent a transcript, which in its raw form is a simple string of texts. We use a pre-processing technique that reduces the complexity of the documents and makes them easier to handle, which is to transform the oral transcripts from the full text version to a document vector/sparse matrix. Every text document is represented as a vector of term weights (word features) from a set of terms (dictionary), where each term occurs at least once in a certain critical number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High dimensionality of text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major characteristic of document classification problems is the extremely high dimensionality of data where the number of potential features often exceeds the number of training documents. Dimensionality reduction is thus critical to allow for efficient data manipulation. Irrelevant and redundant features often degrade performance of classification algorithms both in accuracy and speed, and also tends to fall into the all-common trap of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing of text data involves tokenization of raw text, stop words removal, stemming and eliminating as much as possible the language dependent factors. Brief explanations of these preprocessing stages are as fllows:\n",
    "\n",
    "1. Sentence splitting: identifying sentence boundaries in documents\n",
    "2. Tokenization: partitioning documents that are initially treated as a string into a list of tokens\n",
    "3. Stop words removal: removing common English words like \"the\", \"a\", etc\n",
    "4. Stemming: reducing derived words to its most root form, example happiest -> happy\n",
    "5. Noisy data: cleaning noisy data spilt over from pdf to text conversion, including inclusion of line numbers, page breaks, etc\n",
    "6. Text representation: determining whether we should use words, phrases or entire sentences as a \"token\" for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction vs. feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature extraction, feature selection was conducted to construct a vector space of appropriate dimensionality, which improves the scalability, efficiency and accuracy of our classification algorithm. The main idea of feature selection is to choose a subset of features from the original texts, with subset determined by obtaining features with the highest score according to some predetermined measure of feature importance.\n",
    "\n",
    "We attempt two different approaches for feature selection in our analysis:\n",
    "1. Wrappers: \n",
    "2. Filters: as opposed to wrappers, filters can be conducted independently of the actual classification algorithm, and hence is less computationally expensive. Filters use an evaluation metric that measures ability of a feature to differentiate each class, hence choosing the most discriminative and valuable features. The filter of our choice is a technique called frequency document-inverse document frequency, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Document-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency document–inverse document frequency (tf-idf), is a powerful method to evaluate how important is a word in a document, and captures the relative relevance among words. It converts the textual representation of information into a Vector-Space Model or a sparse matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used to generated tfidf sparse matricies for the importance of words in documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# internal utilities used to replicate functionality of truncated_svd\n",
    "from sklearn.utils import as_float_array\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "# stemmer of words\n",
    "import snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRawText(txtFiles):\n",
    "    '''\n",
    "    Inputs:\n",
    "    txtFiles: a list of paths of textfiles\n",
    "    \n",
    "    Returns:\n",
    "    list of all uncleaned transcripts in raw text form.\n",
    "    '''\n",
    "    return [(open(txtFiles[i]).read()) for i in range(len(txtFiles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRawCleanText(txtFiles):\n",
    "    '''\n",
    "    Inputs:\n",
    "    txtFiles: a list of paths of textfiles\n",
    "    \n",
    "    Returns:\n",
    "    list of all cleaned transcripts in raw text form.\n",
    "    '''\n",
    "    return [(cleanTextMaker(open(txtFiles[i]).read())) for i in range(len(txtFiles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allRawText = getAllRawText(txtFiles)\n",
    "allRawCleanText = getAllRawCleanText(txtFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the approach we took when generating the regular expressions for parsing, we want to have a way to convert the names scraped from the documents into a name that we can generally split the speechs by, and thus we adopt the convention of using the last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toColloquialName(formal_name):\n",
    "    ret = formal_name.split()\n",
    "    return ret[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBagOfWords(txtFiles):\n",
    "    \"\"\"\n",
    "    Gets bag of words dictionary for every document in txtFiles\n",
    "    \n",
    "    Inputs:\n",
    "    txtFiles: lists of paths of textfiles\n",
    "    \n",
    "    Returns:\n",
    "    list of dictionary of word : number of times word appears in transcript\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for File in txtFiles:\n",
    "        cur = open(File)\n",
    "        textual = cur.read()\n",
    "        cleanTextual = wordCounter(textual)\n",
    "        retList.append(cleanTextual)\n",
    "        cur.close()\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transforms bag of words into td-idf weighted\n",
    "bow = getBagOfWords(txtFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Latent Sentiment Indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running LSI, we do not want to possibly split importance among words that are actually very similar (such as \"stealing\" and \"steal\"), so we stem words by removing the suffix, bring down the words to a root, or 'stem' that we can assign importance to. There was an issue with different encodings: the txt documents are stored in Latin1 encoding when converted from PDFs to permit earlier functions to work, but when iterating through words there is some issues with how the words are decoded and passed to the stemmer. As a result, we need to manually convert incompatible strings to a tractable format. This conversion was not possible on a single document in our entire database, so we ultimately had to remove it from our database (if we didn't remove it, there would be an issue when we used the docketId from the document to index into a merged dataframe later on, we would have one more response variable than predictor variables). Stemming naturally can leave words as a non english word, or may incorrectly mistem a word. Nothing short of a large dictionary containing the stem of every possible word would accurately perform the stemming, so we are forced to accept this aggressive trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def destem(allRawText):\n",
    "    \"\"\"\n",
    "    stems all words from a list of documents\n",
    "    documents are assumed to be stored in Latin1 encoding\n",
    "    there is one document that is not tractable so we exclude it\n",
    "    uses snowballstemmer\n",
    "    required to decode string to avoid UnicodeDecodeErrors\n",
    "    \n",
    "    Inputs:\n",
    "    a list of raw text files that have been cleaned\n",
    "    \n",
    "    Returns:\n",
    "    a list of text files that have been stemmed word by word\n",
    "    \"\"\"\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    stemmedList = []\n",
    "    for text in allRawText:\n",
    "        try:\n",
    "            temp = stemmer.stemWords(text.split())\n",
    "            for i in xrange(len(temp)):\n",
    "                if str(type(temp[i])) == \"<type 'str'>\":\n",
    "                    temp[i] = temp[i].decode('Latin1')\n",
    "            res = ' '.join(temp)\n",
    "            stemmedList.append(res)\n",
    "        except UnicodeDecodeError:\n",
    "            # literally just one document\n",
    "            pass\n",
    "    return stemmedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocketNo(text):\n",
    "    '''\n",
    "    Input: the text of a transcript\n",
    "    \n",
    "    Returns:\n",
    "    the docket number of the case\n",
    "    '''\n",
    "    cleantext = cleanTextMaker(text)\n",
    "    docketIdx = cleantext.find(\"No.\")\n",
    "    return cleantext[docketIdx+4:].split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_dict(fileList, fileExt='.txt'):\n",
    "    '''\n",
    "    This function takes the fileList and returns a list of dictionaries of the format \n",
    "    {'case_number': case_number, 'full_text': full_text}\n",
    "    \n",
    "    Inputs:\n",
    "    fileList: list of the paths of the textfiles\n",
    "    fileExt: optional parameter for the type of file\n",
    "    \n",
    "    Returns:\n",
    "    dictionary of the filename:text\n",
    "    '''\n",
    "    fileDict=[]\n",
    "    fields=['docket', 'full_text']\n",
    "    txtFiles_filter = filter(lambda f : f[-4:] == fileExt, fileList)\n",
    "    for each in txtFiles_filter:\n",
    "        name_str=each[4:-4]\n",
    "        try:\n",
    "            indexx=name_str.index('_')\n",
    "            docketNum=name_str[:indexx]\n",
    "        except ValueError:\n",
    "            docketNum=name_str\n",
    "        cur = open(dataPath+each)\n",
    "        textual = cur.read()\n",
    "        cur.close()\n",
    "        tuple_=(docketNum, textual)\n",
    "        fileDict.append(dict(zip(fields, tuple_)))\n",
    "    return fileDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to merge the text files with the supreme court database so that we can easily associate the text files with the docketId and get the decision of the cases. However, the supreme court database unfortunately does not contain information from beyond 2014, so we lose out on several transcripts in the merging process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 54)\n"
     ]
    }
   ],
   "source": [
    "fileDict=get_file_dict(fileList)\n",
    "txtdf = pd.DataFrame(fileDict)\n",
    "casedf = pd.read_csv('supremeCourtDb.csv')\n",
    "merged = pd.merge(left=txtdf, right=casedf, how='inner', left_on='docket', right_on='docket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As referenced earlier, there is a single document that is not tractable to stemming due to codec issues, so we merely drop it for being insolent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop problematic docket\n",
    "merged = merged[merged.docket != '08-351']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 54)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Splitting prepared documents into petitioner and respondent speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have about 930 documents to work with. We now want to find a way to gather the texts of what the petitioners and the respondents say. We adapt a function we wrote earlier that counted the number of words that each speaker said and use it to gather the texts that each party is responsible for. We first gather them by speaker then gather them by the group that they are a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitTextPetRes(text):\n",
    "    '''\n",
    "    Input: \n",
    "    text: raw text document (transcript, recently opened)\n",
    "        \n",
    "    Returns:\n",
    "    a dictionary of speaker: the words they said\n",
    "    a dictionary of group: the words they said\n",
    "    '''\n",
    "    arg_text = get_argument_portion(text)\n",
    "    #keeps track of current speaker\n",
    "    current_speaker = 'N/A'\n",
    "    clean_argument = cleanTextMaker(arg_text)\n",
    "\n",
    "    # first get the names of the judges and speakers\n",
    "    pet_speakers, res_speakers, _ = get_petitioners_and_respondents(text)\n",
    "    \n",
    "    # create the regular expression for the justices and the plebes\n",
    "    petList = generateRES(pet_speakers, True)\n",
    "    resList = generateRES(res_speakers, True)\n",
    "    petList = map(lambda name : name + \":\", petList)\n",
    "    resList = map(lambda name : name + \":\", resList)\n",
    "    all_speakers = (petList + resList)\n",
    "    \n",
    "    RE = '('  + '|'.join(all_speakers) + ')'\n",
    "    \n",
    "    # split argument portion by times elements in plebeList (e.x. MR. FARR: or EUGENE: appears)\n",
    "    split_argument = re.split(RE, clean_argument)\n",
    "    \n",
    "    # dictionary keyed by speaker, with value actual speech (in string format)\n",
    "    speech = dict(zip(all_speakers + [current_speaker], [\"\"] * (len(all_speakers)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating speeches for all speakers\n",
    "    for s in split_argument:\n",
    "        if s in all_speakers:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            speech[current_speaker] += s\n",
    "\n",
    "    #combine all pet and res speakers, if multiple\n",
    "    retDict = {\"resSpeakers\":\"\", \"petSpeakers\":\"\"}\n",
    "    \n",
    "    for rSpeaker in resList:\n",
    "        retDict[\"resSpeakers\"] += speech[rSpeaker]\n",
    "    for pSpeaker in petList:\n",
    "        retDict[\"petSpeakers\"] += speech[pSpeaker]\n",
    "\n",
    "    return speech, retDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because scraping is not perfect, sometimes we fail to gather the names of petitioners or respondents. In this case, we do not want to add that case's speech to the database because then we wouldnt be able to compare either the respondents or the petitioners against an empty speech. Just would not be fair!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate through merged.full_text, trying to fill in merged.pet_speech and merged.res_speech\n",
    "allPetSpeeches = []\n",
    "allResSpeeches = []\n",
    "allDocketNo = []\n",
    "allDecisions = []\n",
    "for row in merged.iterrows():\n",
    "    speech, retDict = splitTextPetRes(row[1][\"full_text\"])\n",
    "    petSpeech = retDict[\"petSpeakers\"]\n",
    "    resSpeech = retDict[\"resSpeakers\"]\n",
    "    # if either petSpeech or resSpeech is an empty string, do not add to workable dataset\n",
    "    if petSpeech and resSpeech:\n",
    "        allPetSpeeches.append(petSpeech)\n",
    "        allResSpeeches.append(resSpeech)\n",
    "        allDocketNo.append(row[1][\"docket\"])\n",
    "        allDecisions.append(row[1][\"partyWinning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 886, 886)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some oral transcripts have empty petitioner or respondent speeches due to dirty scraping of pdf files\n",
    "# for example, get_petitioners_and_respondents sometimes does not scrape properly due to bad formatting\n",
    "len(allPetSpeeches), len(allResSpeeches), len(allDecisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now stem all of the words in the document files. This takes a long time because of the inconsistent encodings of strings as mentioned earlier, necessitating iterating through each word and manually trying to convert it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes long to run\n",
    "allDestemmedPetSpeeches = destem(allPetSpeeches)\n",
    "allDestemmedResSpeeches = destem(allResSpeeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 886)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allDestemmedPetSpeeches), len(allDestemmedResSpeeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Applying term-frequency inverse document frequency vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to pass each set of documents to the vectorizer that will count the number of words and assign them based on Term-Frequency Inverse Document Frequency (tfidf), which first calculates the raw term frequency (aka the number of times that the word appears in the document) and then multiplies it by the inverse document frequency (a global weighing function):\n",
    "$$g_i = \\log_2 \\frac{n}{1 + df_i}$$\n",
    "\n",
    "Where $g_i$ is the weight for term $i$, $n$ is the number of times a word appears in a document, and $df_i$ is the number of documents in which $i$ appears. This properly penalizes words that appear frequently in many documents ('the', 'of', etc). We take $g_i \\times f_i$ (where $f_i$ is the raw frequency) as the tfidf statistic for that word in a given document. (credit to: https://en.wikipedia.org/wiki/Latent_semantic_indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# produallStemmedArgumentsemmedArgumentsvectorizer that will calculate the importance of words\n",
    "vectorizer1 = TfidfVectorizer(min_df=1, norm='l2', use_idf=True, stop_words='english', encoding='Latin1', analyzer='word', token_pattern='\\w+')\n",
    "vectorizer2 = TfidfVectorizer(min_df=1, norm='l2', use_idf=True, stop_words='english', encoding='Latin1', analyzer='word', token_pattern='\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run SVD on the matrix generated by the vectorizers, we get back 3 matrices. The first matrix is a matrix that represents the themes in the documents, the second is a diagonal matrix of singular values representing the relative importance of each theme overall, and the third matrix is representing the importance of each word in the themes. We can run logistic regression on just the first matrix, and specifically the different between the matrix of the respondents and the petitioners, to represent the difference in how strongly the parties speak about a certain theme within a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Running Singular Vector Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runSVD(documentList, vizer, numComponents=25, nIter=5):\n",
    "    \"\"\"\n",
    "    takes a list of documents and a vectorizer\n",
    "    converts document list to a matrix of frequencies \n",
    "        (as determined by the vectorizer) of document by word\n",
    "    takes matrix and runs truncated SVD on it to generate\n",
    "    a matrix that consists of themes (T) in each document\n",
    "    overall importance of the word (S)\n",
    "    and a matrix that consists of how important each word\n",
    "    is in the theme (DT)\n",
    "    \n",
    "    this code is partially derived from sklearn's\n",
    "    truncated_svd function (which doesn't return\n",
    "    all of the matricies we are interested in)\n",
    "    \"\"\"\n",
    "    mat = vizer.fit_transform(documentList)\n",
    "    X = as_float_array(mat, copy=False)\n",
    "    # T is the term by concept matrix\n",
    "    # S the singular value matrix\n",
    "    # D is the concept-document matrix\n",
    "    T, S, DT = randomized_svd(X, numComponents, n_iter=nIter)\n",
    "    return T, S, DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tPet, sPet, dTPet = runSVD(allDestemmedPetSpeeches, vectorizer1, numComponents=25)\n",
    "tRes, sRes, dTRes = runSVD(allDestemmedResSpeeches, vectorizer2, numComponents=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((886, 25), (25,), (25, 29308))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tPet.shape, sPet.shape, dTPet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((886, 25), (25,), (25, 28559))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tRes.shape, sRes.shape, dTRes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tDiff = tPet - tRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Training logistic regression classifier on petitioner and respondent differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run logistic regression on D x numTopics matrix of independent variables, vs. 0/1 result vector\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clflog = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "cv_optimize\n",
    "\n",
    "Inputs\n",
    "------\n",
    "clf : an instance of a scikit-learn classifier\n",
    "parameters: a parameter grid dictionary thats passed to GridSearchCV (see above)\n",
    "X: a samples-features matrix in the scikit-learn style\n",
    "y: the response vectors of 1s and 0s (+ives and -ives)\n",
    "n_folds: the number of cross-validation folds (default 5)\n",
    "score_func: a score function we might want to pass (default python None)\n",
    "\n",
    "Returns\n",
    "-------\n",
    "The best estimator from the GridSearchCV, after the GridSearchCV has been used to\n",
    "fit the model.\n",
    "\"\"\"\n",
    "def cv_optimize(clf, parameters, X, y, n_folds=5):\n",
    "    clf = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    clf.fit(X,y)\n",
    "    return clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Testing cross-validation and evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xTrain, yTrain, xTest, yTest = splitTrainTest(tDiff, allDecisions)\n",
    "clflogopt = cv_optimize(clflog, {\"C\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, xTrain, yTrain, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clflogopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692597239649 0.651685393258\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = clflogopt.score(xTrain, yTrain)\n",
    "test_accuracy = clflogopt.score(xTest, yTest)\n",
    "print training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above analysis, we have focused on parts of the oral transcripts corresponding to attorney speeches, which means we are essentially ignoring an equally valuable portion of information that we can glean from these transcripts: the judge's responses and questions to these attorneys. In this section, we attempt to conduct Natural Language Processing on judge's speeches and other salient features of the text that were not included in the Latent Semantic Analysis above, which might yield interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Cleaning dataset\n",
    "Some of our oral transcripts do not discriminate between individual justice's speeches, instead using \"QUESTION:\" in place of all justice's speeches. We are not unable to use these, and for consistency's sake, delete all these transcripts from our working dataset. It turns out that this was not too consequential since only transcripts from 2001-2002 had the \"QUESTION:\" ambiguity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find qualifying rows without \"QUESTION:\"\n",
    "qualifyingRows = []\n",
    "for rowNo in range(len(merged)):\n",
    "    # find appropriate row and check whether full_text contains QUESTION: - if so, delete from database\n",
    "    row = merged.iloc[rowNo, :]\n",
    "    if row[\"full_text\"].find(\"QUESTION:\") == -1:\n",
    "        qualifyingRows.append(rowNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>full_text</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>...</th>\n",
       "      <th>authorityDecision1</th>\n",
       "      <th>authorityDecision2</th>\n",
       "      <th>lawType</th>\n",
       "      <th>lawSupp</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>02-1472</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-025</td>\n",
       "      <td>2004-025-01</td>\n",
       "      <td>2004-025-01-01</td>\n",
       "      <td>2004-025-01-01-01</td>\n",
       "      <td>3/1/05</td>\n",
       "      <td>1</td>\n",
       "      <td>543 U.S. 631</td>\n",
       "      <td>125 S. Ct. 1172</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>25 U.S.C. � 450</td>\n",
       "      <td>110</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>02-1672</td>\n",
       "      <td>1\\n2\\n\\nIN THE SUPREME COURT OF THE UNITED STA...</td>\n",
       "      <td>2004-033</td>\n",
       "      <td>2004-033-01</td>\n",
       "      <td>2004-033-01-01</td>\n",
       "      <td>2004-033-01-01-01</td>\n",
       "      <td>3/29/05</td>\n",
       "      <td>1</td>\n",
       "      <td>544 U.S. 167</td>\n",
       "      <td>125 S. Ct. 1497</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>03-10198</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-073</td>\n",
       "      <td>2004-073-01</td>\n",
       "      <td>2004-073-01-01</td>\n",
       "      <td>2004-073-01-01-01</td>\n",
       "      <td>6/23/05</td>\n",
       "      <td>1</td>\n",
       "      <td>545 U.S. 605</td>\n",
       "      <td>125 S. Ct. 2582</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>03-1039</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-032</td>\n",
       "      <td>2004-032-01</td>\n",
       "      <td>2004-032-01-01</td>\n",
       "      <td>2004-032-01-01-01</td>\n",
       "      <td>3/22/05</td>\n",
       "      <td>1</td>\n",
       "      <td>544 U.S. 133</td>\n",
       "      <td>125 S. Ct. 1432</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>03-1116</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-045</td>\n",
       "      <td>2004-045-01</td>\n",
       "      <td>2004-045-01-01</td>\n",
       "      <td>2004-045-01-01-01</td>\n",
       "      <td>5/16/05</td>\n",
       "      <td>1</td>\n",
       "      <td>544 U.S. 460</td>\n",
       "      <td>125 S. Ct. 1885</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       docket                                          full_text    caseId  \\\n",
       "158   02-1472  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-025   \n",
       "169   02-1672  1\\n2\\n\\nIN THE SUPREME COURT OF THE UNITED STA...  2004-033   \n",
       "216  03-10198  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-073   \n",
       "218   03-1039  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-032   \n",
       "220   03-1116  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-045   \n",
       "\n",
       "        docketId    caseIssuesId             voteId dateDecision  \\\n",
       "158  2004-025-01  2004-025-01-01  2004-025-01-01-01       3/1/05   \n",
       "169  2004-033-01  2004-033-01-01  2004-033-01-01-01      3/29/05   \n",
       "216  2004-073-01  2004-073-01-01  2004-073-01-01-01      6/23/05   \n",
       "218  2004-032-01  2004-032-01-01  2004-032-01-01-01      3/22/05   \n",
       "220  2004-045-01  2004-045-01-01  2004-045-01-01-01      5/16/05   \n",
       "\n",
       "     decisionType        usCite          sctCite    ...    authorityDecision1  \\\n",
       "158             1  543 U.S. 631  125 S. Ct. 1172    ...                     4   \n",
       "169             1  544 U.S. 167  125 S. Ct. 1497    ...                     4   \n",
       "216             1  545 U.S. 605  125 S. Ct. 2582    ...                     7   \n",
       "218             1  544 U.S. 133  125 S. Ct. 1432    ...                     7   \n",
       "220             1  544 U.S. 460  125 S. Ct. 1885    ...                     2   \n",
       "\n",
       "    authorityDecision2  lawType  lawSupp         lawMinor majOpinWriter  \\\n",
       "158                NaN        6      600  25 U.S.C. � 450           110   \n",
       "169                NaN        3      322              NaN           104   \n",
       "216                  2        2      231              NaN           109   \n",
       "218                  4        3      341              NaN           106   \n",
       "220                NaN        1      111              NaN           106   \n",
       "\n",
       "    majOpinAssigner splitVote  majVotes  minVotes  \n",
       "158             103         1         8         0  \n",
       "169             103         1         5         4  \n",
       "216             102         1         7         2  \n",
       "218             104         1         5         3  \n",
       "220             105         1         5         4  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedJudges = merged.iloc[qualifying_rows, :] \n",
    "mergedJudges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Selection and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cleaned dataframe with information about raw text and decisions for every oral transcript, we want to find the most appropriate features for predictors - we're on the stage of feature extraction again. After much experimentation and linguistical analysis on court proceedings in particular, we identified the following features that we want to introduce as predictors to run our classification algorithm on. \n",
    "\n",
    "For each oral transcript, we want to identify:\n",
    "1. Number of words judges uttered to petitioner's and respondent's sides:\n",
    "        Usage: judges_word_count_split(text)\n",
    "2. Number of times a judge interrupted petitioner or respondent attorneys: \n",
    "        Usage: total_interruptions_pet, total_interruptions_ret = get_total_interruptions(text)\n",
    "3. Sentiment analysis on words judges uttered to petitioner's and respondent's sides: \n",
    "        Usage: judges_speech_split(text) and sentiment analysis using existing dictionary of positive/negative\n",
    "        words\n",
    "\n",
    "We will look at each of these features one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Exploring feature #1: Number of words judges directed at each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSplitArgument(text):\n",
    "    '''\n",
    "    Gets split argument by RE generated with list of all speakers.\n",
    "    '''\n",
    "    # first get the names of the judges and speakers\n",
    "    petSpeakers, resSpeakers, _ = getPetitionersAndRespondents(text)\n",
    "    cleanArg = cleanTextMaker(text)\n",
    "    justiceSpeakers = getJusticeNames(cleanArg)\n",
    "    \n",
    "    # creates duplicate \"JUSTICE\" and \"CHIEF JUSTICE\"; creates RE for pet/res/justices\n",
    "    justiceRE = generateRES(justiceSpeakers, False)\n",
    "    petRE = generateRES(petSpeakers, True)\n",
    "    resRE = generateRES(resSpeakers, True)\n",
    "    \n",
    "    # appends colons to all REs\n",
    "    justiceSpeakersColon = map(lambda name : name + \":\", justiceRE)\n",
    "    petSpeakersColon = map(lambda name : name + \":\", petRE)\n",
    "    resSpeakersColon = map(lambda name : name + \":\", resRE)\n",
    "    justiceSpeakersColon = map(lambda name : name + \":\", justiceRE)\n",
    "    \n",
    "    # aggregates justice and attorney REs\n",
    "    allSpeakers = [\"QUESTION\"]\n",
    "    allSpeakers += (justiceRE + petRE + resRE)\n",
    "    allSpeakersColon = map(lambda name : name + \":\", allSpeakers)\n",
    "    \n",
    "    # finally, creates regular expression for the justices and attorneys (i.e. all_speakers) to split text on\n",
    "    RE = '('  + '|'.join(allSpeakersColon) + ')'\n",
    "    \n",
    "    # splits argument portion according to generated regular expression above that consists of all possible speakers,\n",
    "    # plebes and judges alike\n",
    "    splitArg = re.split(RE, cleanArg)\n",
    "    \n",
    "    return splitArg, allSpeakersColon, petSpeakersColon, resSpeakersColon, justiceSpeakersColon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def speakersWordCount(text):\n",
    "    '''\n",
    "    FINAL FEATURE # 1:\n",
    "    Total number of words spoken by each justice in the entire transcript.\n",
    "    \n",
    "    Returns:\n",
    "    num_words: a dictionary with key being name of justice/attorney and value being total number of words they\n",
    "    spoke in total throughout argument.  \n",
    "    '''\n",
    "    # splits argument\n",
    "    splitArg, allSpeakersColon, petSpeakersColon, resSpeakersColon, justiceSpeakersColon = getSplitArgument(text)\n",
    "    \n",
    "    # num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    currSpeaker = 'NA:'\n",
    "    prevSpeaker = 'NA:'\n",
    "    numWordsToPet = dict(zip(allSpeakersColon + [currSpeaker], [0] * (len(allSpeakersColon)+1)))\n",
    "    numWordsToRes = dict(zip(allSpeakersColon + [currSpeaker], [0] * (len(allSpeakersColon)+1)))\n",
    "    \n",
    "    # iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in splitArg:\n",
    "        print currSpeaker in justiceSpeakersColon\n",
    "        print prevSpeaker in petSpeakersColon\n",
    "        print prevSpeaker in resSpeakersColon\n",
    "        #if split chunk signifies change in speaker\n",
    "        if s in allSpeakersColon:\n",
    "            prevSpeaker = currSpeaker\n",
    "            currSpeaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            if currSpeaker in justiceSpeakersColon and prevSpeaker in petSpeakersColon:\n",
    "                numWordsToPet[currSpeaker] += countWords(s)\n",
    "            elif currSpeaker in justiceSpeakersColon and prevSpeaker in resSpeakersColon:\n",
    "                numWordsToRes[currSpeaker] += countWords(s)\n",
    "\n",
    "    return numWordsToPet, numWordsToRes, petSpeakersColon, resSpeakersColon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deleteVal(dictionary,val):\n",
    "    '''\n",
    "    Get rid of all items in dictionary with value being a specific val: \n",
    "    E.x. when no words spoken means that we can ignore them\n",
    "    '''\n",
    "    for k,v in dictionary.items():\n",
    "        if v == val:\n",
    "            del dictionary[k]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureOne(text):\n",
    "    numWordsToPet, numWordsToRes, petSpeakersColon, resSpeakersColon = speakersWordCount(text)\n",
    "    numWordsToPet = deleteVal(numWordsToPet,0)\n",
    "    numWordsToRes = deleteVal(numWordsToRes,0)\n",
    "    # clump together petitioners and respondents\n",
    "    numWordsPet, numWordsRes = 0,0\n",
    "    for s in petSpeakersColon:\n",
    "        if s in numWords:\n",
    "            numWordsPet += numWords[s]\n",
    "    for s in resSpeakersColon:\n",
    "        if s in numWords:\n",
    "            numWordsRes += numWords[s]\n",
    "    return numWordsPet, numWordsRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can see from this illustrative example that the total number of words corresponds to roughly the sum of words\n",
    "# each judge said to each side. There might have been words uttered to speakers neither on petitioner or respondent's\n",
    "# side, or a prologue directed to the general audience (especially for the Chief Justice), which can be ignored for\n",
    "# our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append NEW FEATURES to mergedJudges!!!\n",
    "allNumWordsPet = map(lambda x: featureOne(x)[0], mergedJudges.full_text.values)\n",
    "allNumWordsRes = map(lambda x: featureOne(x)[1], mergedJudges.full_text.values)\n",
    "# mergedJudges[\"allNumWordsPet\"] = allNumWordsPet\n",
    "# mergedJudges[\"allNumWordsRes\"] = allNumWordsRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 1057,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureOne(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Exploring feature #2: Number of interruptions per side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_num_interruptions(text):\n",
    "    '''\n",
    "    FINAL FEATURE # 2 HELPER:\n",
    "        Number of times a judge interrupted petitioner or respondent attorneys in the entire transcript.\n",
    "    Output:\n",
    "        num_interruptions: \n",
    "        a dictionary with key being name of justice/attorney speaking and value being total number of times that \n",
    "        speaker was interrupted\n",
    "    '''\n",
    "    # define the sign for an interruption at end of speech\n",
    "    interruptions = [\"-\", \"--\"]\n",
    "    \n",
    "    # get split argument\n",
    "    split_argument, all_speakers_with_colon, pet_speakers_with_colon, res_speakers_with_colon, \\\n",
    "        justice_speakers_with_colon = getSplitArgument(text)\n",
    "    \n",
    "    #num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    current_speaker = 'NA:'\n",
    "    num_interruptions = dict(zip(all_speakers_with_colon + [current_speaker], [0] * (len(all_speakers_with_colon)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in split_argument:\n",
    "        #if split chunk signifies change in speaker to a justice speaking!\n",
    "        if s in all_speakers_with_colon:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            #if speech contains at least 2 words, check whether the last or second-last contians an interruption\n",
    "            #the interruption could come as the 2nd last word when the last word is part of the first name of\n",
    "            #next speaker\n",
    "            if len(s.split()) >= 2:\n",
    "                if s.split()[-1] in interruptions or s.split()[-2] in interruptions:\n",
    "                    num_interruptions[current_speaker] += 1\n",
    "    \n",
    "    return num_interruptions, pet_speakers_with_colon, res_speakers_with_colon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_interruptions, pet_speakers_with_colon, res_speakers_with_colon = get_num_interruptions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FINAL FEATURE #2 FUNCTION\n",
    "def get_total_interruptions(text):\n",
    "    num_interruptions, pet_speakers_with_colon, res_speakers_with_colon = get_num_interruptions(text)\n",
    "    # now we need to clump petitioner and respondent interruptions together\n",
    "    total_interruptions_pet = sum([num_interruptions[k] for k in pet_speakers_with_colon])\n",
    "    total_interruptions_res = sum([num_interruptions[k] for k in res_speakers_with_colon])\n",
    "    return total_interruptions_pet, total_interruptions_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_total_interruptions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# again, append NEW FEATURES to mergedJudges!!!\n",
    "allInterruptionsPet = map(lambda x: get_total_interruptions(x)[0], mergedJudges.full_text.values)\n",
    "allInterruptionsRes = map(lambda x: get_total_interruptions(x)[1], mergedJudges.full_text.values)\n",
    "mergedJudges[\"allInterruptionsPet\"] = allInterruptionsPet\n",
    "mergedJudges[\"allInterruptionsRes\"] = allInterruptionsRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring feature #3: Sentiment analysis on judge's speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def judgesSpeechSplit(txt):\n",
    "    '''\n",
    "    FINAL FEATURE # 3:\n",
    "        All words said by each justice to petitioners and respondents in the entire transcript.\n",
    "    Output:\n",
    "        num_words: a dictionary with key being name of justice/attorney and value being total number of words they\n",
    "        spoke in total throughout argument.  \n",
    "    '''\n",
    "    # first get the names of the judges and speakers\n",
    "    petSpeakers, resSpeakers, otherSpeakers = getPetitionersAndRespondents(txt)\n",
    "    cleanArg = cleanTextMaker(txt)\n",
    "    justiceSpeakers = getJusticeNames(cleanArg)\n",
    "    \n",
    "    # creates duplicate \"JUSTICE\" and \"CHIEF JUSTICE\"; creates RE for pet/res/justices\n",
    "    justiceRE = generateRES(justiceSpeakers, False)\n",
    "    petRE = generateRES(petSpeakers, True)\n",
    "    resRE = generateRES(resSpeakers, True)\n",
    "    \n",
    "    # appends colons to all REs\n",
    "    justiceSpeakersColon = map(lambda name : name + \":\", justiceRE)\n",
    "    petSpeakersColon = map(lambda name : name + \":\", petRE)\n",
    "    resSpeakersColon = map(lambda name : name + \":\", resRE)\n",
    "\n",
    "    # aggregates justice and attorney REs\n",
    "    allSpeakers = [\"QUESTION\"]\n",
    "    allSpeakers += (justiceRE + petRE + resRE)\n",
    "    allSpeakersColon = map(lambda name : name + \":\", allSpeakers)\n",
    "    \n",
    "    # finally, creates regular expression for the justices and attorneys (i.e. all_speakers) to split text on\n",
    "    RE = '('  + '|'.join(allSpeakersColon) + ')'\n",
    "    \n",
    "    # splits argument portion according to generated regular expression above that consists of all possible speakers,\n",
    "    # plebes and judges alike\n",
    "    splitArg = re.split(RE, cleanArg)\n",
    "    \n",
    "    # num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    prevSpeaker = 'NA:'\n",
    "    currSpeaker = 'NA:'\n",
    "    wordsToPet = dict(zip(allSpeakersColon + [currSpeaker], [\"\"] * (len(allSpeakersColon)+1)))\n",
    "    wordsToRes = dict(zip(allSpeakersColon + [currSpeaker], [\"\"] * (len(allSpeakersColon)+1)))\n",
    "    \n",
    "    # iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in splitArg:\n",
    "        # if split chunk signifies change in speaker to a justice speaking!\n",
    "        if s in allSpeakersColon:\n",
    "            prevSpeaker = currSpeaker\n",
    "            currSpeaker = s\n",
    "        # if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            if currSpeaker in justiceSpeakersColon and prevSpeaker in petSpeakersColon:\n",
    "                wordsToPet[currSpeaker] += s\n",
    "            elif currSpeaker in justiceSpeakersColon and prevSpeaker in resSpeakersColon:\n",
    "                wordsToRes[currSpeaker] += s\n",
    "    \n",
    "    return wordsToPet, wordsToRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineSpeeches(dictionary):\n",
    "    # joins strings that are values of a particular dictionary (want to use on words_to_pet, words_to_res)\n",
    "    return \" \".join(dictionary.values()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureThreeHelper(txt):\n",
    "    # we obtain dictionaries keyed by justice with values equivalent to the stitched together speeches of every time\n",
    "    # that justice spoke up in the court proceedings\n",
    "    wordsToPet,  wordsToRes = judgesSpeechSplit(txt)\n",
    "    wordsToPet = deleteVal(wordsToPet, \"\")\n",
    "    wordsToRes = deleteVal(wordsToRes, \"\")\n",
    "    wordsToPet = combine_speeches(wordsToPet)\n",
    "    wordsToRes = combine_speeches(wordsToRes)\n",
    "    return wordsToPet, wordsToRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsToPet, wordsToRes = judgesSpeechSplit(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the words judges spoke to the petitioner side: \n",
      "\n",
      "Well, did we take this case on the ground that he wasn't adequately advised or did we 1111 14th Street, NW Suite 400 Alderson Reporting Company 1-800-FOR-DEPO Washington, DC 20005 take the case on the ground that even if he were advised, he'd still have his right? MR.  What the Well, I -- I take it you would challenge the validity of the waiver even if he were advised? MR.  Absolutely. And even if he said, I hereby waive? MR.  How -- why is that prejudicial to him? did it but I can't plead guilt\n",
      "-------------------------\n",
      "And here are the words judges spoke to the respondents side: \n",
      "\n",
      "MR.  Because of the error in scoring? MR.  That -- that ultimately You take the position that in fact there was no error in scoring. MR.   Look, the -- imagine -- I'm just repeating what Justice Souter said. it's so obvious that there must be an obvious answer, but I haven't heard the answer. There must be - He knows Michigan law or his lawyer does. Michigan lawyer looks at the statute. who pleads guilty shall not have appellate counsel appointed for review with some exceptions, which they claim\n"
     ]
    }
   ],
   "source": [
    "print \"Here are the words judges spoke to the petitioner side: \\n\"\n",
    "print wordsToPet[:500]\n",
    "print \"-------------------------\"\n",
    "print \"And here are the words judges spoke to the respondents side: \\n\"\n",
    "print wordsToRes[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureThree(txt):\n",
    "    petSAscore = sentimentAnalysis(words_to_pet)\n",
    "    resSAscore = sentimentAnalysis(words_to_res)\n",
    "    return petSAscore, resSAscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Exploring Sentiment Analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pattern Python library with tools for scraping text and natural language processing. In the getParts function, we will implement the parsing of separate petitioners and respondents' text by tokenizing, parsing out the punctuation, removing stop words in English, and finally returning us information about nouns and descriptives that are most salient in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text \n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from hw5! :D\n",
    "def getParts(thetext):\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all judge's speeches to petitioners and respondents from our initial dataframe: mergedJudges\n",
    "allToPetSpeeches = map(lambda t: featureThreeHelper(t)[0], mergedJudges.full_text.values)\n",
    "allToResSpeeches = map(lambda t: featureThreeHelper(t)[1], mergedJudges.full_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653, 653)"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allToPetSpeeches), len(allToResSpeeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 46s, sys: 2.59 s, total: 4min 49s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get parsed speeches for all speeches to petitioners and respondents, separately\n",
    "parsedPet = [getParts(t) for t in allToPetSpeeches]\n",
    "parsedRes = [getParts(t) for t in allToResSpeeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking all the descriptives from the transcript\n",
    "nbDataPet = [each[1] for each in parsedPet]\n",
    "nbDataRes = [each[1] for each in parsedRes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# right now the list is flattened and duplicates are not dropped since we are counting frequency of appearance\n",
    "flattenedPet = map(lambda x: list(itertools.chain(*x)), nbDataPet)\n",
    "flattenedRes = map(lambda x: list(itertools.chain(*x)), nbDataRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# adding the flattened list of descriptives to the dataframe as columns\n",
    "mergedJudges['descriptivesPet'] = pd.Series(flattenedPet, index=mergedJudges.index)\n",
    "mergedJudges['descriptivesRes'] = pd.Series(flattenedRes, index=mergedJudges.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load opinion lexicon\n",
    "posLink = \"idvsus-backup/opinion-lexicon-English/positive-words.txt\"\n",
    "negLink = \"idvsus-backup/opinion-lexicon-English/negative-words.txt\"\n",
    "\n",
    "def getBothList(posLink, negLink):\n",
    "    '''\n",
    "    Inputs:\n",
    "    The links in for the lexicon files downloaded from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "    \n",
    "    Outputs:\n",
    "    Two lists: posList and negList representing lists of positive and negative words appearing in each transcript\n",
    "    '''\n",
    "    posFile = open(posLink, \"r\")\n",
    "    negFile = open(negLink, \"r\")\n",
    "\n",
    "    posList = posFile.read()\n",
    "    negList = negFile.read()\n",
    "    \n",
    "    posList = getPosLexicon(posList)\n",
    "    negList = getNegLexicon(negList)\n",
    "\n",
    "    posList = posList.split('\\n')\n",
    "    negList = negList.split('\\n')\n",
    "\n",
    "    return posList, negList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNegLexicon(text):\n",
    "    '''\n",
    "    Gets negative-sentiment words from the list\n",
    "    '''\n",
    "    # start and end defines bounds of argument portion of text\n",
    "    start = text.find('2-faced')\n",
    "    end = text.rfind('zombie')\n",
    "    return text[start:end]\n",
    "\n",
    "def getPosLexicon(text):\n",
    "    '''\n",
    "    Gets positive-sentiment words from the list.\n",
    "    '''\n",
    "    # start and end defines bounds of argument portion of text\n",
    "    start = text.find('a+')\n",
    "    end = text.rfind('zippy')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get posList and negList for the next step\n",
    "posList, negList = getBothList(posLink, negLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptivesPet = mergedJudges.descriptivesPet.values\n",
    "descriptivesRes = mergedJudges.descriptivesRes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the positive words spoken to petitioners: \n",
      "\n",
      "[u'significant', u'helpful', u'clear', u'articulate', u'best', u'better', u'important', u'articulate', u'intuitive', u'great', u'tough']\n",
      "---------------------\n",
      "These are the negative words spoken to petitioners: \n",
      "\n",
      "[u'reckless', u'culpable', u'drunk', u'forceful', u'violent', u'violent', u'embarrassing', u'violent', u'odd', u'drunk', u'accidental', u'arbitrary', u'bad', u'blind', u'dangerous', u'difficult', u'false', u'unlawful', u'unlawful', u'explosive', u'drunk', u'assault', u'inconsistent', u'blind']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# for example, here are the list of positive and negative words in a randomly selected descriptive list for petitioners\n",
    "d = descriptivesPet[0]\n",
    "pos, neg = splitDescriptivePosNeg(d, posList, negList)\n",
    "print \"These are the positive words spoken to petitioners: \\n\"\n",
    "print pos\n",
    "print \"---------------------\"\n",
    "print \"These are the negative words spoken to petitioners: \\n\"\n",
    "print neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNetPositivity(descriptive, posList, negList):\n",
    "    '''\n",
    "    Inputs:\n",
    "    descriptive: list of salient descriptive words taken from a transcript\n",
    "    posList: list of all positive words from lookup dictionary\n",
    "    negList: list of all negative words from lookup dictionary\n",
    "    \n",
    "    Returns:\n",
    "    net positivity of that descriptive, using sentiment analysis techniques\n",
    "    '''\n",
    "    pos, neg = [], []\n",
    "    for word in descriptive:\n",
    "        if word in posList:\n",
    "            pos.append(word)\n",
    "        elif word in negList:\n",
    "            neg.append(word)\n",
    "    return len(pos) - len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:15: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "netPosPet = map(lambda x: findNetPositivity(x, posList, negList), mergedJudges.descriptivesPet.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:15: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "netPosRes = map(lambda x: findNetPositivity(x, posList, negList), mergedJudges.descriptivesRes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# append netPosPet and netPosRes to merged as a column as NEW FEATURES\n",
    "mergedJudges['netPosPet'] = pd.Series(netPosPet, index=mergedJudges.index)\n",
    "mergedJudges['netPosRes'] = pd.Series(netPosRes, index=mergedJudges.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Running classifier on combined features #1, #2, #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sneak peek at what our completed dataframe looks like now!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combining all features from #1, #2, #3 in a consolidated predictor space\n",
    "X = mergedJudges[[\"netPosPet\", \"netPosRes\", \"allInterruptionsPet\", \"allInterruptionsRes\", \"allNumWordsPet\", \"allNumWordsRes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = mergedJudges[\"partyWinning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome!!! We have all our pain-stakingly selected features in a predictor matrix and our target values - now it's time to run our classifier! Let's simply run Logistic Regression because we are trying to predict binary outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clflog2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain, yTrain, xTest, yTest = splitTrainTest(X, y)\n",
    "clflogopt2 = cv_optimize(clflog, {\"C\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, xTrain, yTrain, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our optimal classifier after cross-validation\n",
    "clflogopt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695059625213 0.636363636364\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = clflogopt2.score(xTrain, yTrain)\n",
    "test_accuracy = clflogopt2.score(xTest, yTest)\n",
    "print training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis on Justice-Centered Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables  \n",
    "* term: This variable identifies the term in which the Court handed down its decision.  \n",
    "* naturalCourt: A natural court is a period during which no personnel change occurs. Scholars have subdivided them into \"strong\" and \"weak\" natural courts, but no convention exists as to the dates on which they begin and end. Options include 1) date of confirmation, 2) date of seating, 3) cases decided after seating, and 4) cases argued and decided after seating.  \n",
    "* petitioner: Petitioner\" refers to the party who petitioned the Supreme Court to review the case. This party is variously known as the petitioner or the appellant.\n",
    "* respondent: Respondent\" refers to the party being sued or tried and is also known as the appellee.\n",
    "* caseOrigin: The focus of this variable is the court in which the case originated,\n",
    "* caseSource: This variable identifies the court whose decision the Supreme Court reviewed. \n",
    "* lcDisposition: This variable specifies the treatment the court whose decision the Supreme Court reviewed accorded the decision of the court it reviewed; e.g., whether the court below the Supreme Court---typically a federal court of appeals or a state supreme court---affirmed, reversed, remanded, etc. the decision of the court it reviewed---typically a trial court. \n",
    "* issueArea: This variable simply separates the issues identified in the preceding variable (issue) into the following larger categories: criminal procedure, civil rights, First Amendment, due process, privacy , attorneys' or governmental officials' fees or compensation, unions, economic activity, judicial power, federalism, interstate relation, federal taxation, miscellaneous, and private law. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_variable(df, target_column):\n",
    "    \"\"\"Encode variable into a number.\n",
    "\n",
    "    Inputs\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    df_mod[target_column] = df_mod[target_column].replace(map_to_int)\n",
    "\n",
    "    return (df_mod, targets)\n",
    "\n",
    "#from graphviz documentation\n",
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62929, 10)\n"
     ]
    }
   ],
   "source": [
    "#create the target, and train\n",
    "#read  the csv and make a big df\n",
    "bigdf=pd.read_csv(\"SCDB_2015_01_justiceCentered_Citation.csv\")\n",
    "\n",
    "#create an empty small df\n",
    "smalldf = pd.DataFrame()\n",
    "\n",
    "#select the variables to run the classifier on \n",
    "#casedisposition is our target\n",
    "train_vars = ['term', 'naturalCourt', 'petitioner',\n",
    "                'respondent', 'caseOrigin', 'caseSource', 'lcDisposition', 'issueArea', 'partyWinning', 'majority']\n",
    "\n",
    "# [\"term\", \"naturalCourt \", \"petitioner\", \"respondent\", \"caseOrigin\", \"caseSource\", \"lcDisposition\", \"issueArea\"]\n",
    "#add train columns to smalldf\n",
    "smalldf = bigdf[train_vars]\n",
    "\n",
    "#drop row if any values are NAN - maxmimum  22% of original data \n",
    "smalldf=smalldf.dropna(axis=0,how='any')\n",
    "\n",
    "print smalldf.shape\n",
    "\n",
    "# smalldf, _ = encode_variable(smalldf,'chief')\n",
    "# smalldf, _ = encode_variable(smalldf, 'dateDecision')\n",
    "\n",
    "# smalldf.majority refers to whether justice voted with the majority (1 for dissent, 2 for majority)\n",
    "# smalldf.partyWinning indicates winning party (0 for responding party, 1 for petitioning party, 2 for unclear)\n",
    "# We use the above 2 features to infer which party the individual justice voted for\n",
    "# NOTE: majority has around 4000 NaNs that we should filter out?\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, x in smalldf.iterrows(): \n",
    "    if x.partyWinning == 2:\n",
    "        results.append(2)\n",
    "        \n",
    "    if x.partyWinning == 1 and x.majority == 2: \n",
    "        results.append(1)\n",
    "        \n",
    "    if x.partyWinning == 0 and x.majority == 2: \n",
    "        results.append(0)\n",
    "        \n",
    "    if x.partyWinning == 1 and x.majority == 1: \n",
    "        results.append(0)\n",
    "        \n",
    "    if x.partyWinning == 0 and x.majority == 1:\n",
    "        results.append(1)\n",
    "\n",
    "smalldf['justiceVote'] = results\n",
    "\n",
    "smalldf['is_train'] = np.random.uniform(0, 1, len(smalldf)) <= .75\n",
    "\n",
    "train, test = smalldf[smalldf['is_train']==True], smalldf[smalldf['is_train']==False]\n",
    "\n",
    "features = list(smalldf.columns[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75429881543752386"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=99)\n",
    "dt.fit(train[features],train[\"justiceVote\"])\n",
    "visualize_tree(dt, features)\n",
    "dt.score(test[features],test['justiceVote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "gs=GridSearchCV(dt, param_grid={'C':Cs}, cv=5)\n",
    "gs.fit(train[features], train['justiceVote'])\n",
    "print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC(loss=\"hinge\")\n",
    "svc_classifier = svc.fit(train[features], train['justiceVote'])\n",
    "print svc_classifier.score(test[features], test['justiceVote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "gs=GridSearchCV(svc, param_grid={'C':Cs}, cv=5)\n",
    "gs.fit(train[features], train['justiceVote'])\n",
    "print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = gs.best_estimator_\n",
    "best.fit(train[features], train['justiceVote'])\n",
    "best.score(test[features], test['justiceVote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic Regression ?? do we want to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a natural first choice for a model since our target value can be viewed as a probability between 0 or 1 for any individual justice to vote For or Against, with a higher probability representing a higher confidence of that justice voting in favor of the arguing party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigdf=pd.read_csv(\"SCDB_2015_01_justiceCentered_Citation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61833, 434)\n",
      "(61833, 8)\n",
      "       C(vote)[1.0]  C(vote)[2.0]  C(vote)[3.0]  C(vote)[4.0]  C(vote)[5.0]  \\\n",
      "0                 0             1             0             0             0   \n",
      "1                 1             0             0             0             0   \n",
      "2                 1             0             0             0             0   \n",
      "3                 0             0             0             1             0   \n",
      "4                 1             0             0             0             0   \n",
      "5                 1             0             0             0             0   \n",
      "6                 1             0             0             0             0   \n",
      "7                 1             0             0             0             0   \n",
      "8                 1             0             0             0             0   \n",
      "9                 1             0             0             0             0   \n",
      "10                0             1             0             0             0   \n",
      "11                1             0             0             0             0   \n",
      "12                1             0             0             0             0   \n",
      "13                1             0             0             0             0   \n",
      "14                0             1             0             0             0   \n",
      "15                0             0             0             1             0   \n",
      "16                0             1             0             0             0   \n",
      "17                1             0             0             0             0   \n",
      "18                0             1             0             0             0   \n",
      "19                1             0             0             0             0   \n",
      "20                0             1             0             0             0   \n",
      "21                0             1             0             0             0   \n",
      "22                0             1             0             0             0   \n",
      "23                1             0             0             0             0   \n",
      "24                1             0             0             0             0   \n",
      "25                1             0             0             0             0   \n",
      "26                1             0             0             0             0   \n",
      "27                0             1             0             0             0   \n",
      "29                1             0             0             0             0   \n",
      "30                1             0             0             0             0   \n",
      "...             ...           ...           ...           ...           ...   \n",
      "77312             1             0             0             0             0   \n",
      "77313             1             0             0             0             0   \n",
      "77314             1             0             0             0             0   \n",
      "77315             1             0             0             0             0   \n",
      "77316             1             0             0             0             0   \n",
      "77317             1             0             0             0             0   \n",
      "77318             1             0             0             0             0   \n",
      "77319             1             0             0             0             0   \n",
      "77320             1             0             0             0             0   \n",
      "77321             1             0             0             0             0   \n",
      "77322             1             0             0             0             0   \n",
      "77323             1             0             0             0             0   \n",
      "77324             1             0             0             0             0   \n",
      "77325             1             0             0             0             0   \n",
      "77326             1             0             0             0             0   \n",
      "77327             1             0             0             0             0   \n",
      "77328             1             0             0             0             0   \n",
      "77329             1             0             0             0             0   \n",
      "77330             1             0             0             0             0   \n",
      "77331             1             0             0             0             0   \n",
      "77332             1             0             0             0             0   \n",
      "77333             1             0             0             0             0   \n",
      "77334             1             0             0             0             0   \n",
      "77335             1             0             0             0             0   \n",
      "77336             1             0             0             0             0   \n",
      "77337             1             0             0             0             0   \n",
      "77338             1             0             0             0             0   \n",
      "77339             1             0             0             0             0   \n",
      "77340             1             0             0             0             0   \n",
      "77341             1             0             0             0             0   \n",
      "\n",
      "       C(vote)[6.0]  C(vote)[7.0]  C(vote)[8.0]  \n",
      "0                 0             0             0  \n",
      "1                 0             0             0  \n",
      "2                 0             0             0  \n",
      "3                 0             0             0  \n",
      "4                 0             0             0  \n",
      "5                 0             0             0  \n",
      "6                 0             0             0  \n",
      "7                 0             0             0  \n",
      "8                 0             0             0  \n",
      "9                 0             0             0  \n",
      "10                0             0             0  \n",
      "11                0             0             0  \n",
      "12                0             0             0  \n",
      "13                0             0             0  \n",
      "14                0             0             0  \n",
      "15                0             0             0  \n",
      "16                0             0             0  \n",
      "17                0             0             0  \n",
      "18                0             0             0  \n",
      "19                0             0             0  \n",
      "20                0             0             0  \n",
      "21                0             0             0  \n",
      "22                0             0             0  \n",
      "23                0             0             0  \n",
      "24                0             0             0  \n",
      "25                0             0             0  \n",
      "26                0             0             0  \n",
      "27                0             0             0  \n",
      "29                0             0             0  \n",
      "30                0             0             0  \n",
      "...             ...           ...           ...  \n",
      "77312             0             0             0  \n",
      "77313             0             0             0  \n",
      "77314             0             0             0  \n",
      "77315             0             0             0  \n",
      "77316             0             0             0  \n",
      "77317             0             0             0  \n",
      "77318             0             0             0  \n",
      "77319             0             0             0  \n",
      "77320             0             0             0  \n",
      "77321             0             0             0  \n",
      "77322             0             0             0  \n",
      "77323             0             0             0  \n",
      "77324             0             0             0  \n",
      "77325             0             0             0  \n",
      "77326             0             0             0  \n",
      "77327             0             0             0  \n",
      "77328             0             0             0  \n",
      "77329             0             0             0  \n",
      "77330             0             0             0  \n",
      "77331             0             0             0  \n",
      "77332             0             0             0  \n",
      "77333             0             0             0  \n",
      "77334             0             0             0  \n",
      "77335             0             0             0  \n",
      "77336             0             0             0  \n",
      "77337             0             0             0  \n",
      "77338             0             0             0  \n",
      "77339             0             0             0  \n",
      "77340             0             0             0  \n",
      "77341             0             0             0  \n",
      "\n",
      "[61833 rows x 8 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape (61833, 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d6f4e2250678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# check the accuracy on the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                              % self.C)\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)\u001b[0m\n\u001b[1;32m    447\u001b[0m                         dtype=None)\n\u001b[1;32m    448\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (61833, 8)"
     ]
    }
   ],
   "source": [
    "from patsy import dmatrices\n",
    "log_model = LogisticRegression(penalty='l2',C=1.0, fit_intercept=True, class_weight='auto')\n",
    "bigdf=pd.read_csv(\"SCDB_2015_01_justiceCentered_Citation.csv\")\n",
    "\n",
    "smalldf = pd.DataFrame()\n",
    "\n",
    "regress_vars = ['issue', 'issueArea', 'decisionDirection', \n",
    "                'dateDecision', 'justice', 'chief', 'caseSource', 'certReason', 'vote'] \n",
    "\n",
    "for i in regress_vars: \n",
    "    smalldf[i] = bigdf[i]\n",
    "    \n",
    "y, X = dmatrices('C(vote) ~ C(issue) + C(issueArea) + C(decisionDirection) + C(justice) + \\\n",
    "                  C(caseSource) + C(certReason)',\n",
    "                  smalldf, return_type=\"dataframe\")\n",
    "\n",
    "print (X.shape)\n",
    "print(y.shape)\n",
    "# y = np.ravel(y)\n",
    "print((y))\n",
    "model = LogisticRegression()\n",
    "\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-586-6d65a5176fa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'issue'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>ledCite</th>\n",
       "      <th>lexisCite</th>\n",
       "      <th>...</th>\n",
       "      <th>authorityDecision1</th>\n",
       "      <th>authorityDecision2</th>\n",
       "      <th>lawType</th>\n",
       "      <th>lawSupp</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946-001</td>\n",
       "      <td>1946-001-01</td>\n",
       "      <td>1946-001-01-01</td>\n",
       "      <td>1946-001-01-01-01</td>\n",
       "      <td>11/18/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 1</td>\n",
       "      <td>67 S. Ct. 6</td>\n",
       "      <td>91 L. Ed. 3</td>\n",
       "      <td>1946 U.S. LEXIS 1724</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>35 U.S.C. � 33</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946-002</td>\n",
       "      <td>1946-002-01</td>\n",
       "      <td>1946-002-01-01</td>\n",
       "      <td>1946-002-01-01-01</td>\n",
       "      <td>11/18/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 14</td>\n",
       "      <td>67 S. Ct. 13</td>\n",
       "      <td>91 L. Ed. 12</td>\n",
       "      <td>1946 U.S. LEXIS 1725</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>18 U.S.C. � 398</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946-003</td>\n",
       "      <td>1946-003-01</td>\n",
       "      <td>1946-003-01-01</td>\n",
       "      <td>1946-003-01-01-01</td>\n",
       "      <td>11/18/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 29</td>\n",
       "      <td>67 S. Ct. 1</td>\n",
       "      <td>91 L. Ed. 22</td>\n",
       "      <td>1946 U.S. LEXIS 3037</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946-004</td>\n",
       "      <td>1946-004-01</td>\n",
       "      <td>1946-004-01-01</td>\n",
       "      <td>1946-004-01-01-01</td>\n",
       "      <td>11/25/46</td>\n",
       "      <td>7</td>\n",
       "      <td>329 U.S. 40</td>\n",
       "      <td>67 S. Ct. 167</td>\n",
       "      <td>91 L. Ed. 29</td>\n",
       "      <td>1946 U.S. LEXIS 1696</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>49 Stat. 801</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946-005</td>\n",
       "      <td>1946-005-01</td>\n",
       "      <td>1946-005-01-01</td>\n",
       "      <td>1946-005-01-01-01</td>\n",
       "      <td>11/25/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 64</td>\n",
       "      <td>67 S. Ct. 154</td>\n",
       "      <td>91 L. Ed. 44</td>\n",
       "      <td>1946 U.S. LEXIS 2997</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1946-006</td>\n",
       "      <td>1946-006-01</td>\n",
       "      <td>1946-006-01-01</td>\n",
       "      <td>1946-006-01-01-01</td>\n",
       "      <td>11/25/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 69</td>\n",
       "      <td>67 S. Ct. 156</td>\n",
       "      <td>91 L. Ed. 80</td>\n",
       "      <td>1946 U.S. LEXIS 3005</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1946-007</td>\n",
       "      <td>1946-007-01</td>\n",
       "      <td>1946-007-01-01</td>\n",
       "      <td>1946-007-01-01-01</td>\n",
       "      <td>11/25/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 90</td>\n",
       "      <td>67 S. Ct. 133</td>\n",
       "      <td>91 L. Ed. 103</td>\n",
       "      <td>1946 U.S. LEXIS 3053</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>15 U.S.C. � 79</td>\n",
       "      <td>82</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1946-008</td>\n",
       "      <td>1946-008-01</td>\n",
       "      <td>1946-008-01-01</td>\n",
       "      <td>1946-008-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 129</td>\n",
       "      <td>67 S. Ct. 231</td>\n",
       "      <td>91 L. Ed. 128</td>\n",
       "      <td>1946 U.S. LEXIS 2995</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>35 U.S.C. � 89</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1946-009</td>\n",
       "      <td>1946-009-01</td>\n",
       "      <td>1946-009-01-01</td>\n",
       "      <td>1946-009-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 143</td>\n",
       "      <td>67 S. Ct. 245</td>\n",
       "      <td>91 L. Ed. 136</td>\n",
       "      <td>1946 U.S. LEXIS 3047</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1946-010</td>\n",
       "      <td>1946-010-01</td>\n",
       "      <td>1946-010-01-01</td>\n",
       "      <td>1946-010-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 156</td>\n",
       "      <td>67 S. Ct. 237</td>\n",
       "      <td>91 L. Ed. 162</td>\n",
       "      <td>1946 U.S. LEXIS 3048</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1946-011</td>\n",
       "      <td>1946-011-01</td>\n",
       "      <td>1946-011-01-01</td>\n",
       "      <td>1946-011-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 173</td>\n",
       "      <td>67 S. Ct. 216</td>\n",
       "      <td>91 L. Ed. 172</td>\n",
       "      <td>1946 U.S. LEXIS 1657</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1946-012</td>\n",
       "      <td>1946-012-01</td>\n",
       "      <td>1946-012-01-01</td>\n",
       "      <td>1946-012-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 187</td>\n",
       "      <td>67 S. Ct. 261</td>\n",
       "      <td>91 L. Ed. 181</td>\n",
       "      <td>1946 U.S. LEXIS 1658</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1946-013</td>\n",
       "      <td>1946-013-01</td>\n",
       "      <td>1946-013-01-01</td>\n",
       "      <td>1946-013-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 207</td>\n",
       "      <td>67 S. Ct. 211</td>\n",
       "      <td>91 L. Ed. 193</td>\n",
       "      <td>1946 U.S. LEXIS 1659</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>50 U.S.C. � 925</td>\n",
       "      <td>81</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1946-014</td>\n",
       "      <td>1946-014-01</td>\n",
       "      <td>1946-014-01-01</td>\n",
       "      <td>1946-014-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 211</td>\n",
       "      <td>67 S. Ct. 224</td>\n",
       "      <td>91 L. Ed. 196</td>\n",
       "      <td>1946 U.S. LEXIS 1660</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>18 U.S.C. � 88</td>\n",
       "      <td>81</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1946-015</td>\n",
       "      <td>1946-015-01</td>\n",
       "      <td>1946-015-01-01</td>\n",
       "      <td>1946-015-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 223</td>\n",
       "      <td>67 S. Ct. 213</td>\n",
       "      <td>91 L. Ed. 204</td>\n",
       "      <td>1946 U.S. LEXIS 3147</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1946-016</td>\n",
       "      <td>1946-016-01</td>\n",
       "      <td>1946-016-01-01</td>\n",
       "      <td>1946-016-01-01-01</td>\n",
       "      <td>12/9/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 230</td>\n",
       "      <td>67 S. Ct. 252</td>\n",
       "      <td>91 L. Ed. 209</td>\n",
       "      <td>1946 U.S. LEXIS 2996</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>40 U.S.C. � 341</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1946-017</td>\n",
       "      <td>1946-017-01</td>\n",
       "      <td>1946-017-01-01</td>\n",
       "      <td>1946-017-01-01-01</td>\n",
       "      <td>12/16/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 249</td>\n",
       "      <td>67 S. Ct. 274</td>\n",
       "      <td>91 L. Ed. 265</td>\n",
       "      <td>1946 U.S. LEXIS 1616</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1946-018</td>\n",
       "      <td>1946-018-01</td>\n",
       "      <td>1946-018-01-01</td>\n",
       "      <td>1946-018-01-01-01</td>\n",
       "      <td>12/16/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 287</td>\n",
       "      <td>67 S. Ct. 207</td>\n",
       "      <td>91 L. Ed. 290</td>\n",
       "      <td>1946 U.S. LEXIS 1617</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>7 U.S.C. � 601</td>\n",
       "      <td>80</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1946-019</td>\n",
       "      <td>1946-019-01</td>\n",
       "      <td>1946-019-01-01</td>\n",
       "      <td>1946-019-01-01-01</td>\n",
       "      <td>12/16/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 296</td>\n",
       "      <td>67 S. Ct. 271</td>\n",
       "      <td>91 L. Ed. 296</td>\n",
       "      <td>1946 U.S. LEXIS 3145</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1946-020</td>\n",
       "      <td>1946-020-01</td>\n",
       "      <td>1946-020-01-01</td>\n",
       "      <td>1946-020-01-01-01</td>\n",
       "      <td>12/23/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 304</td>\n",
       "      <td>67 S. Ct. 313</td>\n",
       "      <td>91 L. Ed. 308</td>\n",
       "      <td>1946 U.S. LEXIS 1582</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1946-021</td>\n",
       "      <td>1946-021-01</td>\n",
       "      <td>1946-021-01-01</td>\n",
       "      <td>1946-021-01-01-01</td>\n",
       "      <td>12/23/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 317</td>\n",
       "      <td>67 S. Ct. 320</td>\n",
       "      <td>91 L. Ed. 318</td>\n",
       "      <td>1946 U.S. LEXIS 1583</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1946-022</td>\n",
       "      <td>1946-022-01</td>\n",
       "      <td>1946-022-01-01</td>\n",
       "      <td>1946-022-01-01-01</td>\n",
       "      <td>12/23/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 324</td>\n",
       "      <td>67 S. Ct. 324</td>\n",
       "      <td>91 L. Ed. 322</td>\n",
       "      <td>1946 U.S. LEXIS 3044</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1946-023</td>\n",
       "      <td>1946-023-01</td>\n",
       "      <td>1946-023-01-01</td>\n",
       "      <td>1946-023-01-01-01</td>\n",
       "      <td>12/23/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 338</td>\n",
       "      <td>67 S. Ct. 301</td>\n",
       "      <td>91 L. Ed. 331</td>\n",
       "      <td>1946 U.S. LEXIS 1584</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1946-024</td>\n",
       "      <td>1946-024-01</td>\n",
       "      <td>1946-024-01-01</td>\n",
       "      <td>1946-024-01-01-01</td>\n",
       "      <td>12/23/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 362</td>\n",
       "      <td>67 S. Ct. 340</td>\n",
       "      <td>91 L. Ed. 348</td>\n",
       "      <td>1946 U.S. LEXIS 3045</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1946-025</td>\n",
       "      <td>1946-025-01</td>\n",
       "      <td>1946-025-01-01</td>\n",
       "      <td>1946-025-01-01-01</td>\n",
       "      <td>12/23/46</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 379</td>\n",
       "      <td>67 S. Ct. 332</td>\n",
       "      <td>91 L. Ed. 359</td>\n",
       "      <td>1946 U.S. LEXIS 1585</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>18 U.S.C. � 413</td>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1946-026</td>\n",
       "      <td>1946-026-01</td>\n",
       "      <td>1946-026-01-01</td>\n",
       "      <td>1946-026-01-01-01</td>\n",
       "      <td>1/6/47</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 394</td>\n",
       "      <td>67 S. Ct. 416</td>\n",
       "      <td>91 L. Ed. 374</td>\n",
       "      <td>1947 U.S. LEXIS 3019</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1946-027</td>\n",
       "      <td>1946-027-01</td>\n",
       "      <td>1946-027-01-01</td>\n",
       "      <td>1946-027-01-01-01</td>\n",
       "      <td>1/6/47</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 402</td>\n",
       "      <td>67 S. Ct. 421</td>\n",
       "      <td>91 L. Ed. 380</td>\n",
       "      <td>1947 U.S. LEXIS 3020</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1946-028</td>\n",
       "      <td>1946-028-01</td>\n",
       "      <td>1946-028-01-01</td>\n",
       "      <td>1946-028-01-01-01</td>\n",
       "      <td>1/6/47</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 416</td>\n",
       "      <td>67 S. Ct. 444</td>\n",
       "      <td>91 L. Ed. 390</td>\n",
       "      <td>1947 U.S. LEXIS 2796</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1946-029</td>\n",
       "      <td>1946-029-01</td>\n",
       "      <td>1946-029-01-01</td>\n",
       "      <td>1946-029-01-01-01</td>\n",
       "      <td>1/6/47</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 424</td>\n",
       "      <td>67 S. Ct. 435</td>\n",
       "      <td>91 L. Ed. 396</td>\n",
       "      <td>1947 U.S. LEXIS 2902</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1946-030</td>\n",
       "      <td>1946-030-01</td>\n",
       "      <td>1946-030-01-01</td>\n",
       "      <td>1946-030-01-01-01</td>\n",
       "      <td>1/6/47</td>\n",
       "      <td>1</td>\n",
       "      <td>329 U.S. 433</td>\n",
       "      <td>67 S. Ct. 439</td>\n",
       "      <td>91 L. Ed. 402</td>\n",
       "      <td>1947 U.S. LEXIS 2797</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1946-071</td>\n",
       "      <td>1946-071-01</td>\n",
       "      <td>1946-071-01-01</td>\n",
       "      <td>1946-071-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 545</td>\n",
       "      <td>67 S. Ct. 883</td>\n",
       "      <td>91 L. Ed. 1088</td>\n",
       "      <td>1947 U.S. LEXIS 2944</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1946-072</td>\n",
       "      <td>1946-072-01</td>\n",
       "      <td>1946-072-01-01</td>\n",
       "      <td>1946-072-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 552</td>\n",
       "      <td>67 S. Ct. 910</td>\n",
       "      <td>91 L. Ed. 1093</td>\n",
       "      <td>1947 U.S. LEXIS 2870</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1946-073</td>\n",
       "      <td>1946-073-01</td>\n",
       "      <td>1946-073-01-01</td>\n",
       "      <td>1946-073-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 567</td>\n",
       "      <td>67 S. Ct. 894</td>\n",
       "      <td>91 L. Ed. 1102</td>\n",
       "      <td>1947 U.S. LEXIS 2871</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1946-074</td>\n",
       "      <td>1946-074-01</td>\n",
       "      <td>1946-074-01-01</td>\n",
       "      <td>1946-074-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 585</td>\n",
       "      <td>67 S. Ct. 918</td>\n",
       "      <td>91 L. Ed. 1117</td>\n",
       "      <td>1947 U.S. LEXIS 2471</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>28 U.S.C. � 385</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1946-075</td>\n",
       "      <td>1946-075-01</td>\n",
       "      <td>1946-075-01-01</td>\n",
       "      <td>1946-075-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 610</td>\n",
       "      <td>67 S. Ct. 903</td>\n",
       "      <td>91 L. Ed. 1133</td>\n",
       "      <td>1947 U.S. LEXIS 2472</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1946-076</td>\n",
       "      <td>1946-076-01</td>\n",
       "      <td>1946-076-01-01</td>\n",
       "      <td>1946-076-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 622</td>\n",
       "      <td>67 S. Ct. 886</td>\n",
       "      <td>91 L. Ed. 1140</td>\n",
       "      <td>1947 U.S. LEXIS 2473</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1946-077</td>\n",
       "      <td>1946-077-01</td>\n",
       "      <td>1946-077-01-01</td>\n",
       "      <td>1946-077-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 631</td>\n",
       "      <td>67 S. Ct. 874</td>\n",
       "      <td>91 L. Ed. 1145</td>\n",
       "      <td>1947 U.S. LEXIS 2474</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1946-078</td>\n",
       "      <td>1946-078-01</td>\n",
       "      <td>1946-078-01-01</td>\n",
       "      <td>1946-078-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 649</td>\n",
       "      <td>67 S. Ct. 931</td>\n",
       "      <td>91 L. Ed. 1158</td>\n",
       "      <td>1947 U.S. LEXIS 2853</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1946-079</td>\n",
       "      <td>1946-079-01</td>\n",
       "      <td>1946-079-01-01</td>\n",
       "      <td>1946-079-01-01-01</td>\n",
       "      <td>3/31/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 695</td>\n",
       "      <td>67 S. Ct. 954</td>\n",
       "      <td>91 L. Ed. 1184</td>\n",
       "      <td>1947 U.S. LEXIS 2892</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1946-080</td>\n",
       "      <td>1946-080-01</td>\n",
       "      <td>1946-080-01-01</td>\n",
       "      <td>1946-080-01-01-01</td>\n",
       "      <td>4/7/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 709</td>\n",
       "      <td>67 S. Ct. 997</td>\n",
       "      <td>91 L. Ed. 1192</td>\n",
       "      <td>1947 U.S. LEXIS 2438</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>56 Stat. 798</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1946-081</td>\n",
       "      <td>1946-081-01</td>\n",
       "      <td>1946-081-01-01</td>\n",
       "      <td>1946-081-01-01-01</td>\n",
       "      <td>4/7/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 724</td>\n",
       "      <td>67 S. Ct. 1004</td>\n",
       "      <td>91 L. Ed. 1204</td>\n",
       "      <td>1947 U.S. LEXIS 2439</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>50 U.S.C. � 1355</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1946-082</td>\n",
       "      <td>1946-082-01</td>\n",
       "      <td>1946-082-01-01</td>\n",
       "      <td>1946-082-01-01-01</td>\n",
       "      <td>4/7/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 731</td>\n",
       "      <td>67 S. Ct. 1009</td>\n",
       "      <td>91 L. Ed. 1209</td>\n",
       "      <td>1947 U.S. LEXIS 2868</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1946-083</td>\n",
       "      <td>1946-083-01</td>\n",
       "      <td>1946-083-01-01</td>\n",
       "      <td>1946-083-01-01-01</td>\n",
       "      <td>4/7/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 743</td>\n",
       "      <td>67 S. Ct. 1015</td>\n",
       "      <td>91 L. Ed. 1219</td>\n",
       "      <td>1947 U.S. LEXIS 2440</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1946-084</td>\n",
       "      <td>1946-084-01</td>\n",
       "      <td>1946-084-01-01</td>\n",
       "      <td>1946-084-01-01-01</td>\n",
       "      <td>4/7/47</td>\n",
       "      <td>1</td>\n",
       "      <td>330 U.S. 767</td>\n",
       "      <td>67 S. Ct. 1026</td>\n",
       "      <td>91 L. Ed. 1234</td>\n",
       "      <td>1947 U.S. LEXIS 2943</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1946-085</td>\n",
       "      <td>1946-085-01</td>\n",
       "      <td>1946-085-01-01</td>\n",
       "      <td>1946-085-01-01-01</td>\n",
       "      <td>4/14/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 1</td>\n",
       "      <td>67 S. Ct. 1047</td>\n",
       "      <td>91 L. Ed. 1301</td>\n",
       "      <td>1947 U.S. LEXIS 3021</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1946-086</td>\n",
       "      <td>1946-086-01</td>\n",
       "      <td>1946-086-01-01</td>\n",
       "      <td>1946-086-01-01-01</td>\n",
       "      <td>4/14/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 17</td>\n",
       "      <td>67 S. Ct. 1056</td>\n",
       "      <td>91 L. Ed. 1312</td>\n",
       "      <td>1947 U.S. LEXIS 2941</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1946-087</td>\n",
       "      <td>1946-087-01</td>\n",
       "      <td>1946-087-01-01</td>\n",
       "      <td>1946-087-01-01-01</td>\n",
       "      <td>4/14/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 28</td>\n",
       "      <td>67 S. Ct. 1041</td>\n",
       "      <td>91 L. Ed. 1320</td>\n",
       "      <td>1947 U.S. LEXIS 2852</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1946-088</td>\n",
       "      <td>1946-088-01</td>\n",
       "      <td>1946-088-01-01</td>\n",
       "      <td>1946-088-01-01-01</td>\n",
       "      <td>4/14/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 40</td>\n",
       "      <td>67 S. Ct. 982</td>\n",
       "      <td>91 L. Ed. 1328</td>\n",
       "      <td>1947 U.S. LEXIS 2942</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>366</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1946-089</td>\n",
       "      <td>1946-089-01</td>\n",
       "      <td>1946-089-01-01</td>\n",
       "      <td>1946-089-01-01-01</td>\n",
       "      <td>4/14/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 70</td>\n",
       "      <td>67 S. Ct. 1062</td>\n",
       "      <td>91 L. Ed. 1346</td>\n",
       "      <td>1947 U.S. LEXIS 2880</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1946-090</td>\n",
       "      <td>1946-090-01</td>\n",
       "      <td>1946-090-01-01</td>\n",
       "      <td>1946-090-01-01-01</td>\n",
       "      <td>4/28/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 96</td>\n",
       "      <td>67 S. Ct. 1165</td>\n",
       "      <td>91 L. Ed. 1365</td>\n",
       "      <td>1947 U.S. LEXIS 2996</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>28 U.S.C. � 227</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1946-091</td>\n",
       "      <td>1946-091-01</td>\n",
       "      <td>1946-091-01-01</td>\n",
       "      <td>1946-091-01-01-01</td>\n",
       "      <td>4/28/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 100</td>\n",
       "      <td>67 S. Ct. 1140</td>\n",
       "      <td>91 L. Ed. 1368</td>\n",
       "      <td>1947 U.S. LEXIS 2357</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>28 U.S.C. � 401</td>\n",
       "      <td>79</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1946-092</td>\n",
       "      <td>1946-092-01</td>\n",
       "      <td>1946-092-01-01</td>\n",
       "      <td>1946-092-01-01-01</td>\n",
       "      <td>4/28/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 111</td>\n",
       "      <td>67 S. Ct. 1129</td>\n",
       "      <td>91 L. Ed. 1375</td>\n",
       "      <td>1947 U.S. LEXIS 2851</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1946-093</td>\n",
       "      <td>1946-093-01</td>\n",
       "      <td>1946-093-01-01</td>\n",
       "      <td>1946-093-01-01-01</td>\n",
       "      <td>4/28/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 125</td>\n",
       "      <td>67 S. Ct. 1136</td>\n",
       "      <td>91 L. Ed. 1386</td>\n",
       "      <td>1947 U.S. LEXIS 2997</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>15 U.S.C. � 99</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1946-094</td>\n",
       "      <td>1946-094-01</td>\n",
       "      <td>1946-094-01-01</td>\n",
       "      <td>1946-094-01-01-01</td>\n",
       "      <td>4/28/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 132</td>\n",
       "      <td>67 S. Ct. 1168</td>\n",
       "      <td>91 L. Ed. 1391</td>\n",
       "      <td>1947 U.S. LEXIS 2899</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>28 U.S.C. � 380</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1946-095</td>\n",
       "      <td>1946-095-01</td>\n",
       "      <td>1946-095-01-01</td>\n",
       "      <td>1946-095-01-01-01</td>\n",
       "      <td>5/5/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 145</td>\n",
       "      <td>67 S. Ct. 1098</td>\n",
       "      <td>91 L. Ed. 1399</td>\n",
       "      <td>1947 U.S. LEXIS 2936</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1946-096</td>\n",
       "      <td>1946-096-01</td>\n",
       "      <td>1946-096-01-01</td>\n",
       "      <td>1946-096-01-01-01</td>\n",
       "      <td>5/5/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 199</td>\n",
       "      <td>67 S. Ct. 1178</td>\n",
       "      <td>91 L. Ed. 1432</td>\n",
       "      <td>1947 U.S. LEXIS 2937</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1946-097</td>\n",
       "      <td>1946-097-01</td>\n",
       "      <td>1946-097-01-01</td>\n",
       "      <td>1946-097-01-01-01</td>\n",
       "      <td>5/5/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 210</td>\n",
       "      <td>67 S. Ct. 1175</td>\n",
       "      <td>91 L. Ed. 1441</td>\n",
       "      <td>1947 U.S. LEXIS 2994</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1946-098</td>\n",
       "      <td>1946-098-01</td>\n",
       "      <td>1946-098-01-01</td>\n",
       "      <td>1946-098-01-01-01</td>\n",
       "      <td>5/5/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 218</td>\n",
       "      <td>67 S. Ct. 1146</td>\n",
       "      <td>91 L. Ed. 1447</td>\n",
       "      <td>1947 U.S. LEXIS 2938</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>7 U.S.C. � 241</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1946-099</td>\n",
       "      <td>1946-099-01</td>\n",
       "      <td>1946-099-01-01</td>\n",
       "      <td>1946-099-01-01-01</td>\n",
       "      <td>5/5/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 247</td>\n",
       "      <td>67 S. Ct. 1160</td>\n",
       "      <td>91 L. Ed. 1468</td>\n",
       "      <td>1947 U.S. LEXIS 2319</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>7 U.S.C. � 1</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1946-100</td>\n",
       "      <td>1946-100-01</td>\n",
       "      <td>1946-100-01-01</td>\n",
       "      <td>1946-100-01-01-01</td>\n",
       "      <td>5/12/47</td>\n",
       "      <td>1</td>\n",
       "      <td>331 U.S. 256</td>\n",
       "      <td>67 S. Ct. 1287</td>\n",
       "      <td>91 L. Ed. 1474</td>\n",
       "      <td>1947 U.S. LEXIS 2867</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      caseId     docketId    caseIssuesId             voteId dateDecision  \\\n",
       "0   1946-001  1946-001-01  1946-001-01-01  1946-001-01-01-01     11/18/46   \n",
       "1   1946-002  1946-002-01  1946-002-01-01  1946-002-01-01-01     11/18/46   \n",
       "2   1946-003  1946-003-01  1946-003-01-01  1946-003-01-01-01     11/18/46   \n",
       "3   1946-004  1946-004-01  1946-004-01-01  1946-004-01-01-01     11/25/46   \n",
       "4   1946-005  1946-005-01  1946-005-01-01  1946-005-01-01-01     11/25/46   \n",
       "5   1946-006  1946-006-01  1946-006-01-01  1946-006-01-01-01     11/25/46   \n",
       "6   1946-007  1946-007-01  1946-007-01-01  1946-007-01-01-01     11/25/46   \n",
       "7   1946-008  1946-008-01  1946-008-01-01  1946-008-01-01-01      12/9/46   \n",
       "8   1946-009  1946-009-01  1946-009-01-01  1946-009-01-01-01      12/9/46   \n",
       "9   1946-010  1946-010-01  1946-010-01-01  1946-010-01-01-01      12/9/46   \n",
       "10  1946-011  1946-011-01  1946-011-01-01  1946-011-01-01-01      12/9/46   \n",
       "11  1946-012  1946-012-01  1946-012-01-01  1946-012-01-01-01      12/9/46   \n",
       "12  1946-013  1946-013-01  1946-013-01-01  1946-013-01-01-01      12/9/46   \n",
       "13  1946-014  1946-014-01  1946-014-01-01  1946-014-01-01-01      12/9/46   \n",
       "14  1946-015  1946-015-01  1946-015-01-01  1946-015-01-01-01      12/9/46   \n",
       "15  1946-016  1946-016-01  1946-016-01-01  1946-016-01-01-01      12/9/46   \n",
       "16  1946-017  1946-017-01  1946-017-01-01  1946-017-01-01-01     12/16/46   \n",
       "17  1946-018  1946-018-01  1946-018-01-01  1946-018-01-01-01     12/16/46   \n",
       "18  1946-019  1946-019-01  1946-019-01-01  1946-019-01-01-01     12/16/46   \n",
       "19  1946-020  1946-020-01  1946-020-01-01  1946-020-01-01-01     12/23/46   \n",
       "20  1946-021  1946-021-01  1946-021-01-01  1946-021-01-01-01     12/23/46   \n",
       "21  1946-022  1946-022-01  1946-022-01-01  1946-022-01-01-01     12/23/46   \n",
       "22  1946-023  1946-023-01  1946-023-01-01  1946-023-01-01-01     12/23/46   \n",
       "23  1946-024  1946-024-01  1946-024-01-01  1946-024-01-01-01     12/23/46   \n",
       "24  1946-025  1946-025-01  1946-025-01-01  1946-025-01-01-01     12/23/46   \n",
       "25  1946-026  1946-026-01  1946-026-01-01  1946-026-01-01-01       1/6/47   \n",
       "26  1946-027  1946-027-01  1946-027-01-01  1946-027-01-01-01       1/6/47   \n",
       "27  1946-028  1946-028-01  1946-028-01-01  1946-028-01-01-01       1/6/47   \n",
       "28  1946-029  1946-029-01  1946-029-01-01  1946-029-01-01-01       1/6/47   \n",
       "29  1946-030  1946-030-01  1946-030-01-01  1946-030-01-01-01       1/6/47   \n",
       "..       ...          ...             ...                ...          ...   \n",
       "70  1946-071  1946-071-01  1946-071-01-01  1946-071-01-01-01      3/31/47   \n",
       "71  1946-072  1946-072-01  1946-072-01-01  1946-072-01-01-01      3/31/47   \n",
       "72  1946-073  1946-073-01  1946-073-01-01  1946-073-01-01-01      3/31/47   \n",
       "73  1946-074  1946-074-01  1946-074-01-01  1946-074-01-01-01      3/31/47   \n",
       "74  1946-075  1946-075-01  1946-075-01-01  1946-075-01-01-01      3/31/47   \n",
       "75  1946-076  1946-076-01  1946-076-01-01  1946-076-01-01-01      3/31/47   \n",
       "76  1946-077  1946-077-01  1946-077-01-01  1946-077-01-01-01      3/31/47   \n",
       "77  1946-078  1946-078-01  1946-078-01-01  1946-078-01-01-01      3/31/47   \n",
       "78  1946-079  1946-079-01  1946-079-01-01  1946-079-01-01-01      3/31/47   \n",
       "79  1946-080  1946-080-01  1946-080-01-01  1946-080-01-01-01       4/7/47   \n",
       "80  1946-081  1946-081-01  1946-081-01-01  1946-081-01-01-01       4/7/47   \n",
       "81  1946-082  1946-082-01  1946-082-01-01  1946-082-01-01-01       4/7/47   \n",
       "82  1946-083  1946-083-01  1946-083-01-01  1946-083-01-01-01       4/7/47   \n",
       "83  1946-084  1946-084-01  1946-084-01-01  1946-084-01-01-01       4/7/47   \n",
       "84  1946-085  1946-085-01  1946-085-01-01  1946-085-01-01-01      4/14/47   \n",
       "85  1946-086  1946-086-01  1946-086-01-01  1946-086-01-01-01      4/14/47   \n",
       "86  1946-087  1946-087-01  1946-087-01-01  1946-087-01-01-01      4/14/47   \n",
       "87  1946-088  1946-088-01  1946-088-01-01  1946-088-01-01-01      4/14/47   \n",
       "88  1946-089  1946-089-01  1946-089-01-01  1946-089-01-01-01      4/14/47   \n",
       "89  1946-090  1946-090-01  1946-090-01-01  1946-090-01-01-01      4/28/47   \n",
       "90  1946-091  1946-091-01  1946-091-01-01  1946-091-01-01-01      4/28/47   \n",
       "91  1946-092  1946-092-01  1946-092-01-01  1946-092-01-01-01      4/28/47   \n",
       "92  1946-093  1946-093-01  1946-093-01-01  1946-093-01-01-01      4/28/47   \n",
       "93  1946-094  1946-094-01  1946-094-01-01  1946-094-01-01-01      4/28/47   \n",
       "94  1946-095  1946-095-01  1946-095-01-01  1946-095-01-01-01       5/5/47   \n",
       "95  1946-096  1946-096-01  1946-096-01-01  1946-096-01-01-01       5/5/47   \n",
       "96  1946-097  1946-097-01  1946-097-01-01  1946-097-01-01-01       5/5/47   \n",
       "97  1946-098  1946-098-01  1946-098-01-01  1946-098-01-01-01       5/5/47   \n",
       "98  1946-099  1946-099-01  1946-099-01-01  1946-099-01-01-01       5/5/47   \n",
       "99  1946-100  1946-100-01  1946-100-01-01  1946-100-01-01-01      5/12/47   \n",
       "\n",
       "    decisionType        usCite         sctCite         ledCite  \\\n",
       "0              1    329 U.S. 1     67 S. Ct. 6     91 L. Ed. 3   \n",
       "1              1   329 U.S. 14    67 S. Ct. 13    91 L. Ed. 12   \n",
       "2              1   329 U.S. 29     67 S. Ct. 1    91 L. Ed. 22   \n",
       "3              7   329 U.S. 40   67 S. Ct. 167    91 L. Ed. 29   \n",
       "4              1   329 U.S. 64   67 S. Ct. 154    91 L. Ed. 44   \n",
       "5              1   329 U.S. 69   67 S. Ct. 156    91 L. Ed. 80   \n",
       "6              1   329 U.S. 90   67 S. Ct. 133   91 L. Ed. 103   \n",
       "7              1  329 U.S. 129   67 S. Ct. 231   91 L. Ed. 128   \n",
       "8              1  329 U.S. 143   67 S. Ct. 245   91 L. Ed. 136   \n",
       "9              1  329 U.S. 156   67 S. Ct. 237   91 L. Ed. 162   \n",
       "10             1  329 U.S. 173   67 S. Ct. 216   91 L. Ed. 172   \n",
       "11             1  329 U.S. 187   67 S. Ct. 261   91 L. Ed. 181   \n",
       "12             1  329 U.S. 207   67 S. Ct. 211   91 L. Ed. 193   \n",
       "13             1  329 U.S. 211   67 S. Ct. 224   91 L. Ed. 196   \n",
       "14             1  329 U.S. 223   67 S. Ct. 213   91 L. Ed. 204   \n",
       "15             1  329 U.S. 230   67 S. Ct. 252   91 L. Ed. 209   \n",
       "16             1  329 U.S. 249   67 S. Ct. 274   91 L. Ed. 265   \n",
       "17             1  329 U.S. 287   67 S. Ct. 207   91 L. Ed. 290   \n",
       "18             1  329 U.S. 296   67 S. Ct. 271   91 L. Ed. 296   \n",
       "19             1  329 U.S. 304   67 S. Ct. 313   91 L. Ed. 308   \n",
       "20             1  329 U.S. 317   67 S. Ct. 320   91 L. Ed. 318   \n",
       "21             1  329 U.S. 324   67 S. Ct. 324   91 L. Ed. 322   \n",
       "22             1  329 U.S. 338   67 S. Ct. 301   91 L. Ed. 331   \n",
       "23             1  329 U.S. 362   67 S. Ct. 340   91 L. Ed. 348   \n",
       "24             1  329 U.S. 379   67 S. Ct. 332   91 L. Ed. 359   \n",
       "25             1  329 U.S. 394   67 S. Ct. 416   91 L. Ed. 374   \n",
       "26             1  329 U.S. 402   67 S. Ct. 421   91 L. Ed. 380   \n",
       "27             1  329 U.S. 416   67 S. Ct. 444   91 L. Ed. 390   \n",
       "28             1  329 U.S. 424   67 S. Ct. 435   91 L. Ed. 396   \n",
       "29             1  329 U.S. 433   67 S. Ct. 439   91 L. Ed. 402   \n",
       "..           ...           ...             ...             ...   \n",
       "70             1  330 U.S. 545   67 S. Ct. 883  91 L. Ed. 1088   \n",
       "71             1  330 U.S. 552   67 S. Ct. 910  91 L. Ed. 1093   \n",
       "72             1  330 U.S. 567   67 S. Ct. 894  91 L. Ed. 1102   \n",
       "73             1  330 U.S. 585   67 S. Ct. 918  91 L. Ed. 1117   \n",
       "74             1  330 U.S. 610   67 S. Ct. 903  91 L. Ed. 1133   \n",
       "75             1  330 U.S. 622   67 S. Ct. 886  91 L. Ed. 1140   \n",
       "76             1  330 U.S. 631   67 S. Ct. 874  91 L. Ed. 1145   \n",
       "77             1  330 U.S. 649   67 S. Ct. 931  91 L. Ed. 1158   \n",
       "78             1  330 U.S. 695   67 S. Ct. 954  91 L. Ed. 1184   \n",
       "79             1  330 U.S. 709   67 S. Ct. 997  91 L. Ed. 1192   \n",
       "80             1  330 U.S. 724  67 S. Ct. 1004  91 L. Ed. 1204   \n",
       "81             1  330 U.S. 731  67 S. Ct. 1009  91 L. Ed. 1209   \n",
       "82             1  330 U.S. 743  67 S. Ct. 1015  91 L. Ed. 1219   \n",
       "83             1  330 U.S. 767  67 S. Ct. 1026  91 L. Ed. 1234   \n",
       "84             1    331 U.S. 1  67 S. Ct. 1047  91 L. Ed. 1301   \n",
       "85             1   331 U.S. 17  67 S. Ct. 1056  91 L. Ed. 1312   \n",
       "86             1   331 U.S. 28  67 S. Ct. 1041  91 L. Ed. 1320   \n",
       "87             1   331 U.S. 40   67 S. Ct. 982  91 L. Ed. 1328   \n",
       "88             1   331 U.S. 70  67 S. Ct. 1062  91 L. Ed. 1346   \n",
       "89             1   331 U.S. 96  67 S. Ct. 1165  91 L. Ed. 1365   \n",
       "90             1  331 U.S. 100  67 S. Ct. 1140  91 L. Ed. 1368   \n",
       "91             1  331 U.S. 111  67 S. Ct. 1129  91 L. Ed. 1375   \n",
       "92             1  331 U.S. 125  67 S. Ct. 1136  91 L. Ed. 1386   \n",
       "93             1  331 U.S. 132  67 S. Ct. 1168  91 L. Ed. 1391   \n",
       "94             1  331 U.S. 145  67 S. Ct. 1098  91 L. Ed. 1399   \n",
       "95             1  331 U.S. 199  67 S. Ct. 1178  91 L. Ed. 1432   \n",
       "96             1  331 U.S. 210  67 S. Ct. 1175  91 L. Ed. 1441   \n",
       "97             1  331 U.S. 218  67 S. Ct. 1146  91 L. Ed. 1447   \n",
       "98             1  331 U.S. 247  67 S. Ct. 1160  91 L. Ed. 1468   \n",
       "99             1  331 U.S. 256  67 S. Ct. 1287  91 L. Ed. 1474   \n",
       "\n",
       "               lexisCite    ...     authorityDecision1  authorityDecision2  \\\n",
       "0   1946 U.S. LEXIS 1724    ...                      4                 NaN   \n",
       "1   1946 U.S. LEXIS 1725    ...                      4                 NaN   \n",
       "2   1946 U.S. LEXIS 3037    ...                      1                 NaN   \n",
       "3   1946 U.S. LEXIS 1696    ...                      4                 NaN   \n",
       "4   1946 U.S. LEXIS 2997    ...                      7                 NaN   \n",
       "5   1946 U.S. LEXIS 3005    ...                      2                 NaN   \n",
       "6   1946 U.S. LEXIS 3053    ...                      1                 NaN   \n",
       "7   1946 U.S. LEXIS 2995    ...                      4                 NaN   \n",
       "8   1946 U.S. LEXIS 3047    ...                      4                   5   \n",
       "9   1946 U.S. LEXIS 3048    ...                      4                 NaN   \n",
       "10  1946 U.S. LEXIS 1657    ...                      2                 NaN   \n",
       "11  1946 U.S. LEXIS 1658    ...                      3                 NaN   \n",
       "12  1946 U.S. LEXIS 1659    ...                      4                 NaN   \n",
       "13  1946 U.S. LEXIS 1660    ...                      4                 NaN   \n",
       "14  1946 U.S. LEXIS 3147    ...                      4                   5   \n",
       "15  1946 U.S. LEXIS 2996    ...                      4                 NaN   \n",
       "16  1946 U.S. LEXIS 1616    ...                      2                 NaN   \n",
       "17  1946 U.S. LEXIS 1617    ...                      5                   4   \n",
       "18  1946 U.S. LEXIS 3145    ...                      7                 NaN   \n",
       "19  1946 U.S. LEXIS 1582    ...                      5                   4   \n",
       "20  1946 U.S. LEXIS 1583    ...                      5                   4   \n",
       "21  1946 U.S. LEXIS 3044    ...                      5                   4   \n",
       "22  1946 U.S. LEXIS 1584    ...                      4                 NaN   \n",
       "23  1946 U.S. LEXIS 3045    ...                      4                 NaN   \n",
       "24  1946 U.S. LEXIS 1585    ...                      4                 NaN   \n",
       "25  1947 U.S. LEXIS 3019    ...                      7                 NaN   \n",
       "26  1947 U.S. LEXIS 3020    ...                      7                 NaN   \n",
       "27  1947 U.S. LEXIS 2796    ...                      2                 NaN   \n",
       "28  1947 U.S. LEXIS 2902    ...                      4                   5   \n",
       "29  1947 U.S. LEXIS 2797    ...                      6                 NaN   \n",
       "..                   ...    ...                    ...                 ...   \n",
       "70  1947 U.S. LEXIS 2944    ...                      4                 NaN   \n",
       "71  1947 U.S. LEXIS 2870    ...                      2                 NaN   \n",
       "72  1947 U.S. LEXIS 2871    ...                      5                   4   \n",
       "73  1947 U.S. LEXIS 2471    ...                      4                 NaN   \n",
       "74  1947 U.S. LEXIS 2472    ...                      2                 NaN   \n",
       "75  1947 U.S. LEXIS 2473    ...                      2                 NaN   \n",
       "76  1947 U.S. LEXIS 2474    ...                      1                 NaN   \n",
       "77  1947 U.S. LEXIS 2853    ...                      4                 NaN   \n",
       "78  1947 U.S. LEXIS 2892    ...                      4                 NaN   \n",
       "79  1947 U.S. LEXIS 2438    ...                      4                 NaN   \n",
       "80  1947 U.S. LEXIS 2439    ...                      4                 NaN   \n",
       "81  1947 U.S. LEXIS 2868    ...                      4                 NaN   \n",
       "82  1947 U.S. LEXIS 2440    ...                      4                 NaN   \n",
       "83  1947 U.S. LEXIS 2943    ...                      4                 NaN   \n",
       "84  1947 U.S. LEXIS 3021    ...                      4                 NaN   \n",
       "85  1947 U.S. LEXIS 2941    ...                      4                 NaN   \n",
       "86  1947 U.S. LEXIS 2852    ...                      4                 NaN   \n",
       "87  1947 U.S. LEXIS 2942    ...                      4                 NaN   \n",
       "88  1947 U.S. LEXIS 2880    ...                      2                 NaN   \n",
       "89  1947 U.S. LEXIS 2996    ...                      4                 NaN   \n",
       "90  1947 U.S. LEXIS 2357    ...                      4                 NaN   \n",
       "91  1947 U.S. LEXIS 2851    ...                      4                 NaN   \n",
       "92  1947 U.S. LEXIS 2997    ...                      4                 NaN   \n",
       "93  1947 U.S. LEXIS 2899    ...                      4                 NaN   \n",
       "94  1947 U.S. LEXIS 2936    ...                      1                 NaN   \n",
       "95  1947 U.S. LEXIS 2937    ...                      4                 NaN   \n",
       "96  1947 U.S. LEXIS 2994    ...                      4                 NaN   \n",
       "97  1947 U.S. LEXIS 2938    ...                      2                 NaN   \n",
       "98  1947 U.S. LEXIS 2319    ...                      4                 NaN   \n",
       "99  1947 U.S. LEXIS 2867    ...                      4                 NaN   \n",
       "\n",
       "   lawType lawSupp          lawMinor majOpinWriter majOpinAssigner  splitVote  \\\n",
       "0        6     600    35 U.S.C. � 33            78              78          1   \n",
       "1        6     600   18 U.S.C. � 398            81              87          1   \n",
       "2        2     207               NaN            84              78          1   \n",
       "3        6     600      49 Stat. 801            87              87          1   \n",
       "4      NaN     NaN               NaN            78              87          1   \n",
       "5        1     129               NaN            81              87          1   \n",
       "6        6     600    15 U.S.C. � 79            82              74          1   \n",
       "7        6     600    35 U.S.C. � 89            87              87          1   \n",
       "8        5     512               NaN            87              87          1   \n",
       "9        3     307               NaN            78              87          1   \n",
       "10       2     214               NaN            80              87          1   \n",
       "11     NaN     NaN               NaN            81              78          1   \n",
       "12       6     600   50 U.S.C. � 925            81              78          1   \n",
       "13       6     600    18 U.S.C. � 88            81              78          1   \n",
       "14       3     326               NaN            84              87          1   \n",
       "15       6     600   40 U.S.C. � 341            86              87          1   \n",
       "16       1     111               NaN            80              74          1   \n",
       "17       6     600    7 U.S.C. � 601            80              87          1   \n",
       "18     NaN     NaN               NaN            84              87          1   \n",
       "19       3     366               NaN            81              87          1   \n",
       "20       3     366               NaN            81              87          1   \n",
       "21       3     356               NaN            82              87          1   \n",
       "22       3     366               NaN            85              78          1   \n",
       "23       3     307               NaN            85              87          1   \n",
       "24       6     600   18 U.S.C. � 413            85              78          1   \n",
       "25       5     507               NaN            78              87          1   \n",
       "26       5     507               NaN            78              87          1   \n",
       "27       2     230               NaN            78              87          1   \n",
       "28       3     343               NaN            78              87          1   \n",
       "29     NaN     NaN               NaN            78              87          1   \n",
       "..     ...     ...               ...           ...             ...        ...   \n",
       "70       3     334               NaN            87              87          1   \n",
       "71       2     231               NaN            78              87          1   \n",
       "72       3     343               NaN            78              87          1   \n",
       "73       6     600   28 U.S.C. � 385            81              87          1   \n",
       "74       1     143               NaN            81              78          1   \n",
       "75       1     143               NaN            82              87          1   \n",
       "76       1     142               NaN            84              87          1   \n",
       "77       3     334               NaN            86              87          1   \n",
       "78       3     352               NaN            86              87          1   \n",
       "79       6     600      56 Stat. 798            78              87          1   \n",
       "80       6     600  50 U.S.C. � 1355            78              78          1   \n",
       "81       4     400               NaN            81              87          1   \n",
       "82       3     364               NaN            84              87          1   \n",
       "83       3     356               NaN            84              87          1   \n",
       "84       3     345               NaN            87              87          1   \n",
       "85       3     334               NaN            87              87          1   \n",
       "86       3     307               NaN            82              87          1   \n",
       "87       3     366               NaN            85              87          1   \n",
       "88       2     231               NaN            85              78          1   \n",
       "89       6     600   28 U.S.C. � 227            78              78          1   \n",
       "90       6     600   28 U.S.C. � 401            79              87          1   \n",
       "91       4     400               NaN            81              87          1   \n",
       "92       6     600    15 U.S.C. � 99            81              87          1   \n",
       "93       6     600   28 U.S.C. � 380            82              87          1   \n",
       "94       2     205               NaN            87              87          1   \n",
       "95       3     334               NaN            87              87          1   \n",
       "96       3     345               NaN            78              87          1   \n",
       "97       6     600    7 U.S.C. � 241            81              87          1   \n",
       "98       6     600      7 U.S.C. � 1            81              87          1   \n",
       "99       5     512               NaN            79              79          1   \n",
       "\n",
       "    majVotes  minVotes  \n",
       "0          8         1  \n",
       "1          6         3  \n",
       "2          5         4  \n",
       "3          5         3  \n",
       "4          6         3  \n",
       "5          7         1  \n",
       "6          6         0  \n",
       "7          9         0  \n",
       "8          9         0  \n",
       "9          8         0  \n",
       "10         5         4  \n",
       "11         6         3  \n",
       "12         9         0  \n",
       "13         9         0  \n",
       "14         8         0  \n",
       "15         9         0  \n",
       "16         6         3  \n",
       "17         9         0  \n",
       "18         6         3  \n",
       "19         9         0  \n",
       "20         9         0  \n",
       "21         8         1  \n",
       "22         9         0  \n",
       "23         7         2  \n",
       "24         7         2  \n",
       "25         5         4  \n",
       "26         5         4  \n",
       "27         9         0  \n",
       "28         9         0  \n",
       "29         9         0  \n",
       "..       ...       ...  \n",
       "70         6         3  \n",
       "71         5         4  \n",
       "72         7         2  \n",
       "73         7         2  \n",
       "74         9         0  \n",
       "75         9         0  \n",
       "76         8         1  \n",
       "77         6         3  \n",
       "78         9         0  \n",
       "79         7         2  \n",
       "80         9         0  \n",
       "81         8         0  \n",
       "82         5         4  \n",
       "83         9         0  \n",
       "84         6         3  \n",
       "85         7         2  \n",
       "86         8         1  \n",
       "87         7         2  \n",
       "88         7         2  \n",
       "89         9         0  \n",
       "90         8         1  \n",
       "91         9         0  \n",
       "92         9         0  \n",
       "93         8         1  \n",
       "94         5         4  \n",
       "95         9         0  \n",
       "96         9         0  \n",
       "97         7         2  \n",
       "98         9         0  \n",
       "99         5         4  \n",
       "\n",
       "[100 rows x 53 columns]"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in SCDB data from file\n",
    "bigdf=pd.read_csv(\"supremeCourtDb.csv\")\n",
    "bigdf[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('docketId', 'dateDecision', 'case')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-045043c2ac69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"docketId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dateDecision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"case\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1912\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1919\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3102\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3103\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/main/anaconda/lib/python2.7/site-packages/pandas/core/index.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1690\u001b[0m                 raise ValueError('tolerance argument only valid if using pad, '\n\u001b[1;32m   1691\u001b[0m                                  'backfill or nearest lookups')\n\u001b[0;32m-> 1692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m         indexer = self.get_indexer([key], method=method,\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3979)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3843)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12265)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12216)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('docketId', 'dateDecision', 'case')"
     ]
    }
   ],
   "source": [
    "df = bigdf[\"docketId\", \"dateDecision\", \"case\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C=1.0, kernel='linear', probability=True, class_weight='auto')\n",
    "svm_model = my_svm.fit(X, y)\n",
    "svm_pred = svm_fit.predict(W)\n",
    "# Class probabilities, based on log regression on distance to hyperplane.\n",
    "svm_prob = svm_fit.predict_proba(W)\n",
    "svm_dist = svm_fit.decision_function(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Justice Ruling Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a different dataset in a slightly different approach to making Supreme Court ruling predictions. This method is motivated by the fact that usually, only 2 justices tend to be swing votes and justice decisions are highly influenced by factors outside of what transpires in court proceedings, such as background information about the case itself. The Supreme Court website contains a Justice-centered database which contains extensive information about each case; in particular, the most pertinent fields we are interested in analyzing are:\n",
    "\n",
    "1. Decision Year\n",
    "2. Natural Court\n",
    "3. Petitioner\n",
    "4. Respondent\n",
    "5. Case Origin\n",
    "6. Case Source\n",
    "7. Lower Court Disposition Direction\n",
    "8. Issue Area\n",
    "\n",
    "Our target value to predict is the field called winningParty (petitioner or respondent), which using our justice-centered approach involves aggregating predicted votes for each individual justice and taking majority vote. The associated confidence of our entire prediction is obtained by averaging individual confidences of our models for each justice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in justice-centered SCDB data from file\n",
    "newdf=pd.read_csv(\"SCDB_justice_centered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2853c809a88f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# maybe lcDispositionDirection? choose features with continuous/numerical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# do the numbers mean anything though?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnewsmalldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"term\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"naturalCourt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"petitioner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"respondent\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"caseOrigin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"caseSource\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lcDisposition\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"issueArea\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsmalldf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'newdf' is not defined"
     ]
    }
   ],
   "source": [
    "# maybe lcDispositionDirection? choose features with continuous/numerical features\n",
    "# do the numbers mean anything though?\n",
    "newsmalldf = newdf[[\"term\", \"naturalCourt\", \"petitioner\", \"respondent\", \"caseOrigin\", \"caseSource\", \"lcDisposition\", \"issueArea\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>naturalCourt</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>caseOrigin</th>\n",
       "      <th>caseSource</th>\n",
       "      <th>lcDisposition</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   term  naturalCourt  petitioner  respondent  caseOrigin  caseSource  \\\n",
       "0  1946          1301         198         172          51          29   \n",
       "1  1946          1301         198         172          51          29   \n",
       "2  1946          1301         198         172          51          29   \n",
       "3  1946          1301         198         172          51          29   \n",
       "4  1946          1301         198         172          51          29   \n",
       "\n",
       "   lcDisposition  issueArea  \n",
       "0              2          8  \n",
       "1              2          8  \n",
       "2              2          8  \n",
       "3              2          8  \n",
       "4              2          8  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsmalldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an intuitive understanding of the features above, check out the documentation here: http://scdb.wustl.edu/documentation.php?var=petitioner. All the above features are categorical instead of continuous (which means the numbers specify a category instead of having a numerical meaning). For an illustrative example, the \"petitioner\" variable includes:\n",
    "\n",
    "1. attorney general of the United States, or his office\n",
    "2. specified state board or department of education\n",
    "7. state department or agency\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Using Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an intuitive understanding of the meanings behind the variables is important and leads us to our idea of usign the decision tree classifier. A distinct advantage of using decisiontrees is that the decision at each node has an intuitive meaning and corresponds to querying along one feature axis at a time (e.g. is the petitioner an attorney general of the United States?). \n",
    "\n",
    "Furthermore, trees are easy to understand and interpret. We can look at the top node and figure out which feature it corresponds to, and conclude that this feature contributes the most information gain, i.e. is the most important/predictive feature. This makes it easy to verify whether our results make intuitive sense.\n",
    "\n",
    "We will show the process of running decision trees on each justice, before aggregating the votes now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Justice-Centered Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, the feature that we want to predict is the vote for each justice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# newdf.majority refers to whether justice voted with the majority (1 for dissent, 2 for majority)\n",
    "# newdf.partyWinning indicates winning party (0 for responding party, 1 for petitioning party, 2 for unclear)\n",
    "# We use the above 2 features to infer which party the individual justice voted for\n",
    "# NOTE: majority has around 4000 NaNs that we should filter out?\n",
    "results = []\n",
    "ctr1, ctr2 = 0,0\n",
    "for idx, x in enumerate(newdf.majority):\n",
    "    #if decision is unclear, append 2 to results (in reality, apparently there aren't ANY 2s)\n",
    "    if newdf.partyWinning[idx] == 2:\n",
    "        results.append(2)\n",
    "        ctr += 1\n",
    "        break\n",
    "    #if justice voted in the majority\n",
    "    if x == 2:\n",
    "        results.append(newdf.partyWinning[idx]) #results contains 0/1\n",
    "    #if justice voted in the minority\n",
    "    elif x == 1:\n",
    "        results.append(1 - newdf.partyWinning[idx]) #results contains 0/1\n",
    "    else:\n",
    "        #need to clean this up o.o\n",
    "        results.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-36f959fc3bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# these are our target values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnewsmalldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/idzhang/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4218\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4219\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 4220\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   4221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4222\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/Users/idzhang/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   4247\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m             \u001b[0mcan_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcan_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/idzhang/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((df,))\u001b[0m\n\u001b[1;32m   4247\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m             \u001b[0mcan_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcan_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# these are our target values\n",
    "pd.concat([newsmalldf, results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>naturalCourt</th>\n",
       "      <th>petitioner</th>\n",
       "      <th>respondent</th>\n",
       "      <th>caseOrigin</th>\n",
       "      <th>caseSource</th>\n",
       "      <th>lcDisposition</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1946</td>\n",
       "      <td>1301</td>\n",
       "      <td>198</td>\n",
       "      <td>172</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   term  naturalCourt  petitioner  respondent  caseOrigin  caseSource  \\\n",
       "0  1946          1301         198         172          51          29   \n",
       "1  1946          1301         198         172          51          29   \n",
       "2  1946          1301         198         172          51          29   \n",
       "3  1946          1301         198         172          51          29   \n",
       "4  1946          1301         198         172          51          29   \n",
       "\n",
       "   lcDisposition  issueArea  \n",
       "0              2          8  \n",
       "1              2          8  \n",
       "2              2          8  \n",
       "3              2          8  \n",
       "4              2          8  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows where any column value is NaN - dealing with missing data\n",
    "newsmalldf.dropna(axis=0).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
