{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109 Project - The Court Rules In Favor Of...\n",
    "## Aidi Adnan Brian John (Team AABJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "The purpose of this project is to predict votes of Supreme Court justices using oral argument transcripts. Studies in linguistics and psychology, as well as common sense, dictates that the word choices that people make convey crucial information about their beliefs and intentions with regard to issues. Rather than use precedents or formal analysis of the law to predict Supreme Court decisions, we attempt to extract essential emotional features of oral arguments made by justices and advocates in the court. Using aggregate data from 1946 to present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Oral Argument Transcripts - obtained from http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx. Transcripts are made available on the day of court hearing.\n",
    "Justice Vote Counts/Case Information - obtained from the Supreme Court Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a python script (scraper.py) to first scrape the pdfs from the Supreme Court Justice Website (but didn't upload those to the repository, because we ultimately wanted to use text files in our process). We then used a script to convert the pdf files to text files, but not before removing the last 10 pages which were reserved as an index for certain words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gather all txt files, first get the path to the data directory\n",
    "# then list the files and filter out all non-txt files\n",
    "curPath = os.getcwd()\n",
    "dataPath = curPath + '/data/'\n",
    "fileList = os.listdir(dataPath)\n",
    "fileExt = \".txt\"\n",
    "txtFiles = filter(lambda f : f[-4:] == fileExt, fileList)\n",
    "txtFiles = map(lambda f : dataPath + f, txtFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reads in text file, replace path of \"wut.txt\" to relevant txt; only processes one text file currently\n",
    "fil = \"data/cut_126, orig_ppl4.txt\"\n",
    "fil = \"wut.txt\"\n",
    "text_file = open(fil, \"r\")\n",
    "text = text_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a parser to extract the names of the petitioner and respondant attorneys from the first 2 pages of the converted text document. An example of list of petitioner and respondant speakers, taken from the example case in 2014 of Johnson v United States (docket number 13-7120) which shall be henceforth used as the recurring example in this process book, is:\n",
    "\n",
    "Katherine M. Menendez, ESQ., Minneapolis, Minn.; on behalf of Petitioner\n",
    "Michael R. Dreeben, ESQ., Deputy Solicitor General, Department of Justice, Washington D.C.; on behalf of Respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_petitioners_and_respondents(text):\n",
    "    '''\n",
    "    This function takes in input text file as string and outputs 2 lists of speakers speaking for petitioners and\n",
    "    respondents sides.\n",
    "    '''\n",
    "    #get portion of transcript between APPEARANCES and CONTENTS that specifies speakers for petitioners/respondents\n",
    "    start = text.find('APPEARANCES:') + len('APPEARNACES')\n",
    "    end = text.find('C O N T E N T S')\n",
    "    speakers_text = text[start:end]\n",
    "    split_speakers_text = re.split('\\.[ ]*\\n', speakers_text)\n",
    "    #for each speaker, get name (capitalized) and side (Pet/Res) he/she is speaking for\n",
    "    pet_speakers, res_speakers, other_speakers = [], [], []\n",
    "    for speaker in split_speakers_text:\n",
    "        name = speaker.strip().split(',')[0]\n",
    "        #search for first index of capitalized word (which will be start of speaker name)\n",
    "        start = 0\n",
    "        for idx, char in enumerate(name):\n",
    "            if str.isupper(char):\n",
    "                start = idx\n",
    "                break\n",
    "        #actual name to be appended to correct list\n",
    "        name = name[start:]\n",
    "        \n",
    "        #if words Petition, Plaintiff, etc occur in speaker blurb, speaker belongs to Pet\n",
    "        if any(x in speaker for x in ['etition' , 'ppellant', 'emand', 'evers', 'laintiff']):\n",
    "            pet_speakers.append(name)\n",
    "        #otherwise if words Respondent, Defendant, etc occur, speaker belongs to Res\n",
    "        elif any(x in speaker for x in ['espond' , 'ppellee', 'efendant']):\n",
    "            res_speakers.append(name)\n",
    "        #otherwise if neither side is specified in blurb, speaking belongs to Other\n",
    "        elif 'neither' in speaker:\n",
    "            other_speakers.append(name)\n",
    "    return pet_speakers, res_speakers, other_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MR. H. BARTOW FARR'],\n",
       " ['MR. ROY L. REARDON', 'MS. BARBARA D. UNDERWOOD'],\n",
       " [])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, for wut.txt, there's 1 petitioner and 2 respondents\n",
    "pet_speakers, res_speakers, other_speakers = get_petitioners_and_respondents(text)\n",
    "pet_speakers, res_speakers, other_speakers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a list of regular expressions to split the text on\n",
    "def generateRES(nameList, plebe):\n",
    "    \"\"\"\n",
    "    plebe is a boolean determining whether or not the list is of\n",
    "    justices or not\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for name in nameList:\n",
    "        address = \"\"\n",
    "        if plebe:\n",
    "            words = name.split(' ')\n",
    "            # first term is the title, last\n",
    "            # word is the last name\n",
    "            address = \"%s %s\" % (words[0], words[-1])\n",
    "            retList.append(address)\n",
    "        else:\n",
    "            address = \"JUSTICE %s\" % name\n",
    "            address2 = \"CHIEF JUSTICE %s\" % name\n",
    "            retList.append(address)\n",
    "            retList.append(address2)\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getJusticeNames(text):\n",
    "    index = 0\n",
    "    retList = []\n",
    "    while index < len(text):\n",
    "        index = text.find(\"JUSTICE\", index)\n",
    "        if index == -1:\n",
    "            break\n",
    "        index += 8 # because length of JUSTICE is 7, plus length of the space\n",
    "        prevIndex = index\n",
    "        while text[index] != ':':\n",
    "            index +=1\n",
    "        retList.append(text[prevIndex:index])\n",
    "    return list(set(retList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nms = getJusticeNames(clean_argument)\n",
    "# generateRES(nms, False)\n",
    "# print nms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general flow of court proceedings is that the Petitioner attornies make their oral argument, followed by the Respondent attornies, before we hear the rebuttal argument of the Petitioners again. Throughout all proceedings, Justices are free to interject with questions and statements of their own. The below function extracts the main argument portion of the oral transcripts, which is the meat of the proceedings that we are interested in conducting analysis on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_argument_portion(text):\n",
    "    '''\n",
    "    This function gets just the argument portion of the text.\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('P R O C E E D I N G S')\n",
    "    end = text.rfind('Whereupon')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P R O C E E D I N G S\\n\\n2\\n\\n[10:13 a.m.]\\n\\n3\\n4\\n\\nCHIEF JUSTICE REHNQUIST:\\n\\nWe'll hear argument on\\n\\nNumber 00-24, PGA Tour, Inc. vs. Casey Martin.\\n\\n5\\n\\nORAL ARGUMENT OF H. BARTOW FARR, III\\n\\n6\\n\\nON BEHALF OF THE PETITIONER\\n\\n7\\n\\nMR. FARR:\\n\\nMr. Farr?\\n\\nMr. Chief Justice and may it please\\n\\n8\\n\\nthe Court:\\n\\nThe Ninth Circuit in our view made two\\n\\n9\\n\\ncritical mistakes in applying the Disabilities Act to this\\n\\n10\\n\\ntype of claim by a professional athlete. First it failed\\n\\n11\\n\\nto recognize that Title 3 of the act, \""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argument_portion = get_argument_portion(text)\n",
    "argument_portion[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(s):\n",
    "    '''\n",
    "    This function counts number of proper English words in a string s (not non-words like - or --)\n",
    "    '''\n",
    "    s = s.split()\n",
    "    non_words = ['-', '--']\n",
    "    return sum([x not in non_words for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_speaker_names(speakers):\n",
    "    '''\n",
    "    This function modifies speaker names like 'QUESTION' to 'QUESTION: ', for word count parsing later on\n",
    "    '''\n",
    "    return map(lambda x: x+': ', speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTextMaker(text):\n",
    "    '''\n",
    "    This function takes in the portions of text, and gets rid of the \\n and the line numbers. \n",
    "    '''\n",
    "    text_arr=text.splitlines()\n",
    "    text_clean=[]\n",
    "    for each in text_arr:\n",
    "        if each != '':\n",
    "            try:\n",
    "                int(each)\n",
    "            except ValueError: #assummption: if the item only has integers, it is a line number.\n",
    "                text_clean.append(each)\n",
    "    out_text=' '.join(text_clean)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P R O C E E D I N G S [10:13 a.m.] CHIEF JUSTICE REHNQUIST: We'll hear argument on Number 00-24, PGA Tour, Inc. vs. Casey Martin. ORAL ARGUMENT OF H. BARTOW FARR, III ON BEHALF OF THE PETITIONER MR. FARR: Mr. Farr? Mr. Chief Justice and may it please the Court: The Ninth Circuit in our view made two critical mistakes in applying the Disabilities Act to this type of claim by a professional athlete. First it failed to recognize that Title 3 of the act, the public accommodations provision, apply on\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanText=cleanTextMaker(argument_portion)\n",
    "cleanText[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_wordcount(text):\n",
    "    '''\n",
    "    POSSIBLE FEATURE 1:\n",
    "    This function returns a dictionary with key: name of speaker/justice and value: total number of words they\n",
    "    spoke in total throughout argument.\n",
    "    '''\n",
    "    arg_text = get_argument_portion(text)\n",
    "    #keeps track of current speaker\n",
    "    current_speaker = 'N/A'\n",
    "    clean_argument = cleanTextMaker(arg_text)\n",
    "    \n",
    "    #clean argument text split by instances where speakers change\n",
    "\n",
    "    # first get the names of the judges and speakers\n",
    "    pet_speakers, res_speakers, other_speakers = get_petitioners_and_respondents(text)\n",
    "    justiceLeague = getJusticeNames(clean_argument)\n",
    "    # create the regular expression for the justices and the plebes\n",
    "    # need to also add the justice speaker\n",
    "    JLList = generateRES(justiceLeague, False)\n",
    "    plebeList = pet_speakers + res_speakers + other_speakers\n",
    "    plebeRE = generateRES(plebeList, True)\n",
    "    finREList = [\"QUESTION\"]\n",
    "    finREList += plebeRE + JLList\n",
    "    \n",
    "    finREList = map(lambda name : name + \":\", finREList)\n",
    "    RE = '('  + '|'.join(finREList) + ')'\n",
    "    \n",
    "    split_argument = re.split(RE, clean_argument)\n",
    "    all_speakers = finREList\n",
    "    \n",
    "    #num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    num_words = dict(zip(all_speakers + [current_speaker], [0] * (len(all_speakers)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in split_argument:\n",
    "        #if split chunk signifies change in speaker\n",
    "        if s in all_speakers:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            num_words[current_speaker] = num_words[current_speaker] + count_words(s)\n",
    "    \n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHIEF JUSTICE REHNQUIST:': 24,\n",
       " 'JUSTICE REHNQUIST:': 0,\n",
       " 'MR. FARR:': 3433,\n",
       " 'MR. REARDON:': 1480,\n",
       " 'MS. UNDERWOOD:': 1129,\n",
       " 'N/A': 13,\n",
       " 'QUESTION:': 4009}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for example, this gives us total number of words uttered by each speaker\n",
    "#we just need to find list of all speakers in the form they're referred to in the argument, \"JUSTICE SCALIA: \" for ex.\n",
    "total_wordcount(text)\n",
    "# print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordCounter(text):\n",
    "    \"\"\"\n",
    "    counts number of times each word appears in a file\n",
    "    \n",
    "    returns a dictionary of word : times it appears\n",
    "    \"\"\"\n",
    "    wordCount={}\n",
    "    for word in text.split():\n",
    "        # unfortunately, isalpha does discount some real words\n",
    "        # like those with apostrophes, and words with question\n",
    "        # marks at the end of them\n",
    "        if word.lower() not in wordCount and word.isalpha():\n",
    "            wordCount[word.lower()] = 1\n",
    "        elif word.isalpha():\n",
    "            wordCount[word.lower()] += 1\n",
    "    return wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topWords(diction, num, verbose=False):\n",
    "    \"\"\"\n",
    "    returns the top num words in a dictionary\n",
    "    dictionary is expected to be of the format {word : count}\n",
    "    \"\"\"\n",
    "    d = collections.Counter(diction)\n",
    "    if verbose:\n",
    "        for k, v in d.most_common(numTop):\n",
    "            print '%s: %i' % (k, v)\n",
    "    return d.most_common(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# credit to:\n",
    "# http://stackoverflow.com/questions/15173225/how-to-calculate-cosine-similarity-given-2-sentence-strings-python\n",
    "def getCosine(vec1, vec2):\n",
    "    \"\"\"\n",
    "    cosine similarity is used to calculate the similarity index between two vectors\n",
    "    \"\"\"\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = np.sqrt(sum1) * np.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def textToVector(text):\n",
    "    \"\"\"\n",
    "    turn a word into a vector\n",
    "    \"\"\"\n",
    "    WORD = re.compile(r'\\w+')\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textCompiler(text):\n",
    "    \"\"\"\n",
    "    cleanly puts together all of the functions we have written to return a text ready for\n",
    "    classification\n",
    "    \"\"\"\n",
    "    txt = get_argument_portion(text)\n",
    "    txt = cleanTextMaker(txt)\n",
    "    #txt = txt.replace('.', '\\n').split('\\n')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    \"\"\"\n",
    "    opens and cleans all data, putting it in a list\n",
    "    uses txtFiles, which is declared at the beginning of\n",
    "    the ipynb\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for File in txtFiles:\n",
    "        cur = open(File)\n",
    "        textual = cur.read()\n",
    "        cleanTextual = textCompiler(textual)\n",
    "        retList.append(cleanTextual)\n",
    "        cur.close()\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splits along 1 dimension deterministically\n",
    "def splitData(X, fraction_train=9.0 / 10.0):\n",
    "    end_train = int(len(X) * fraction_train)\n",
    "    X_train = X[0:end_train]\n",
    "    X_test = X[end_train:]\n",
    "    return X_train, X_test\n",
    "\n",
    "# def splitTrainTest(X, Y, fraction_train = 9.0 / 10.0):\n",
    "#     X_train, X_test = splitData(X, fraction_train)\n",
    "#     Y_train, Y_test = splitData(Y, fraction_train)\n",
    "#     assert(X_train.shape[0] == Y_train.shape[0])\n",
    "#     assert(X_test.shape[0] == Y_test.shape[0])\n",
    "#     return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullBody = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splits data. note that tdidf is a measure of the relative importance of a word in a document\n",
    "# based on its inverse frequency\n",
    "testBody, trainBody = splitData(fullBody)\n",
    "vectorizer = TfidfVectorizer(min_df=1, norm='l2', use_idf=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textToMat(vizer, docList):\n",
    "    \"\"\"\n",
    "    turns documents into a tfidf matrix\n",
    "    Parameters:\n",
    "        vizer is a vectorizer of type TfidfVectorizer\n",
    "        docList is a list of (ideally) preprocessed documents\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for doc in docList:\n",
    "        resMat = vizer.transform(doc).todense()\n",
    "        retList.append(resMat)\n",
    "    return retList    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print vectorizer.vocabulary_\n",
    "# print mat.todense()#[:,942]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat = vectorizer.fit_transform(trainBody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a natural first choice for a model since our target value can be viewed as a probability between 0 or 1 for any individual justice to vote For or Against, with a higher probability representing a higher confidence of that justice voting in favor of the arguing party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "caseId  \n",
    "docketId   \n",
    "caseIssuesId  \n",
    "voteId  \n",
    "dateDecision  \n",
    "decisionType  \n",
    "usCite  \n",
    "sctCite  \n",
    "ledCite  \n",
    "lexisCite  \n",
    "term   \n",
    "naturalCourt  \n",
    "chief  \n",
    "docket   \n",
    "caseName  \n",
    "dateArgument  \n",
    "dateRearg  \n",
    "petitioner   \n",
    "petitionerState  \n",
    "respondent  \n",
    "respondentState  \n",
    "jurisdiction  \n",
    "adminAction  \n",
    "adminActionState  \n",
    "threeJudgeFdc  \n",
    "caseOrigin  \n",
    "caseOriginState  \n",
    "caseSource  \n",
    "caseSourceState  \n",
    "lcDisagreement  \n",
    "certReason  \n",
    "lcDisposition  \n",
    "lcDispositionDirection  \n",
    "declarationUncon  \n",
    "caseDisposition   \n",
    "caseDispositionUnusual  \n",
    "partyWinning    \n",
    "precedentAlteration  \n",
    "voteUnclear    \n",
    "issue  \n",
    "issueArea  \n",
    "decisionDirection  \n",
    "decisionDirectionDissent  \n",
    "authorityDecision1  \n",
    "authorityDecision2  \n",
    "lawType  \n",
    "lawSupp  \n",
    "lawMinor  \n",
    "majOpinWriter  \n",
    "majOpinAssigner  \n",
    "splitVote  \n",
    "majVotes  \n",
    "minVotes \n",
    "\n",
    "issue: This variable identifies the issue for each decision. Although criteria for the identification of issues are hard to articulate, the focus here is on the subject matter of the controversy (e.g., sex discrimination, state tax, affirmative action) rather than its legal basis (e.g., the equal protection clause) \n",
    "\n",
    "issueArea: This variable simply separates the issues identified in the preceding variable (issue) into the following larger categories: criminal procedure (issues 10010-10600), civil rights (issues 20010-20410), First Amendment (issues 30010-30020), etc \n",
    "\n",
    "decisionDirection: In order to determine whether the Court supports or opposes the issue to which the case pertains, this variable codes the ideological \"direction\" of the decision. \n",
    "    An outcome is liberal (=2) or conservative (=1)\n",
    "\n",
    "dateDecision: This variable contains the year, month, and day that the Court announced its decision in the case.\n",
    "\n",
    "justiceName: This is a string variable indicating the first initial for the five justices with a common surname (Harlan, Johnson, Marshall, Roberts, and White) and last name of each justice.\n",
    "\n",
    "chief: This variable identifies the chief justice durinmg whose tenure the case was decided.\n",
    "\n",
    "caseSource: This variable identifies the court whose decision the Supreme Court reviewed. If the case originated in the same court whose decision the Supreme Court reviewed, the entry in the caseOrigin should be the same as here. This variable has no entry if the case arose under the Supreme Court's original jurisdiction. \n",
    "\n",
    "certReason: This variable provides the reason, if any, that the Court gives for granting the petition for certiorari. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigdf=pd.read_csv(\"SCDB_2015_01_justiceCentered_Citation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8\n",
       "1    8\n",
       "2    8\n",
       "3    8\n",
       "4    8\n",
       "Name: issueArea, dtype: float64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf['issueArea'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "log_model = LogisticRegression(penalty='l2',C=1.0, fit_intercept=True, class_weight='auto')\n",
    "bigdf=pd.read_csv(\"SCDB_2015_01_justiceCentered_Citation.csv\")\n",
    "\n",
    "smalldf = pd.DataFrame()\n",
    "\n",
    "regress_vars = ['issue', 'issueArea', 'decisionDirection', \n",
    "                'dateDecision', 'justice', 'chief', 'caseSource', 'certReason', 'vote'] \n",
    "\n",
    "for i in regress_vars: \n",
    "    smalldf[i] = bigdf[i]\n",
    "    \n",
    "y, X = dmatrices('C(vote) ~ C(issue) + C(issueArea) + C(decisionDirection) + C(justice) + \\\n",
    "                  C(caseSource) + C(certReason)',\n",
    "                  smalldf, return_type=\"dataframe\")\n",
    "\n",
    "print (X.shape)\n",
    "print(y.shape)\n",
    "# y = np.ravel(y)\n",
    "print((y))\n",
    "model = LogisticRegression()\n",
    "\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in SCDB data from file\n",
    "bigdf=pd.read_csv(\"supremeCourtDb.csv\")\n",
    "bigdf[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = bigdf[\"docketId\", \"dateDecision\", \"case\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C=1.0, kernel='linear', probability=True, class_weight='auto')\n",
    "svm_model = my_svm.fit(X, y)\n",
    "svm_pred = svm_fit.predict(W)\n",
    "# Class probabilities, based on log regression on distance to hyperplane.\n",
    "svm_prob = svm_fit.predict_proba(W)\n",
    "svm_dist = svm_fit.decision_function(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Justice Ruling Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a different dataset in a slightly different approach to making Supreme Court ruling predictions. This method is motivated by the fact that usually, only 2 justices tend to be swing votes and justice decisions are highly influenced by factors outside of what transpires in court proceedings, such as background information about the case itself. The Supreme Court website contains a Justice-centered database which contains extensive information about each case; in particular, the most pertinent fields we are interested in analyzing are:\n",
    "\n",
    "1. Decision Year\n",
    "2. Natural Court\n",
    "3. Petitioner\n",
    "4. Respondent\n",
    "5. Case Origin\n",
    "6. Case Source\n",
    "7. Lower Court Disposition Direction\n",
    "8. Issue Area\n",
    "\n",
    "Our target value to predict is the field called winningParty (petitioner or respondent), which using our justice-centered approach involves aggregating predicted votes for each individual justice and taking majority vote. The associated confidence of our entire prediction is obtained by averaging individual confidences of our models for each justice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in justice-centered SCDB data from file\n",
    "newdf=pd.read_csv(\"SCDB_justice_centered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maybe lcDispositionDirection? choose features with continuous/numerical features\n",
    "# do the numbers mean anything though?\n",
    "newsmalldf = newdf[[\"term\", \"naturalCourt\", \"petitioner\", \"respondent\", \"caseOrigin\", \"caseSource\", \"lcDisposition\", \"issueArea\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsmalldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an intuitive understanding of the features above, check out the documentation here: http://scdb.wustl.edu/documentation.php?var=petitioner. All the above features are categorical instead of continuous (which means the numbers specify a category instead of having a numerical meaning). For an illustrative example, the \"petitioner\" variable includes:\n",
    "\n",
    "1. attorney general of the United States, or his office\n",
    "2. specified state board or department of education\n",
    "7. state department or agency\n",
    "etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Using Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an intuitive understanding of the meanings behind the variables is important and leads us to our idea of usign the decision tree classifier. A distinct advantage of using decisiontrees is that the decision at each node has an intuitive meaning and corresponds to querying along one feature axis at a time (e.g. is the petitioner an attorney general of the United States?). \n",
    "\n",
    "Furthermore, trees are easy to understand and interpret. We can look at the top node and figure out which feature it corresponds to, and conclude that this feature contributes the most information gain, i.e. is the most important/predictive feature. This makes it easy to verify whether our results make intuitive sense.\n",
    "\n",
    "We will show the process of running decision trees on each justice, before aggregating the votes now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Justice-Centered Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, the feature that we want to predict is the vote for each justice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# newdf.majority refers to whether justice voted with the majority (1 for dissent, 2 for majority)\n",
    "# newdf.partyWinning indicates winning party (0 for responding party, 1 for petitioning party, 2 for unclear)\n",
    "# We use the above 2 features to infer which party the individual justice voted for\n",
    "# NOTE: majority has around 4000 NaNs that we should filter out?\n",
    "results = []\n",
    "ctr1, ctr2 = 0,0\n",
    "for idx, x in enumerate(newdf.majority):\n",
    "    #if decision is unclear, append 2 to results (in reality, apparently there aren't ANY 2s)\n",
    "    if newdf.partyWinning[idx] == 2:\n",
    "        results.append(2)\n",
    "        ctr += 1\n",
    "        break\n",
    "    #if justice voted in the majority\n",
    "    if x == 2:\n",
    "        results.append(newdf.partyWinning[idx]) #results contains 0/1\n",
    "    #if justice voted in the minority\n",
    "    elif x == 1:\n",
    "        results.append(1 - newdf.partyWinning[idx]) #results contains 0/1\n",
    "    else:\n",
    "        #need to clean this up o.o\n",
    "        results.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(newsmalldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsmalldf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are our target values\n",
    "pd.concat([newsmalldf, results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop rows where any column value is NaN - dealing with missing data\n",
    "newsmalldf.dropna(axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(newsmalldf), len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsmalldf[\"results\"] = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsmalldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "In this section, we will analyze the transcripts using sentiment analysis to get a sense of what the verdict will be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepping the Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_file_dict(fileList, fileExt='.txt'):\n",
    "    '''\n",
    "    This function takes the fileList and returns a list of dictionaries of the format \n",
    "    {'case_number': case_number, 'full_text': full_text}\n",
    "    '''\n",
    "    fileDict=[]\n",
    "    fields=['docket', 'full_text']\n",
    "    txtFiles_filter = filter(lambda f : f[-4:] == fileExt, fileList)\n",
    "    for each in txtFiles_filter:\n",
    "        name_str=each[4:-4]\n",
    "        try:\n",
    "            indexx=name_str.index('_')\n",
    "            docketNum=name_str[:indexx]\n",
    "        except ValueError:\n",
    "            docketNum=name_str\n",
    "        cur = open(dataPath+each)\n",
    "        textual = cur.read()\n",
    "        cleanTextual = textCompiler(textual) #textCompiler edited (see above) @Aidi I comented out the replace-split\n",
    "        cur.close()\n",
    "        tuple=(docketNum, cleanTextual)\n",
    "        fileDict.append(dict(zip(fields, tuple)))\n",
    "    return fileDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileDict=get_file_dict(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-1011</td>\n",
       "      <td>P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-1021</td>\n",
       "      <td>P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-1045</td>\n",
       "      <td>P R O C E E D I N G S (11:02 a.m.) CHIEF JUSTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-10666</td>\n",
       "      <td>P R O C E E D I N G S (11:03 a.m.) CHIEF JUSTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-1072</td>\n",
       "      <td>P R O C E E D I N G S (11:18 a.m.) CHIEF JUSTI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     docket                                          full_text\n",
       "0   00-1011  P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...\n",
       "1   00-1021  P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...\n",
       "2   00-1045  P R O C E E D I N G S (11:02 a.m.) CHIEF JUSTI...\n",
       "3  00-10666  P R O C E E D I N G S (11:03 a.m.) CHIEF JUSTI...\n",
       "4   00-1072  P R O C E E D I N G S (11:18 a.m.) CHIEF JUSTI..."
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtdf = pd.DataFrame(fileDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "casedf=pd.read_csv('supremeCourtDb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the 2015 transcripts will be dropped in the process as the database does not contain 2015 entries\n",
    "merged=pd.merge(left=txtdf, right=casedf, how='inner', left_on='docket', right_on='docket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stopwords=text.ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adapted from hw5\n",
    "def get_parts(thetext):\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wang/anaconda/lib/python2.7/site-packages/pattern/text/__init__.py:979: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  and tokens[j] in (\"'\", \"\\\"\", u\"”\", u\"’\", \"...\", \".\", \"!\", \"?\", \")\", EOS):\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed=[get_parts(t) for t in merged.full_text.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumped `parsed` to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####storing the parsed variable in a file in case it's needed later.\n",
    "#import pickle\n",
    "\n",
    "#credit: http://stackoverflow.com/questions/6568007/how-do-i-save-and-restore-multiple-variables-in-python\n",
    "#with open('parsed.pickle', 'w') as f:\n",
    "#    pickle.dump(parsed, f)\n",
    "\n",
    "# Getting back the objects:\n",
    "#with open('objs.pickle') as f:\n",
    "#    parsed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#taking all the descriptives from the transcript. \n",
    "nbdata=[each[1] for each in parsed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#right now the list is flattened and duplicates are dropped for simplicity; room for improvement to consider dup.\n",
    "flattened=[]\n",
    "for sub_nb in nbdata:\n",
    "    flattened.append([item for l in sub_nb for item in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>full_text</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>...</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "      <th>net_pos</th>\n",
       "      <th>descriptives</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-1011</td>\n",
       "      <td>P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2000-079</td>\n",
       "      <td>2000-079-01</td>\n",
       "      <td>2000-079-01-01</td>\n",
       "      <td>2000-079-01-01-01</td>\n",
       "      <td>6/25/01</td>\n",
       "      <td>1</td>\n",
       "      <td>533 U.S. 348</td>\n",
       "      <td>121 S. Ct. 2268</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>[chief, oral, jurisdictional, legal, pure, com...</td>\n",
       "      <td>[[accurate, good, permissible, right, consiste...</td>\n",
       "      <td>[[unconstitutional, criminal, arbitrary, odd, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-1021</td>\n",
       "      <td>P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-071</td>\n",
       "      <td>2001-071-01</td>\n",
       "      <td>2001-071-01-01</td>\n",
       "      <td>2001-071-01-01-01</td>\n",
       "      <td>6/20/02</td>\n",
       "      <td>1</td>\n",
       "      <td>536 U.S. 355</td>\n",
       "      <td>122 S. Ct. 2151</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[chief, oral, straightforward, civil, exclusiv...</td>\n",
       "      <td>[[precise, interesting, adequate, excited, str...</td>\n",
       "      <td>[[reluctant, undermined, puzzled, bad, difficu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-1045</td>\n",
       "      <td>P R O C E E D I N G S (11:02 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-002</td>\n",
       "      <td>2001-002-01</td>\n",
       "      <td>2001-002-01-01</td>\n",
       "      <td>2001-002-01-01-01</td>\n",
       "      <td>11/13/01</td>\n",
       "      <td>1</td>\n",
       "      <td>534 U.S. 19</td>\n",
       "      <td>122 S. Ct. 441</td>\n",
       "      <td>...</td>\n",
       "      <td>15 U.S.C. � 1681</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "      <td>[chief, oral, improper, opposed, improper, exp...</td>\n",
       "      <td>[[accurate, timely, like, reasonable, right, f...</td>\n",
       "      <td>[[limited, negligent, defamatory, bad, liable,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-10666</td>\n",
       "      <td>P R O C E E D I N G S (11:03 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-076</td>\n",
       "      <td>2001-076-01</td>\n",
       "      <td>2001-076-01-01</td>\n",
       "      <td>2001-076-01-01-01</td>\n",
       "      <td>6/24/02</td>\n",
       "      <td>1</td>\n",
       "      <td>536 U.S. 545</td>\n",
       "      <td>122 S. Ct. 2406</td>\n",
       "      <td>...</td>\n",
       "      <td>18 U.S.C. � 924</td>\n",
       "      <td>106</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[chief, oral, brandish, reasonable, separate, ...</td>\n",
       "      <td>[[precise, intricate, harmless, good, unlimite...</td>\n",
       "      <td>[[unconstitutional, violent, worst, anxious, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-1072</td>\n",
       "      <td>P R O C E E D I N G S (11:18 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-031</td>\n",
       "      <td>2001-031-01</td>\n",
       "      <td>2001-031-01-01</td>\n",
       "      <td>2001-031-01-01-01</td>\n",
       "      <td>3/19/02</td>\n",
       "      <td>1</td>\n",
       "      <td>535 U.S. 106</td>\n",
       "      <td>122 S. Ct. 1145</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[chief, oral, relation-back, proper, procedura...</td>\n",
       "      <td>[[accurate, perfect, good, permissible, timely...</td>\n",
       "      <td>[[stringent, inconsistent, untimely, discrimin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     docket                                          full_text    caseId  \\\n",
       "0   00-1011  P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...  2000-079   \n",
       "1   00-1021  P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...  2001-071   \n",
       "2   00-1045  P R O C E E D I N G S (11:02 a.m.) CHIEF JUSTI...  2001-002   \n",
       "3  00-10666  P R O C E E D I N G S (11:03 a.m.) CHIEF JUSTI...  2001-076   \n",
       "4   00-1072  P R O C E E D I N G S (11:18 a.m.) CHIEF JUSTI...  2001-031   \n",
       "\n",
       "      docketId    caseIssuesId             voteId dateDecision  decisionType  \\\n",
       "0  2000-079-01  2000-079-01-01  2000-079-01-01-01      6/25/01             1   \n",
       "1  2001-071-01  2001-071-01-01  2001-071-01-01-01      6/20/02             1   \n",
       "2  2001-002-01  2001-002-01-01  2001-002-01-01-01     11/13/01             1   \n",
       "3  2001-076-01  2001-076-01-01  2001-076-01-01-01      6/24/02             1   \n",
       "4  2001-031-01  2001-031-01-01  2001-031-01-01-01      3/19/02             1   \n",
       "\n",
       "         usCite          sctCite  \\\n",
       "0  533 U.S. 348  121 S. Ct. 2268   \n",
       "1  536 U.S. 355  122 S. Ct. 2151   \n",
       "2   534 U.S. 19   122 S. Ct. 441   \n",
       "3  536 U.S. 545  122 S. Ct. 2406   \n",
       "4  535 U.S. 106  122 S. Ct. 1145   \n",
       "\n",
       "                         ...                                  lawMinor  \\\n",
       "0                        ...                                       NaN   \n",
       "1                        ...                                       NaN   \n",
       "2                        ...                          15 U.S.C. � 1681   \n",
       "3                        ...                           18 U.S.C. � 924   \n",
       "4                        ...                                       NaN   \n",
       "\n",
       "  majOpinWriter  majOpinAssigner  splitVote majVotes minVotes net_pos  \\\n",
       "0           103              103          1        5        4       9   \n",
       "1           107              103          1        5        4      -1   \n",
       "2           109              102          1        9        0      -6   \n",
       "3           106              102          1        5        4       5   \n",
       "4           107              102          1        9        0      10   \n",
       "\n",
       "                                        descriptives  \\\n",
       "0  [chief, oral, jurisdictional, legal, pure, com...   \n",
       "1  [chief, oral, straightforward, civil, exclusiv...   \n",
       "2  [chief, oral, improper, opposed, improper, exp...   \n",
       "3  [chief, oral, brandish, reasonable, separate, ...   \n",
       "4  [chief, oral, relation-back, proper, procedura...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [[accurate, good, permissible, right, consiste...   \n",
       "1  [[precise, interesting, adequate, excited, str...   \n",
       "2  [[accurate, timely, like, reasonable, right, f...   \n",
       "3  [[precise, intricate, harmless, good, unlimite...   \n",
       "4  [[accurate, perfect, good, permissible, timely...   \n",
       "\n",
       "                                                 neg  \n",
       "0  [[unconstitutional, criminal, arbitrary, odd, ...  \n",
       "1  [[reluctant, undermined, puzzled, bad, difficu...  \n",
       "2  [[limited, negligent, defamatory, bad, liable,...  \n",
       "3  [[unconstitutional, violent, worst, anxious, u...  \n",
       "4  [[stringent, inconsistent, untimely, discrimin...  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the flattened list of descriptives to the dataframe as a column. \n",
    "merged['descriptives']=pd.Series(flattened, index=merged.index)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load opinion lexicon\n",
    "pos_link = \"opinion-lexicon-English/positive-words.txt\"\n",
    "neg_link = \"opinion-lexicon-English/negative-words.txt\"\n",
    "\n",
    "def get_both_list(pos_link, neg_link):\n",
    "    '''\n",
    "    This function takes the links in for the lexicon files downloaded from \n",
    "    https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "    and outputs two lists: posList and negList\n",
    "    '''\n",
    "    pos_file = open(pos_link, \"r\")\n",
    "    neg_file = open(neg_link, \"r\")\n",
    "\n",
    "    posList = pos_file.read()\n",
    "    negList = neg_file.read()\n",
    "    \n",
    "    posList=get_pos_lexicon(posList)\n",
    "    negList=get_neg_lexicon(negList)\n",
    "\n",
    "    posList=posList.split('\\n')\n",
    "    negList=negList.split('\\n')\n",
    "\n",
    "    return posList, negList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neg_lexicon(text):\n",
    "    '''\n",
    "    This function gets neg words from the list\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('2-faced')\n",
    "    end = text.rfind('zombie')\n",
    "    return text[start:end]\n",
    "\n",
    "def get_pos_lexicon(text):\n",
    "    '''\n",
    "    This function gets pos words from the list.\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('a+')\n",
    "    end = text.rfind('zippy')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get posList and negList for the next step\n",
    "posList, negList = get_both_list(pos_link, neg_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_pos_neg(descriptives, posList, negList):\n",
    "    '''\n",
    "    This function takes all the descriptives for each transcript and \n",
    "    splits the list into two lists: one of positive and one of negative words. \n",
    "    '''\n",
    "    pos=[]\n",
    "    neg=[]\n",
    "    for l in descriptives:\n",
    "        positive=[]\n",
    "        negative=[]\n",
    "        for e in l:\n",
    "            if e in posList:\n",
    "                positive.append(e)\n",
    "            elif e in negList:\n",
    "                negative.append(e)\n",
    "        pos.append([positive, 'pos'])\n",
    "        neg.append([negative, 'neg'])\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Wang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "pos, neg=split_pos_neg(list(merged.descriptives.values), posList, negList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>full_text</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>...</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "      <th>net_pos</th>\n",
       "      <th>descriptives</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-1011</td>\n",
       "      <td>P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2000-079</td>\n",
       "      <td>2000-079-01</td>\n",
       "      <td>2000-079-01-01</td>\n",
       "      <td>2000-079-01-01-01</td>\n",
       "      <td>6/25/01</td>\n",
       "      <td>1</td>\n",
       "      <td>533 U.S. 348</td>\n",
       "      <td>121 S. Ct. 2268</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>[chief, oral, jurisdictional, legal, pure, com...</td>\n",
       "      <td>[[pure, pure, proper, gracious, permissible, c...</td>\n",
       "      <td>[[awkward, difficult, awkward, ambiguous, crim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-1021</td>\n",
       "      <td>P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-071</td>\n",
       "      <td>2001-071-01</td>\n",
       "      <td>2001-071-01-01</td>\n",
       "      <td>2001-071-01-01-01</td>\n",
       "      <td>6/20/02</td>\n",
       "      <td>1</td>\n",
       "      <td>536 U.S. 355</td>\n",
       "      <td>122 S. Ct. 2151</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>[chief, oral, straightforward, civil, exclusiv...</td>\n",
       "      <td>[[straightforward, clear, correct, available, ...</td>\n",
       "      <td>[[improper, reluctant, puzzled, unable, wrong,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00-1045</td>\n",
       "      <td>P R O C E E D I N G S (11:02 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-002</td>\n",
       "      <td>2001-002-01</td>\n",
       "      <td>2001-002-01-01</td>\n",
       "      <td>2001-002-01-01-01</td>\n",
       "      <td>11/13/01</td>\n",
       "      <td>1</td>\n",
       "      <td>534 U.S. 19</td>\n",
       "      <td>122 S. Ct. 441</td>\n",
       "      <td>...</td>\n",
       "      <td>15 U.S.C. � 1681</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "      <td>[chief, oral, improper, opposed, improper, exp...</td>\n",
       "      <td>[[correct, like, available, reasonable, reason...</td>\n",
       "      <td>[[improper, improper, improper, improper, impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00-10666</td>\n",
       "      <td>P R O C E E D I N G S (11:03 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-076</td>\n",
       "      <td>2001-076-01</td>\n",
       "      <td>2001-076-01-01</td>\n",
       "      <td>2001-076-01-01-01</td>\n",
       "      <td>6/24/02</td>\n",
       "      <td>1</td>\n",
       "      <td>536 U.S. 545</td>\n",
       "      <td>122 S. Ct. 2406</td>\n",
       "      <td>...</td>\n",
       "      <td>18 U.S.C. � 924</td>\n",
       "      <td>106</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>[chief, oral, brandish, reasonable, separate, ...</td>\n",
       "      <td>[[reasonable, clear, leading, important, good,...</td>\n",
       "      <td>[[steep, disadvantaged, ineffective, ineffecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00-1072</td>\n",
       "      <td>P R O C E E D I N G S (11:18 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-031</td>\n",
       "      <td>2001-031-01</td>\n",
       "      <td>2001-031-01-01</td>\n",
       "      <td>2001-031-01-01-01</td>\n",
       "      <td>3/19/02</td>\n",
       "      <td>1</td>\n",
       "      <td>535 U.S. 106</td>\n",
       "      <td>122 S. Ct. 1145</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[chief, oral, relation-back, proper, procedura...</td>\n",
       "      <td>[[proper, consistent, significant, sufficient,...</td>\n",
       "      <td>[[unlawful, unlawful, discriminatory, unlawful...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     docket                                          full_text    caseId  \\\n",
       "0   00-1011  P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...  2000-079   \n",
       "1   00-1021  P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...  2001-071   \n",
       "2   00-1045  P R O C E E D I N G S (11:02 a.m.) CHIEF JUSTI...  2001-002   \n",
       "3  00-10666  P R O C E E D I N G S (11:03 a.m.) CHIEF JUSTI...  2001-076   \n",
       "4   00-1072  P R O C E E D I N G S (11:18 a.m.) CHIEF JUSTI...  2001-031   \n",
       "\n",
       "      docketId    caseIssuesId             voteId dateDecision  decisionType  \\\n",
       "0  2000-079-01  2000-079-01-01  2000-079-01-01-01      6/25/01             1   \n",
       "1  2001-071-01  2001-071-01-01  2001-071-01-01-01      6/20/02             1   \n",
       "2  2001-002-01  2001-002-01-01  2001-002-01-01-01     11/13/01             1   \n",
       "3  2001-076-01  2001-076-01-01  2001-076-01-01-01      6/24/02             1   \n",
       "4  2001-031-01  2001-031-01-01  2001-031-01-01-01      3/19/02             1   \n",
       "\n",
       "         usCite          sctCite  \\\n",
       "0  533 U.S. 348  121 S. Ct. 2268   \n",
       "1  536 U.S. 355  122 S. Ct. 2151   \n",
       "2   534 U.S. 19   122 S. Ct. 441   \n",
       "3  536 U.S. 545  122 S. Ct. 2406   \n",
       "4  535 U.S. 106  122 S. Ct. 1145   \n",
       "\n",
       "                         ...                                  lawMinor  \\\n",
       "0                        ...                                       NaN   \n",
       "1                        ...                                       NaN   \n",
       "2                        ...                          15 U.S.C. � 1681   \n",
       "3                        ...                           18 U.S.C. � 924   \n",
       "4                        ...                                       NaN   \n",
       "\n",
       "  majOpinWriter  majOpinAssigner  splitVote majVotes minVotes net_pos  \\\n",
       "0           103              103          1        5        4       9   \n",
       "1           107              103          1        5        4      -1   \n",
       "2           109              102          1        9        0      -6   \n",
       "3           106              102          1        5        4       5   \n",
       "4           107              102          1        9        0      10   \n",
       "\n",
       "                                        descriptives  \\\n",
       "0  [chief, oral, jurisdictional, legal, pure, com...   \n",
       "1  [chief, oral, straightforward, civil, exclusiv...   \n",
       "2  [chief, oral, improper, opposed, improper, exp...   \n",
       "3  [chief, oral, brandish, reasonable, separate, ...   \n",
       "4  [chief, oral, relation-back, proper, procedura...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [[pure, pure, proper, gracious, permissible, c...   \n",
       "1  [[straightforward, clear, correct, available, ...   \n",
       "2  [[correct, like, available, reasonable, reason...   \n",
       "3  [[reasonable, clear, leading, important, good,...   \n",
       "4  [[proper, consistent, significant, sufficient,...   \n",
       "\n",
       "                                                 neg  \n",
       "0  [[awkward, difficult, awkward, ambiguous, crim...  \n",
       "1  [[improper, reluctant, puzzled, unable, wrong,...  \n",
       "2  [[improper, improper, improper, improper, impr...  \n",
       "3  [[steep, disadvantaged, ineffective, ineffecti...  \n",
       "4  [[unlawful, unlawful, discriminatory, unlawful...  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the split lists to merged as columns\n",
    "merged['pos']=pd.Series(pos, index=merged.index)\n",
    "merged['neg']=pd.Series(neg, index=merged.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get net count of positive words over negative words\n",
    "net_count=[len(e1[0])-len(e2[0]) for (e1, e2) in zip(merged.pos, merged.neg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>full_text</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>...</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "      <th>net_pos</th>\n",
       "      <th>descriptives</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-1011</td>\n",
       "      <td>P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2000-079</td>\n",
       "      <td>2000-079-01</td>\n",
       "      <td>2000-079-01-01</td>\n",
       "      <td>2000-079-01-01-01</td>\n",
       "      <td>6/25/01</td>\n",
       "      <td>1</td>\n",
       "      <td>533 U.S. 348</td>\n",
       "      <td>121 S. Ct. 2268</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>[chief, oral, jurisdictional, legal, pure, com...</td>\n",
       "      <td>[[pure, pure, proper, gracious, permissible, c...</td>\n",
       "      <td>[[awkward, difficult, awkward, ambiguous, crim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00-1021</td>\n",
       "      <td>P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...</td>\n",
       "      <td>2001-071</td>\n",
       "      <td>2001-071-01</td>\n",
       "      <td>2001-071-01-01</td>\n",
       "      <td>2001-071-01-01-01</td>\n",
       "      <td>6/20/02</td>\n",
       "      <td>1</td>\n",
       "      <td>536 U.S. 355</td>\n",
       "      <td>122 S. Ct. 2151</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>[chief, oral, straightforward, civil, exclusiv...</td>\n",
       "      <td>[[straightforward, clear, correct, available, ...</td>\n",
       "      <td>[[improper, reluctant, puzzled, unable, wrong,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    docket                                          full_text    caseId  \\\n",
       "0  00-1011  P R O C E E D I N G S (10:17 a.m.) CHIEF JUSTI...  2000-079   \n",
       "1  00-1021  P R O C E E D I N G S (10:07 a.m.) CHIEF JUSTI...  2001-071   \n",
       "\n",
       "      docketId    caseIssuesId             voteId dateDecision  decisionType  \\\n",
       "0  2000-079-01  2000-079-01-01  2000-079-01-01-01      6/25/01             1   \n",
       "1  2001-071-01  2001-071-01-01  2001-071-01-01-01      6/20/02             1   \n",
       "\n",
       "         usCite          sctCite  \\\n",
       "0  533 U.S. 348  121 S. Ct. 2268   \n",
       "1  536 U.S. 355  122 S. Ct. 2151   \n",
       "\n",
       "                         ...                         lawMinor majOpinWriter  \\\n",
       "0                        ...                              NaN           103   \n",
       "1                        ...                              NaN           107   \n",
       "\n",
       "   majOpinAssigner  splitVote majVotes minVotes net_pos  \\\n",
       "0              103          1        5        4       6   \n",
       "1              103          1        5        4      15   \n",
       "\n",
       "                                        descriptives  \\\n",
       "0  [chief, oral, jurisdictional, legal, pure, com...   \n",
       "1  [chief, oral, straightforward, civil, exclusiv...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [[pure, pure, proper, gracious, permissible, c...   \n",
       "1  [[straightforward, clear, correct, available, ...   \n",
       "\n",
       "                                                 neg  \n",
       "0  [[awkward, difficult, awkward, ambiguous, crim...  \n",
       "1  [[improper, reluctant, puzzled, unable, wrong,...  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append net_post to merged as a column\n",
    "merged['net_pos']=pd.Series(net_count, index=merged.index)\n",
    "merged.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlsc=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[13] at parallelize at PythonRDD.scala:423"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdatardd=sc.parallelize([ele[1] for ele in parsed])\n",
    "nbdatardd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#adapted from hw5\n",
    "adjvocab = (nbdatardd.flatMap(lambda word: word)\n",
    "             .flatMap(lambda word: word)\n",
    "             .map(lambda word: (word, 1))\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "             .map(lambda (x,y): x)\n",
    "             .zipWithIndex()\n",
    "            ).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "Xarraypre=nbdatardd.map(lambda l: \" \".join(list(itertools.chain.from_iterable(l))))\n",
    "Xarray=Xarraypre.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(len(Xarray)), train_size=0.7)\n",
    "mask=np.ones(len(Xarray), dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array(Xarray)\n",
    "y=np.array(merged.partyWinning.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_xy(X_col, y_col, vectorizer):\n",
    "    X = vectorizer.fit_transform(X_col)\n",
    "    y = y_col\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    favorable = y == 1\n",
    "    unclear = ~favorable\n",
    "    return prob[unclear, 0].sum() + prob[favorable, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_score(clf, x, y, score_func, nfold=5):\n",
    "    result = 0\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf, x[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#search for best alphas and min_dfs\n",
    "alphas = [0, .1, 1, 5]\n",
    "min_dfs = [1e-5, 1e-3, 1e-1]\n",
    "\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(min_df = min_df, vocabulary=adjvocab)       \n",
    "        Xthis, ythis = make_xy(X_col=X, y_col=y, vectorizer=vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        clf = MultinomialNB(alpha=alpha)\n",
    "        cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_alpha, best_min_df = alpha, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1.000000\n",
      "min_df: 0.000010\n"
     ]
    }
   ],
   "source": [
    "print \"alpha: %f\" % best_alpha\n",
    "print \"min_df: %f\" % best_min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training set 0.886676875957\n",
      "accuracy on testing set 0.55871886121\n"
     ]
    }
   ],
   "source": [
    "#get train and test accuracy\n",
    "vectorizer = CountVectorizer(min_df = 0.00001, vocabulary=adjvocab)       \n",
    "Xthis, ythis = make_xy(X_col=X, y_col=y, vectorizer=vectorizer)\n",
    "Xtrainthis=Xthis[mask]\n",
    "ytrainthis=ythis[mask]\n",
    "clf = MultinomialNB(alpha=1.)\n",
    "cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "print \"accuracy on training set\", clf.score(Xtrainthis, ytrainthis)\n",
    "print \"accuracy on testing set\", clf.score(Xthis[~mask], ythis[~mask])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
