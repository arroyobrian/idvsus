{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109 Project - The Court Rules In Favor Of...\n",
    "## Aidi Adnan Brian John (Team AABJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [The Court Rules in Favor Of...](#CS109 Project - The Court Rules In Favor Of...)\n",
    "    * [1. Data Cleaning and Preparation](#1.-Data-Cleaning-and-Preparation)\n",
    "    * [2. Latent Sentiment Indexing](#2.-LSI-:-Latent-Sentiment-Indexing)\n",
    "        * [2.0.1 Pre-processing](##Stage-1:-Pre-processing)\n",
    "        * [2.0.2 Frequency Document-Inverse Document Frequency (TF-IDF)](###-TF-IDF-:-Frequency-Document-Inverse-Document Frequency)\n",
    "        * [2.1 Splitting prepared documents into petitioner and respondent speeches](##-2.1-Splitting-prepared-documents-into-petitioner-and-respondent-speeches)\n",
    "        * [2.2 Applying term-frequency inverse document frequency vectorizer](##-2.2-Applying-term-frequency-inverse-document-frequency-vectorizer)\n",
    "        * [2.3 Running Singular Vector Decomposition](##-2.3-Running-Singular-Vector-Decomposition)\n",
    "        * [2.4 Training logistic regression classifier on petitioner and respondent differences](##-2.4-Training logistic-regression-classifier-on-petitioner-and-respondent-differences)\n",
    "    * [3. Sentiment Analysis](#-3.-Sentiment-Analysis)\n",
    "        * [3.1 Cleaning dataset](##-3.1-Cleaning-dataset)\n",
    "        * [3.2 Feature eelection and extraction](##-3.2-Feature-selection-and-extraction)\n",
    "        * [3.3 Exploring feature #1: Number of words judges directed at each side](##-3.3-Exploring-feature-#1:-Number-of-words-judges-directed-at-each-side)\n",
    "        * [3.4 Exploring feature #2: Number of justice interruptions to each side](##-3.4)\n",
    "        * [3.5 Exploring feature #3: Sentiment analysis on judge's speeches](##-3.5)\n",
    "    * [4. Justice-Centered Data Analysis](#-4.-Justice-Centered-Data-Analysis)\n",
    "        * [4.1 Feature/variable selection](##-4.1)\n",
    "        * [4.2 Overview of process](##.4.2)\n",
    "        * [4.3 Linear SVM classifier](##.4.3)\n",
    "        * [4.4 Decision trees classifier](##.4.4)\n",
    "        * [4.5 Random forests classifier](##.4.5)\n",
    "    * [5. Conclusions!](#-5.-Conclusions!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "The purpose of this project is to predict votes of Supreme Court justices using oral argument transcripts. Studies in linguistics and psychology, as well as common sense, dictates that the word choices that people make convey crucial information about their beliefs and intentions with regard to issues. Rather than use precedents or formal analysis of the law to predict Supreme Court decisions, we attempt to extract essential emotional features of oral arguments made by justices and advocates in the court."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources\n",
    "* Oral Argument Transcripts - obtained from http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx. Transcripts are made available on the day of court hearing.\n",
    "* Justice Vote Counts/Case Information - obtained from the Supreme Court Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a python script (scraper.py) to first scrape the pdfs from the Supreme Court Justice Website (but didn't upload those to the repository, because we ultimately wanted to use text files in our process). We then used a script to convert the pdf files to text files, but not before removing the last 10 pages from each transcript, which were reserved as an index for certain words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gather all txt files, first get the path to the data directory\n",
    "# then list the files and filter out all non-txt files\n",
    "curPath = os.getcwd()\n",
    "dataPath = curPath + '/data/'\n",
    "fileList = os.listdir(dataPath)\n",
    "fileExt = \".txt\"\n",
    "txtFiles = filter(lambda f : f[-4:] == fileExt, fileList)\n",
    "txtFiles = map(lambda f : dataPath + f, txtFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRawText(txtFiles):\n",
    "    '''\n",
    "    Inputs:\n",
    "    txtFiles: a list of paths of textfiles\n",
    "    \n",
    "    Returns:\n",
    "    list of all uncleaned transcripts in raw text form.\n",
    "    '''\n",
    "    return [(open(txtFiles[i]).read()) for i in range(len(txtFiles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileDict(fileList, fileExt='.txt'):\n",
    "    '''\n",
    "    This function takes the fileList and returns a list of dictionaries of the format \n",
    "    {'case_number': case_number, 'full_text': full_text}\n",
    "    \n",
    "    Inputs:\n",
    "    fileList: list of the paths of the textfiles\n",
    "    fileExt: optional parameter for the type of file\n",
    "    \n",
    "    Returns:\n",
    "    dictionary of the filename:text\n",
    "    '''\n",
    "    fileDict = []\n",
    "    fields = ['docket', 'full_text']\n",
    "    txtFiles_filter = filter(lambda f : f[-4:] == fileExt, fileList)\n",
    "    for each in txtFiles_filter:\n",
    "        name_str=each[4:-4]\n",
    "        try:\n",
    "            indexx=name_str.index('_')\n",
    "            docketNum=name_str[:indexx]\n",
    "        except ValueError:\n",
    "            docketNum=name_str\n",
    "        cur = open(dataPath+each)\n",
    "        textual = cur.read()\n",
    "        cur.close()\n",
    "        tuple_=(docketNum, textual)\n",
    "        fileDict.append(dict(zip(fields, tuple_)))\n",
    "    return fileDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a parser to extract the names of the petitioner and respondant attorneys from the first 2 pages of the converted text document. An example of list of petitioner and respondant speakers, taken from the example case in 2014 of Johnson v United States (docket number 13-7120) is:\n",
    "\n",
    "Katherine M. Menendez, ESQ., Minneapolis, Minn.; on behalf of Petitioner\n",
    "Michael R. Dreeben, ESQ., Deputy Solicitor General, Department of Justice, Washington D.C.; on behalf of Respondent\n",
    "\n",
    "To get these speakers, we write a function that uses a regular expression to split the lines based on new line and checks whether there is a name in the line. If we find a name, then we check if that name was listed as a petitoner or respondent at the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getPetitionersAndRespondents(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text : a transcript in its raw form, without having run cleanTextMaker\n",
    "\n",
    "    Returns:\n",
    "    pet_speakers, res_speakers, other_speakers\n",
    "    the petitoner speakers, the respondent speakers, and any other speakers as a list\n",
    "    '''\n",
    "    #get portion of transcript between APPEARANCES and CONTENTS that specifies speakers for petitioners/respondents\n",
    "    start = text.find('APPEARANCES:') + len('APPEARNACES')\n",
    "    end = text.find('C O N T E N T S')\n",
    "    speakers_text = text[start:end]\n",
    "    split_speakers_text = re.split('\\.[ ]*\\n', speakers_text)\n",
    "    #for each speaker, get name (capitalized) and side (Pet/Res) he/she is speaking for\n",
    "    pet_speakers, res_speakers, other_speakers = [], [], []\n",
    "    for speaker in split_speakers_text:\n",
    "        name = speaker.strip().split(',')[0]\n",
    "        #search for first index of capitalized word (which will be start of speaker name)\n",
    "        start = 0\n",
    "        for idx, char in enumerate(name):\n",
    "            if str.isupper(char):\n",
    "                start = idx\n",
    "                break\n",
    "        #actual name to be appended to correct list\n",
    "        name = name[start:]\n",
    "        \n",
    "        #if words Petition, Plaintiff, etc occur in speaker blurb, speaker belongs to Pet\n",
    "        if any(x in speaker for x in ['etition' , 'ppellant', 'emand', 'evers', 'laintiff']):\n",
    "            pet_speakers.append(name)\n",
    "        #otherwise if words Respondent, Defendant, etc occur, speaker belongs to Res\n",
    "        elif any(x in speaker for x in ['espond' , 'ppellee', 'efendant']):\n",
    "            res_speakers.append(name)\n",
    "        #otherwise if neither side is specified in blurb, speaking belongs to Other\n",
    "        elif 'neither' in speaker:\n",
    "            other_speakers.append(name)\n",
    "    return pet_speakers, res_speakers, other_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function used to generate a regular expression for later use based on \"TITLE. LASTNAME\"; however, that pattern ultimately ended up not being used in some of the cases, and we abandonded this format in favor of just using the last name to generate the regular expression that we would ultimately use to separate the text into which speaker was responsible for a portion of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateRES(nameList, plebe):\n",
    "    \"\"\"\n",
    "    generates a regular expression that splits text based on speaker names\n",
    "    \n",
    "    Inputs:\n",
    "    nameList: a list of strings of the names\n",
    "    plebe: a boolean determining whether or not the list is of justices or not\n",
    "    \n",
    "    Returns:\n",
    "    A list of regular expressions for each of the names that work for supreme\n",
    "    court case transcripts\n",
    "    \"\"\"\n",
    "    retList = []\n",
    "    for name in nameList:\n",
    "        address = \"\"\n",
    "        if plebe:\n",
    "            words = name.split(' ')\n",
    "            # first term is the title, last\n",
    "            # word is the last name\n",
    "            address = words[-1]\n",
    "            retList.append(address)\n",
    "        else:\n",
    "            # Justice can appear in two ways!\n",
    "            address = \"JUSTICE %s\" % name\n",
    "            address2 = \"CHIEF JUSTICE %s\" % name\n",
    "            retList.append(address)\n",
    "            retList.append(address2)\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't yet have a way to gather the names of the Justices from the text, so we do so! We just find the word JUSTICE in all caps and then gather whatever comes after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getJusticeNames(text):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    text is the raw text of a transcript\n",
    "    \n",
    "    Returns:\n",
    "    A set of the names of Justices mentioned by name in the transcript\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    retList = []\n",
    "    while index < len(text):\n",
    "        index = text.find(\"JUSTICE\", index)\n",
    "        if index == -1:\n",
    "            break\n",
    "        index += 8 # because length of JUSTICE is 7, plus length of the space\n",
    "        prevIndex = index\n",
    "        while text[index] != ':':\n",
    "            index +=1\n",
    "        retList.append(text[prevIndex:index])\n",
    "    return list(set(retList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general flow of court proceedings is that the Petitioner attornies make their oral argument, followed by the Respondent attornies, before we hear the rebuttal argument of the Petitioners again. Throughout all proceedings, Justices are free to interject with questions and statements of their own. The below function extracts the main argument portion of the oral transcripts, which is the meat of the proceedings that we are interested in conducting analysis on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getArgumentPortion(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    Raw text of a transcript as a string\n",
    "    \n",
    "    Returns:\n",
    "    The argument portion of the case as a string\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('P R O C E E D I N G S')\n",
    "    end = text.rfind('Whereupon')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countWords(s):\n",
    "    '''\n",
    "    Inputs:\n",
    "    s: string of words\n",
    "    \n",
    "    Returns:\n",
    "    An integer counting the number of the words in s\n",
    "    '''\n",
    "    s = s.split()\n",
    "    non_words = ['-', '--']\n",
    "    return sum([x not in non_words for x in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcripts contain a lot of line numbers as well as linebreaks in between sentences, so we want to remove those before we try and do any analysis on them. We can also use this function to return a list of the cleaned version of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanTextMaker(text):\n",
    "    '''\n",
    "    Inputs:\n",
    "    text: a raw text with newlines and numbers, as is usual in the transcripts\n",
    "    \n",
    "    Returns:\n",
    "    A file with newlines and numbers scrubbed\n",
    "    '''\n",
    "    text_arr=text.splitlines()\n",
    "    text_clean=[]\n",
    "    for each in text_arr:\n",
    "        if each != '':\n",
    "            try:\n",
    "                int(each)\n",
    "            except ValueError: #assummption: if the item only has integers, it is a line number.\n",
    "                text_clean.append(each)\n",
    "    out_text=' '.join(text_clean)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllRawCleanText(txtFiles):\n",
    "    '''\n",
    "    Inputs:\n",
    "    txtFiles: a list of paths of textfiles\n",
    "    \n",
    "    Returns:\n",
    "    list of all cleaned transcripts in raw text form.\n",
    "    '''\n",
    "    return [(cleanTextMaker(open(txtFiles[i]).read())) for i in range(len(txtFiles))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be useful to split data into test and train sets, so lets write a function that does so for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitData(X, fraction_train=9.0 / 10.0):\n",
    "    \"\"\"\n",
    "    Deterministically splits a vector\n",
    "    \n",
    "    Inputs:\n",
    "    X: a one dimensional vector\n",
    "    fraction_train: the fraction of data that is desired to be train\n",
    "    \n",
    "    Returns:\n",
    "    the train portion and test portions of the vector\n",
    "    \"\"\"\n",
    "    end_train = int(len(X) * fraction_train)\n",
    "    X_train = X[0:end_train]\n",
    "    X_test = X[end_train:]\n",
    "    return X_train, X_test\n",
    "\n",
    "def splitTrainTest(X, Y, fraction_train = 9.0 / 10.0):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    X, Y : vectors to be split\n",
    "    \n",
    "    Returns:\n",
    "    Each vector split into train and test\n",
    "    \"\"\"\n",
    "    X_train, X_test = splitData(X, fraction_train)\n",
    "    Y_train, Y_test = splitData(Y, fraction_train)\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSI : Latent Sentiment Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI is a method that uses singular value decomposition to find patterns and themes/topics/concepts between unstructured documents. The key idea behind LSI is to select the conceptual content of some text by finding connections between terms that occur in similar contexts. Once we produce a SVD, we can also truncate the resultant matricies to instead produce smaller matricies that are easier to handle and more likely to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document representation is the first step of our analysis since there are a variety of ways to represent a transcript, which in its raw form is a simple string of texts. We use a pre-processing technique that reduces the complexity of the documents and makes them easier to handle, which is to transform the oral transcripts from the full text version to a document vector/sparse matrix. Every text document is represented as a vector of term weights (word features) from a set of terms (dictionary), where each term occurs at least once in a certain critical number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High dimensionality of text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major characteristic of document classification problems is the extremely high dimensionality of data where the number of potential features often exceeds the number of training documents. Dimensionality reduction is thus critical to allow for efficient data manipulation. Irrelevant and redundant features often degrade performance of classification algorithms both in accuracy and speed, and also tends to fall into the all-common trap of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing of text data involves tokenization of raw text, stop words removal, stemming and eliminating as much as possible the language dependent factors. Brief explanations of these preprocessing stages are as fllows:\n",
    "\n",
    "1. Sentence splitting: identifying sentence boundaries in documents\n",
    "2. Tokenization: partitioning documents that are initially treated as a string into a list of tokens\n",
    "3. Stop word removal: removing common English words like \"the\", \"a\", etc\n",
    "4. Stemming: reducing derived words to its most root form, example happiest -> happy\n",
    "5. Noisy data: cleaning noisy data spilt over from pdf to text conversion, including inclusion of line numbers, page breaks, etc\n",
    "6. Text representation: determining whether we should use words, phrases or entire sentences as a \"token\" for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Feature extraction vs. feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature extraction, feature selection was conducted to construct a vector space of appropriate dimensionality, which improves the scalability, efficiency and accuracy of our classification algorithm. The main idea of feature selection is to choose a subset of features from the original texts, with subset determined by obtaining features with the highest score according to some predetermined measure of feature importance.\n",
    "\n",
    "We use filters to generate our features. Filters can be conducted independently of the actual classification algorithm, and is not very computationally expensive. Filters use an evaluation metric that measures ability of a feature to differentiate each class, hence choosing the most discriminative and valuable features. The filter of our choice is a technique called frequency document-inverse document frequency, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF : Frequency Document-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency document–inverse document frequency (tf-idf), is a powerful method to evaluate how important is a word in a document, and captures the relative significance among words. It converts the textual representation of information into a Vector-Space Model or a sparse matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used to generated tfidf sparse matricies for the importance of words in documents\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# internal utilities used to replicate functionality of truncated_svd\n",
    "from sklearn.utils import as_float_array\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "# stemmer of words\n",
    "import snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allRawText = getAllRawText(txtFiles)\n",
    "allRawCleanText = getAllRawCleanText(txtFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the approach we took when generating the regular expressions for parsing, we want to have a way to convert the names scraped from the documents into a name that we can generally split the speechs by, and thus we adopt the convention of using the last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toColloquialName(formal_name):\n",
    "    ret = formal_name.split()\n",
    "    return ret[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running LSI, we do not want to possibly split importance among words that are actually very similar (such as \"stealing\" and \"steal\"), so we stem words by removing the suffix, bring down the words to a root, or 'stem' that we can assign importance to. There was an issue with different encodings: the txt documents are stored in Latin1 encoding when converted from PDFs to permit earlier functions to work, but when iterating through words there is some issues with how the words are decoded and passed to the stemmer. As a result, we need to manually convert incompatible strings to a tractable format. This conversion was not possible on a single document in our entire database, so we ultimately had to remove it from our database (if we didn't remove it, there would be an issue when we used the docketId from the document to index into a merged dataframe later on, we would have one more response variable than predictor variables). Stemming naturally can leave words as a non english word, or may incorrectly mistem a word. Nothing short of a large dictionary containing the stem of every possible word would accurately perform the stemming, so we are forced to accept this aggressive trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def destem(allRawText):\n",
    "    \"\"\"\n",
    "    stems all words from a list of documents\n",
    "    documents are assumed to be stored in Latin1 encoding\n",
    "    there is one document that is not tractable so we exclude it\n",
    "    uses snowballstemmer\n",
    "    required to decode string to avoid UnicodeDecodeErrors\n",
    "    \n",
    "    Inputs:\n",
    "    a list of raw text files that have been cleaned\n",
    "    \n",
    "    Returns:\n",
    "    a list of text files that have been stemmed word by word\n",
    "    \"\"\"\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    stemmedList = []\n",
    "    for text in allRawText:\n",
    "        try:\n",
    "            temp = stemmer.stemWords(text.split())\n",
    "            for i in xrange(len(temp)):\n",
    "                if str(type(temp[i])) == \"<type 'str'>\":\n",
    "                    temp[i] = temp[i].decode('Latin1')\n",
    "            res = ' '.join(temp)\n",
    "            stemmedList.append(res)\n",
    "        except UnicodeDecodeError:\n",
    "            # literally just one document\n",
    "            pass\n",
    "    return stemmedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcript cases on the first page have a number that yields the docket ID of the case. We can very easily retrieve the docket number by just performing a .find() on the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDocketNo(text):\n",
    "    '''\n",
    "    Input: the text of a transcript\n",
    "    \n",
    "    Returns:\n",
    "    the docket number of the case\n",
    "    '''\n",
    "    cleantext = cleanTextMaker(text)\n",
    "    docketIdx = cleantext.find(\"No.\")\n",
    "    return cleantext[docketIdx+4:].split()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to merge the text files with the supreme court database so that we can easily associate the text files with the docketId and get the decision of the cases. However, the supreme court database unfortunately does not contain information from beyond 2014, so we lose out on several transcripts in the merging process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileDict=get_file_dict(fileList)\n",
    "txtdf = pd.DataFrame(fileDict)\n",
    "casedf = pd.read_csv('supremeCourtDb.csv')\n",
    "merged = pd.merge(left=txtdf, right=casedf, how='inner', left_on='docket', right_on='docket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As referenced earlier, there is a single document that is not tractable to stemming due to codec issues, so we merely drop it for being insolent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop problematic docket\n",
    "merged = merged[merged.docket != '08-351']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931, 54)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Splitting prepared documents into petitioner and respondent speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have about 930 documents to work with. We now want to find a way to gather the texts of what the petitioners and the respondents say. We adapt a function we wrote earlier that counted the number of words that each speaker said and use it to gather the texts that each party is responsible for. We first gather them by speaker then gather them by the group that they are a part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitTextPetRes(text):\n",
    "    '''\n",
    "    Input: \n",
    "    text: raw text document (transcript, recently opened)\n",
    "        \n",
    "    Returns:\n",
    "    a dictionary of speaker: the words they said\n",
    "    a dictionary of group: the words they said\n",
    "    '''\n",
    "    arg_text = get_argument_portion(text)\n",
    "    #keeps track of current speaker\n",
    "    current_speaker = 'N/A'\n",
    "    clean_argument = cleanTextMaker(arg_text)\n",
    "\n",
    "    # first get the names of the judges and speakers\n",
    "    pet_speakers, res_speakers, _ = get_petitioners_and_respondents(text)\n",
    "    \n",
    "    # create the regular expression for the justices and the plebes\n",
    "    petList = generateRES(pet_speakers, True)\n",
    "    resList = generateRES(res_speakers, True)\n",
    "    petList = map(lambda name : name + \":\", petList)\n",
    "    resList = map(lambda name : name + \":\", resList)\n",
    "    all_speakers = (petList + resList)\n",
    "    \n",
    "    RE = '('  + '|'.join(all_speakers) + ')'\n",
    "    \n",
    "    # split argument portion by times elements in plebeList (e.x. MR. FARR: or EUGENE: appears)\n",
    "    split_argument = re.split(RE, clean_argument)\n",
    "    \n",
    "    # dictionary keyed by speaker, with value actual speech (in string format)\n",
    "    speech = dict(zip(all_speakers + [current_speaker], [\"\"] * (len(all_speakers)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating speeches for all speakers\n",
    "    for s in split_argument:\n",
    "        if s in all_speakers:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            speech[current_speaker] += s\n",
    "\n",
    "    #combine all pet and res speakers, if multiple\n",
    "    retDict = {\"resSpeakers\":\"\", \"petSpeakers\":\"\"}\n",
    "    \n",
    "    for rSpeaker in resList:\n",
    "        retDict[\"resSpeakers\"] += speech[rSpeaker]\n",
    "    for pSpeaker in petList:\n",
    "        retDict[\"petSpeakers\"] += speech[pSpeaker]\n",
    "\n",
    "    return speech, retDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the transcripts are not consistently formatted, sometimes we fail to gather the names of petitioners or respondents. In this case, we do not want to add that case's speech to the database because then we wouldnt be able to compare either the respondents or the petitioners against an empty speech. Just would not be fair!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate through merged.full_text, trying to fill in merged.pet_speech and merged.res_speech\n",
    "allPetSpeeches = []\n",
    "allResSpeeches = []\n",
    "allDocketNo = []\n",
    "allDecisions = []\n",
    "for row in merged.iterrows():\n",
    "    speech, retDict = splitTextPetRes(row[1][\"full_text\"])\n",
    "    petSpeech = retDict[\"petSpeakers\"]\n",
    "    resSpeech = retDict[\"resSpeakers\"]\n",
    "    # if either petSpeech or resSpeech is an empty string, do not add to workable dataset\n",
    "    if petSpeech and resSpeech:\n",
    "        allPetSpeeches.append(petSpeech)\n",
    "        allResSpeeches.append(resSpeech)\n",
    "        allDocketNo.append(row[1][\"docket\"])\n",
    "        allDecisions.append(row[1][\"partyWinning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 886, 886)"
      ]
     },
     "execution_count": 1089,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some oral transcripts have empty petitioner or respondent speeches due to dirty scraping of pdf files\n",
    "# for example, get_petitioners_and_respondents sometimes does not scrape properly due to bad formatting\n",
    "len(allPetSpeeches), len(allResSpeeches), len(allDecisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now stem all of the words in the document files. This takes a long time because of the inconsistent encodings of strings as mentioned earlier, necessitating iterating through each word and manually trying to convert it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes long to run\n",
    "allDestemmedPetSpeeches = destem(allPetSpeeches)\n",
    "allDestemmedResSpeeches = destem(allResSpeeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 886)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allDestemmedPetSpeeches), len(allDestemmedResSpeeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Applying term-frequency inverse document frequency vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to pass each set of documents to the vectorizer that will count the number of words and assign them based on Term-Frequency Inverse Document Frequency (tfidf), which first calculates the raw term frequency (aka the number of times that the word appears in the document) and then multiplies it by the inverse document frequency (a global weighing function):\n",
    "$$g_i = \\log_2 \\frac{n}{1 + df_i}$$\n",
    "\n",
    "Where $g_i$ is the weight for term $i$, $n$ is the number of times a word appears in a document, and $df_i$ is the number of documents in which $i$ appears. This properly penalizes words that appear frequently in many documents. We take $g_i \\times f_i$ (where $f_i$ is the raw frequency) as the tfidf statistic for that word in a given document. \n",
    "\n",
    "(credit to: https://en.wikipedia.org/wiki/Latent_semantic_indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer(min_df=1, norm='l2', use_idf=True, stop_words='english', encoding='Latin1', analyzer='word', token_pattern='\\w+')\n",
    "vectorizer2 = TfidfVectorizer(min_df=1, norm='l2', use_idf=True, stop_words='english', encoding='Latin1', analyzer='word', token_pattern='\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Running Singular Vector Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is a method to decompose a matrix into three other matricies that perform the following transformations: a rotation, a scaling along the coordinate axes, and another rotation. Respectively, we label these as T, S, DT in our code.\n",
    "\n",
    "When we run SVD on the matrix generated by the vectorizers, the 3 matricies we get back have a meaning in the context of LSI. The first matrix is a matrix that represents the themes in the documents, the second is a diagonal matrix of singular values representing the relative importance of each theme overall, and the third matrix is representing the importance of each word in the themes. We can run logistic regression on just the first matrix, and specifically the different between the matrix of the respondents and the petitioners, to represent the difference in how strongly the parties speak about a certain theme within a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runSVD(documentList, vizer, numComponents=25, nIter=5):\n",
    "    \"\"\"\n",
    "    takes a list of documents and a vectorizer\n",
    "    converts document list to a matrix of frequencies \n",
    "        (as determined by the vectorizer) of document by word\n",
    "    takes matrix and runs truncated SVD on it to generate\n",
    "    a matrix that consists of themes (T) in each document\n",
    "    overall importance of the word (S)\n",
    "    and a matrix that consists of how important each word\n",
    "    is in the theme (DT)\n",
    "    \n",
    "    this code is partially derived from sklearn's\n",
    "    truncated_svd function (which doesn't return\n",
    "    all of the matricies we are interested in)\n",
    "    \"\"\"\n",
    "    mat = vizer.fit_transform(documentList)\n",
    "    X = as_float_array(mat, copy=False)\n",
    "    # T is the term by concept matrix\n",
    "    # S the singular value matrix\n",
    "    # D is the concept-document matrix\n",
    "    T, S, DT = randomized_svd(X, numComponents, n_iter=nIter)\n",
    "    return T, S, DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tPet, sPet, dTPet = runSVD(allDestemmedPetSpeeches, vectorizer1, numComponents=25)\n",
    "tRes, sRes, dTRes = runSVD(allDestemmedResSpeeches, vectorizer2, numComponents=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((886, 25), (25,), (25, 29308))"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print tPet.shape, sPet.shape, dTPet.shape\n",
    "print tRes.shape, sRes.shape, dTRes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Training logistic regression classifier on petitioner and respondent differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested earlier, we run logisitic regression on the difference of the two matricies that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run logistic regression on D x numTopics matrix of independent variables, vs. 0/1 result vector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    From CS109's problem sets\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "    clf : an instance of a scikit-learn classifier\n",
    "    parameters: a parameter grid dictionary thats passed to GridSearchCV (see above)\n",
    "    X: a samples-features matrix in the scikit-learn style\n",
    "    y: the response vectors of 1s and 0s (+ives and -ives)\n",
    "    n_folds: the number of cross-validation folds (default 5)\n",
    "    score_func: a score function we might want to pass (default python None)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    The best estimator from the GridSearchCV, after the GridSearchCV has been used to\n",
    "    fit the model.\n",
    "    \"\"\"\n",
    "    clf = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    clf.fit(X,y)\n",
    "    return clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression, our good old friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clflog = LogisticRegression()\n",
    "tDiff = tPet - tRes\n",
    "xTrain, yTrain, xTest, yTest = splitTrainTest(tDiff, allDecisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clflogopt = cv_optimize(clflog, {\"C\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, xTrain, yTrain, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clflogopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692597239649 0.651685393258\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = clflogopt.score(xTrain, yTrain)\n",
    "test_accuracy = clflogopt.score(xTest, yTest)\n",
    "print training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so maybe we are not at the point where we can replace judges just yet. However, there were some issues that may have hindered our accuracy. For example, we had to get rid of some documents when we were merging document texts with databases, because we didn't have data from decisions in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above analysis, we have focused on parts of the oral transcripts corresponding to attorney speeches, which means we are essentially ignoring an equally valuable portion of information that we can glean from these transcripts: the judge's responses and questions to these attorneys. In this section, we attempt to conduct Natural Language Processing on judge's speeches and other salient features of the text that were not included in the Latent Semantic Analysis above, which might yield interesting results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Cleaning dataset\n",
    "Some of our oral transcripts do not discriminate between individual justice's speeches, instead using \"QUESTION:\" in place of all justice's speeches. We are not unable to use these, and for consistency's sake, delete all these transcripts from our working dataset. It turns out that this was not too consequential since only transcripts from 2001-2002 had the \"QUESTION:\" ambiguity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find qualifying rows without \"QUESTION:\"\n",
    "qualifyingRows = []\n",
    "for rowNo in range(len(merged)):\n",
    "    # find appropriate row and check whether full_text contains QUESTION: - if so, delete from database\n",
    "    row = merged.iloc[rowNo, :]\n",
    "    if row[\"full_text\"].find(\"QUESTION:\") == -1:\n",
    "        qualifyingRows.append(rowNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docket</th>\n",
       "      <th>full_text</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseIssuesId</th>\n",
       "      <th>voteId</th>\n",
       "      <th>dateDecision</th>\n",
       "      <th>decisionType</th>\n",
       "      <th>usCite</th>\n",
       "      <th>sctCite</th>\n",
       "      <th>...</th>\n",
       "      <th>authorityDecision1</th>\n",
       "      <th>authorityDecision2</th>\n",
       "      <th>lawType</th>\n",
       "      <th>lawSupp</th>\n",
       "      <th>lawMinor</th>\n",
       "      <th>majOpinWriter</th>\n",
       "      <th>majOpinAssigner</th>\n",
       "      <th>splitVote</th>\n",
       "      <th>majVotes</th>\n",
       "      <th>minVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>02-1472</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-025</td>\n",
       "      <td>2004-025-01</td>\n",
       "      <td>2004-025-01-01</td>\n",
       "      <td>2004-025-01-01-01</td>\n",
       "      <td>3/1/05</td>\n",
       "      <td>1</td>\n",
       "      <td>543 U.S. 631</td>\n",
       "      <td>125 S. Ct. 1172</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>25 U.S.C. � 450</td>\n",
       "      <td>110</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>02-1672</td>\n",
       "      <td>1\\n2\\n\\nIN THE SUPREME COURT OF THE UNITED STA...</td>\n",
       "      <td>2004-033</td>\n",
       "      <td>2004-033-01</td>\n",
       "      <td>2004-033-01-01</td>\n",
       "      <td>2004-033-01-01-01</td>\n",
       "      <td>3/29/05</td>\n",
       "      <td>1</td>\n",
       "      <td>544 U.S. 167</td>\n",
       "      <td>125 S. Ct. 1497</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>03-10198</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-073</td>\n",
       "      <td>2004-073-01</td>\n",
       "      <td>2004-073-01-01</td>\n",
       "      <td>2004-073-01-01-01</td>\n",
       "      <td>6/23/05</td>\n",
       "      <td>1</td>\n",
       "      <td>545 U.S. 605</td>\n",
       "      <td>125 S. Ct. 2582</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>03-1039</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-032</td>\n",
       "      <td>2004-032-01</td>\n",
       "      <td>2004-032-01-01</td>\n",
       "      <td>2004-032-01-01-01</td>\n",
       "      <td>3/22/05</td>\n",
       "      <td>1</td>\n",
       "      <td>544 U.S. 133</td>\n",
       "      <td>125 S. Ct. 1432</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>03-1116</td>\n",
       "      <td>1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...</td>\n",
       "      <td>2004-045</td>\n",
       "      <td>2004-045-01</td>\n",
       "      <td>2004-045-01-01</td>\n",
       "      <td>2004-045-01-01-01</td>\n",
       "      <td>5/16/05</td>\n",
       "      <td>1</td>\n",
       "      <td>544 U.S. 460</td>\n",
       "      <td>125 S. Ct. 1885</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       docket                                          full_text    caseId  \\\n",
       "158   02-1472  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-025   \n",
       "169   02-1672  1\\n2\\n\\nIN THE SUPREME COURT OF THE UNITED STA...  2004-033   \n",
       "216  03-10198  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-073   \n",
       "218   03-1039  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-032   \n",
       "220   03-1116  1\\n\\nIN THE SUPREME COURT OF THE UNITED STATES...  2004-045   \n",
       "\n",
       "        docketId    caseIssuesId             voteId dateDecision  \\\n",
       "158  2004-025-01  2004-025-01-01  2004-025-01-01-01       3/1/05   \n",
       "169  2004-033-01  2004-033-01-01  2004-033-01-01-01      3/29/05   \n",
       "216  2004-073-01  2004-073-01-01  2004-073-01-01-01      6/23/05   \n",
       "218  2004-032-01  2004-032-01-01  2004-032-01-01-01      3/22/05   \n",
       "220  2004-045-01  2004-045-01-01  2004-045-01-01-01      5/16/05   \n",
       "\n",
       "     decisionType        usCite          sctCite    ...    authorityDecision1  \\\n",
       "158             1  543 U.S. 631  125 S. Ct. 1172    ...                     4   \n",
       "169             1  544 U.S. 167  125 S. Ct. 1497    ...                     4   \n",
       "216             1  545 U.S. 605  125 S. Ct. 2582    ...                     7   \n",
       "218             1  544 U.S. 133  125 S. Ct. 1432    ...                     7   \n",
       "220             1  544 U.S. 460  125 S. Ct. 1885    ...                     2   \n",
       "\n",
       "    authorityDecision2  lawType  lawSupp         lawMinor majOpinWriter  \\\n",
       "158                NaN        6      600  25 U.S.C. � 450           110   \n",
       "169                NaN        3      322              NaN           104   \n",
       "216                  2        2      231              NaN           109   \n",
       "218                  4        3      341              NaN           106   \n",
       "220                NaN        1      111              NaN           106   \n",
       "\n",
       "    majOpinAssigner splitVote  majVotes  minVotes  \n",
       "158             103         1         8         0  \n",
       "169             103         1         5         4  \n",
       "216             102         1         7         2  \n",
       "218             104         1         5         3  \n",
       "220             105         1         5         4  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedJudges = merged.iloc[qualifying_rows, :] \n",
    "mergedJudges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Selection and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cleaned dataframe with information about raw text and decisions for every oral transcript, we want to find the most appropriate features for predictors - we're on the stage of feature extraction again. After much experimentation and linguistical analysis on court proceedings in particular, we identified the following features that we want to introduce as predictors to run our classification algorithm on. \n",
    "\n",
    "For each oral transcript, we want to identify:\n",
    "1. Number of words judges uttered to petitioner's and respondent's sides:\n",
    "        Usage: judges_word_count_split(text)\n",
    "2. Number of times a judge interrupted petitioner or respondent attorneys: \n",
    "        Usage: total_interruptions_pet, total_interruptions_ret = get_total_interruptions(text)\n",
    "3. Sentiment analysis on words judges uttered to petitioner's and respondent's sides: \n",
    "        Usage: judges_speech_split(text) and sentiment analysis using existing dictionary of positive/negative\n",
    "        words\n",
    "\n",
    "We will look at each of these features one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Exploring feature #1: Number of words judges directed at each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSplitArgument(text):\n",
    "    '''\n",
    "    Function:\n",
    "    Gets split argument by RE generated with list of all speakers. Used in all 3 features below, henced factored out.\n",
    "    \n",
    "    Inputs:\n",
    "    text in its raw form (entire oral transcript)\n",
    "    \n",
    "    Returns:\n",
    "    splitArg: argument portion of script split by our generated regular expression that searches for all instances\n",
    "        of speaker names that indicate switches in speakers. This means each element in splitArg could either \n",
    "        contain name of new speaker name or content of speech, which will help us when we iterate through splitArg.\n",
    "    allSpeakersColon, petSpeakersColon, resSpeakersColon, justiceSpeakersColon: speaker names belong to each camp\n",
    "        (all speakers, pet speakers, res speakers, justices) in an appropriate format for subsequent parsing\n",
    "    '''\n",
    "    # first get the names of the judges and speakers\n",
    "    petSpeakers, resSpeakers, _ = getPetitionersAndRespondents(text)\n",
    "    cleanArg = cleanTextMaker(text)\n",
    "    justiceSpeakers = getJusticeNames(cleanArg)\n",
    "    \n",
    "    # creates duplicate \"JUSTICE\" and \"CHIEF JUSTICE\"; creates RE for pet/res/justices\n",
    "    justiceRE = generateRES(justiceSpeakers, False)\n",
    "    petRE = generateRES(petSpeakers, True)\n",
    "    resRE = generateRES(resSpeakers, True)\n",
    "    \n",
    "    # appends colons to all REs\n",
    "    justiceSpeakersColon = map(lambda name : name + \":\", justiceRE)\n",
    "    petSpeakersColon = map(lambda name : name + \":\", petRE)\n",
    "    resSpeakersColon = map(lambda name : name + \":\", resRE)\n",
    "    justiceSpeakersColon = map(lambda name : name + \":\", justiceRE)\n",
    "    \n",
    "    # aggregates justice and attorney REs\n",
    "    allSpeakers = [\"QUESTION\"]\n",
    "    allSpeakers += (justiceRE + petRE + resRE)\n",
    "    allSpeakersColon = map(lambda name : name + \":\", allSpeakers)\n",
    "    \n",
    "    # finally, creates regular expression for the justices and attorneys (i.e. all_speakers) to split text on\n",
    "    RE = '('  + '|'.join(allSpeakersColon) + ')'\n",
    "    \n",
    "    # splits argument portion according to generated regular expression above that consists of all possible speakers,\n",
    "    # plebes and judges alike\n",
    "    splitArg = re.split(RE, cleanArg)\n",
    "    \n",
    "    return splitArg, allSpeakersColon, petSpeakersColon, resSpeakersColon, justiceSpeakersColon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def speakersWordCount(text):\n",
    "    '''\n",
    "    FEATURE # 1:\n",
    "    Total number of words spoken by each justice to pet and res in the entire transcript.\n",
    "    \n",
    "    Returns:\n",
    "    numWordsPet: a dictionary with key being name of justice/attorney and value being total number of words they\n",
    "    spoke to petitioners throughout argument\n",
    "    numWordsRes: a dictionary with key being name of justice/attorney and value being total number of words they\n",
    "    spoke to respondents throughout argument\n",
    "    '''\n",
    "    # splits argument\n",
    "    splitArg, allSpeakersColon, petSpeakersColon, resSpeakersColon, justiceSpeakersColon = getSplitArgument(text)\n",
    "    \n",
    "    # num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    currSpeaker = 'NA:'\n",
    "    prevSpeaker = 'NA:'\n",
    "    numWordsToPet = dict(zip(allSpeakersColon + [currSpeaker], [0] * (len(allSpeakersColon)+1)))\n",
    "    numWordsToRes = dict(zip(allSpeakersColon + [currSpeaker], [0] * (len(allSpeakersColon)+1)))\n",
    "    \n",
    "    # iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in splitArg:\n",
    "        #if split chunk signifies change in speaker\n",
    "        if s in allSpeakersColon:\n",
    "            prevSpeaker = currSpeaker\n",
    "            currSpeaker = s\n",
    "        #if split chunk is part of speech of current speaker\n",
    "        else:\n",
    "            # if current speaker is a justice and he is speaking to a petitioner\n",
    "            if currSpeaker in justiceSpeakersColon and prevSpeaker in petSpeakersColon:\n",
    "                numWordsToPet[currSpeaker] += countWords(s)\n",
    "            # if current speaker is a justice and he is speaking to a respondents\n",
    "            elif currSpeaker in justiceSpeakersColon and prevSpeaker in resSpeakersColon:\n",
    "                numWordsToRes[currSpeaker] += countWords(s)\n",
    "\n",
    "    return numWordsToPet, numWordsToRes, petSpeakersColon, resSpeakersColon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deleteVal(dictionary,val):\n",
    "    '''\n",
    "    Function:\n",
    "    Handy helper to get rid of all items in dictionary with value being a specific val: \n",
    "    E.x. when no words spoken means that we can ignore them\n",
    "    '''\n",
    "    for k,v in dictionary.items():\n",
    "        if v == val:\n",
    "            del dictionary[k]\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumWords(text):\n",
    "    numWordsToPet, numWordsToRes, petSpeakersColon, resSpeakersColon = speakersWordCount(text)\n",
    "    numWordsToPet = deleteVal(numWordsToPet,0)\n",
    "    numWordsToRes = deleteVal(numWordsToRes,0)\n",
    "    # clump together petitioners and respondents\n",
    "    numWordsPet, numWordsRes = 0,0\n",
    "    for s in petSpeakersColon:\n",
    "        if s in numWords:\n",
    "            numWordsPet += numWords[s]\n",
    "    for s in resSpeakersColon:\n",
    "        if s in numWords:\n",
    "            numWordsRes += numWords[s]\n",
    "    return numWordsPet, numWordsRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can see from this illustrative example that the total number of words corresponds to roughly the sum of words\n",
    "# each judge said to each side. There might have been words uttered to speakers neither on petitioner or respondent's\n",
    "# side, or a prologue directed to the general audience (especially for the Chief Justice), which can be ignored for\n",
    "# our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to put our newly derived features into our supplemented dataframe!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append NEW FEATURES to mergedJudges!!!\n",
    "allNumWordsPet = map(lambda x: getNumWords(x)[0], mergedJudges.full_text.values)\n",
    "allNumWordsRes = map(lambda x: getNumWords(x)[1], mergedJudges.full_text.values)\n",
    "mergedJudges[\"allNumWordsPet\"] = allNumWordsPet\n",
    "mergedJudges[\"allNumWordsRes\"] = allNumWordsRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Exploring feature #2: Number of justice interruptions to each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getNumInterruptions(text):\n",
    "    '''\n",
    "    FEATURE # 2 HELPER:\n",
    "    Number of times a judge interrupted petitioner or respondent attorneys in the entire transcript.\n",
    "    \n",
    "    Returns:\n",
    "    numInterruptions: a dictionary with key being name of justice/attorney speaking and value being total number of times that \n",
    "        speaker was interrupted\n",
    "    '''\n",
    "    # define the sign for an interruption at end of speech\n",
    "    interruptions = [\"-\", \"--\"]\n",
    "    \n",
    "    # get split argument\n",
    "    splitArg, allSpeakersColon, petSpeakersColon, resSpeakersColon, \\\n",
    "        justiceSpeakersColon = getSplitArgument(text)\n",
    "    \n",
    "    # num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    currSpeaker = 'NA:'\n",
    "    numInterruptions = dict(zip(allSpeakersColon + [currSpeaker], [0] * (len(allSpeakersColon)+1)))\n",
    "    \n",
    "    # iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in splitArg:\n",
    "        # if split chunk signifies change in speaker to a justice speaking!\n",
    "        if s in allSpeakersColon:\n",
    "            currSpeaker = s\n",
    "        # if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            # if speech contains at least 2 words, check whether the last or second-last contians an interruption\n",
    "            # the interruption could come as the 2nd last word when the last word is part of the first name of\n",
    "            # next speaker\n",
    "            if len(s.split()) >= 2:\n",
    "                if s.split()[-1] in interruptions or s.split()[-2] in interruptions:\n",
    "                    numInterruptions[currSpeaker] += 1\n",
    "    \n",
    "    return numInterruptions, petSpeakersColon, resSpeakersColon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numInterruptions, petSpeakersColon, resSpeakersColon = getNumInterruptions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FINAL FEATURE #2 FUNCTION\n",
    "def getTotalInterruptions(text):\n",
    "    numInterruptions, petSpeakersColon, resSpeakersColon = getNumInterruptions(text)\n",
    "    # now we need to clump petitioner and respondent interruptions together\n",
    "    totalInterruptionsPet = sum([numInterruptions[k] for k in petSpeakersColon])\n",
    "    totalInterruptionsRes = sum([numInterruptions[k] for k in resSpeakersColon])\n",
    "    return totalInterruptionsPet, totalInterruptionsRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example: these are the total interruptions for pet and res respectively\n",
    "getTotalInterruptions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# again, append NEW FEATURES to mergedJudges!!!\n",
    "allInterruptionsPet = map(lambda x: getTotalInterruptions(x)[0], mergedJudges.full_text.values)\n",
    "allInterruptionsRes = map(lambda x: getTotalInterruptions(x)[1], mergedJudges.full_text.values)\n",
    "mergedJudges[\"allInterruptionsPet\"] = allInterruptionsPet\n",
    "mergedJudges[\"allInterruptionsRes\"] = allInterruptionsRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Exploring feature #3: Sentiment analysis on judge's speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def judgesSpeechSplit(txt):\n",
    "    '''\n",
    "    FEATURE # 3: All words said by each justice to petitioners and respondents in the entire transcript.\n",
    "    \n",
    "    Returns:\n",
    "    num_words: a dictionary with key being name of justice/attorney and value being total number of words they\n",
    "        spoke in total throughout argument.  \n",
    "    '''\n",
    "    # first get the names of the judges and speakers\n",
    "    petSpeakers, resSpeakers, otherSpeakers = getPetitionersAndRespondents(txt)\n",
    "    cleanArg = cleanTextMaker(txt)\n",
    "    justiceSpeakers = getJusticeNames(cleanArg)\n",
    "    \n",
    "    # creates duplicate \"JUSTICE\" and \"CHIEF JUSTICE\"; creates RE for pet/res/justices\n",
    "    justiceRE = generateRES(justiceSpeakers, False)\n",
    "    petRE = generateRES(petSpeakers, True)\n",
    "    resRE = generateRES(resSpeakers, True)\n",
    "    \n",
    "    # appends colons to all REs\n",
    "    justiceSpeakersColon = map(lambda name : name + \":\", justiceRE)\n",
    "    petSpeakersColon = map(lambda name : name + \":\", petRE)\n",
    "    resSpeakersColon = map(lambda name : name + \":\", resRE)\n",
    "\n",
    "    # aggregates justice and attorney REs\n",
    "    allSpeakers = [\"QUESTION\"]\n",
    "    allSpeakers += (justiceRE + petRE + resRE)\n",
    "    allSpeakersColon = map(lambda name : name + \":\", allSpeakers)\n",
    "    \n",
    "    # finally, creates regular expression for the justices and attorneys (i.e. all_speakers) to split text on\n",
    "    RE = '('  + '|'.join(allSpeakersColon) + ')'\n",
    "    \n",
    "    # splits argument portion according to generated regular expression above that consists of all possible speakers,\n",
    "    # plebes and judges alike\n",
    "    splitArg = re.split(RE, cleanArg)\n",
    "    \n",
    "    # num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    prevSpeaker = 'NA:'\n",
    "    currSpeaker = 'NA:'\n",
    "    wordsToPet = dict(zip(allSpeakersColon + [currSpeaker], [\"\"] * (len(allSpeakersColon)+1)))\n",
    "    wordsToRes = dict(zip(allSpeakersColon + [currSpeaker], [\"\"] * (len(allSpeakersColon)+1)))\n",
    "    \n",
    "    # iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in splitArg:\n",
    "        # if split chunk signifies change in speaker to a justice speaking!\n",
    "        if s in allSpeakersColon:\n",
    "            prevSpeaker = currSpeaker\n",
    "            currSpeaker = s\n",
    "        # if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            if currSpeaker in justiceSpeakersColon and prevSpeaker in petSpeakersColon:\n",
    "                wordsToPet[currSpeaker] += s\n",
    "            elif currSpeaker in justiceSpeakersColon and prevSpeaker in resSpeakersColon:\n",
    "                wordsToRes[currSpeaker] += s\n",
    "    \n",
    "    return wordsToPet, wordsToRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combineSpeeches(dictionary):\n",
    "    # joins strings that are values of a particular dictionary (want to use on words_to_pet, words_to_res)\n",
    "    return \" \".join(dictionary.values()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureThreeHelper(txt):\n",
    "    # we obtain dictionaries keyed by justice with values equivalent to the stitched together speeches of every time\n",
    "    # that justice spoke up in the court proceedings\n",
    "    wordsToPet,  wordsToRes = judgesSpeechSplit(txt)\n",
    "    wordsToPet = deleteVal(wordsToPet, \"\")\n",
    "    wordsToRes = deleteVal(wordsToRes, \"\")\n",
    "    wordsToPet = combine_speeches(wordsToPet)\n",
    "    wordsToRes = combine_speeches(wordsToRes)\n",
    "    return wordsToPet, wordsToRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsToPet, wordsToRes = judgesSpeechSplit(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the words judges spoke to the petitioner side: \n",
      "\n",
      "Well, did we take this case on the ground that he wasn't adequately advised or did we 1111 14th Street, NW Suite 400 Alderson Reporting Company 1-800-FOR-DEPO Washington, DC 20005 take the case on the ground that even if he were advised, he'd still have his right? MR.  What the Well, I -- I take it you would challenge the validity of the waiver even if he were advised? MR.  Absolutely. And even if he said, I hereby waive? MR.  How -- why is that prejudicial to him? did it but I can't plead guilt\n",
      "-------------------------\n",
      "And here are the words judges spoke to the respondents side: \n",
      "\n",
      "MR.  Because of the error in scoring? MR.  That -- that ultimately You take the position that in fact there was no error in scoring. MR.   Look, the -- imagine -- I'm just repeating what Justice Souter said. it's so obvious that there must be an obvious answer, but I haven't heard the answer. There must be - He knows Michigan law or his lawyer does. Michigan lawyer looks at the statute. who pleads guilty shall not have appellate counsel appointed for review with some exceptions, which they claim\n"
     ]
    }
   ],
   "source": [
    "print \"Here are the words judges spoke to the petitioner side: \\n\"\n",
    "print wordsToPet[:500]\n",
    "print \"-------------------------\"\n",
    "print \"And here are the words judges spoke to the respondents side: \\n\"\n",
    "print wordsToRes[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureThree(txt):\n",
    "    petSAscore = sentimentAnalysis(words_to_pet)\n",
    "    resSAscore = sentimentAnalysis(words_to_res)\n",
    "    return petSAscore, resSAscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Exploring Sentiment Analysis of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pattern Python library with tools for scraping text and natural language processing. In the getParts function, we will implement the parsing of separate petitioners and respondents' text by tokenizing, parsing out the punctuation, removing stop words in English, and finally returning us information about nouns and descriptives that are most salient in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text \n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "punctuation = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from hw5! :D\n",
    "def getParts(thetext):\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            #print token\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stopwords or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all judge's speeches to petitioners and respondents from our initial dataframe: mergedJudges\n",
    "allToPetSpeeches = map(lambda t: featureThreeHelper(t)[0], mergedJudges.full_text.values)\n",
    "allToResSpeeches = map(lambda t: featureThreeHelper(t)[1], mergedJudges.full_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653, 653)"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allToPetSpeeches), len(allToResSpeeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 46s, sys: 2.59 s, total: 4min 49s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get parsed speeches for all speeches to petitioners and respondents, separately\n",
    "parsedPet = [getParts(t) for t in allToPetSpeeches]\n",
    "parsedRes = [getParts(t) for t in allToResSpeeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking all the descriptives from the transcript\n",
    "nbDataPet = [each[1] for each in parsedPet]\n",
    "nbDataRes = [each[1] for each in parsedRes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# right now the list is flattened and duplicates are not dropped since we are counting frequency of appearance\n",
    "flattenedPet = map(lambda x: list(itertools.chain(*x)), nbDataPet)\n",
    "flattenedRes = map(lambda x: list(itertools.chain(*x)), nbDataRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding the flattened list of descriptives to the dataframe as columns\n",
    "mergedJudges['descriptivesPet'] = pd.Series(flattenedPet, index=mergedJudges.index)\n",
    "mergedJudges['descriptivesRes'] = pd.Series(flattenedRes, index=mergedJudges.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load opinion lexicon\n",
    "posLink = \"idvsus-backup/opinion-lexicon-English/positive-words.txt\"\n",
    "negLink = \"idvsus-backup/opinion-lexicon-English/negative-words.txt\"\n",
    "\n",
    "def getBothList(posLink, negLink):\n",
    "    '''\n",
    "    Inputs:\n",
    "    The links in for the lexicon files downloaded from https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n",
    "    \n",
    "    Outputs:\n",
    "    Two lists: posList and negList representing lists of positive and negative words appearing in each transcript\n",
    "    '''\n",
    "    posFile = open(posLink, \"r\")\n",
    "    negFile = open(negLink, \"r\")\n",
    "\n",
    "    posList = posFile.read()\n",
    "    negList = negFile.read()\n",
    "    \n",
    "    posList = getPosLexicon(posList)\n",
    "    negList = getNegLexicon(negList)\n",
    "\n",
    "    posList = posList.split('\\n')\n",
    "    negList = negList.split('\\n')\n",
    "\n",
    "    return posList, negList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNegLexicon(text):\n",
    "    '''\n",
    "    Gets negative-sentiment words from the list\n",
    "    '''\n",
    "    # start and end defines bounds of argument portion of text\n",
    "    start = text.find('2-faced')\n",
    "    end = text.rfind('zombie')\n",
    "    return text[start:end]\n",
    "\n",
    "def getPosLexicon(text):\n",
    "    '''\n",
    "    Gets positive-sentiment words from the list.\n",
    "    '''\n",
    "    # start and end defines bounds of argument portion of text\n",
    "    start = text.find('a+')\n",
    "    end = text.rfind('zippy')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get posList and negList for the next step\n",
    "posList, negList = getBothList(posLink, negLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptivesPet = mergedJudges.descriptivesPet.values\n",
    "descriptivesRes = mergedJudges.descriptivesRes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the positive words spoken to petitioners: \n",
      "\n",
      "[u'significant', u'helpful', u'clear', u'articulate', u'best', u'better', u'important', u'articulate', u'intuitive', u'great', u'tough']\n",
      "---------------------\n",
      "These are the negative words spoken to petitioners: \n",
      "\n",
      "[u'reckless', u'culpable', u'drunk', u'forceful', u'violent', u'violent', u'embarrassing', u'violent', u'odd', u'drunk', u'accidental', u'arbitrary', u'bad', u'blind', u'dangerous', u'difficult', u'false', u'unlawful', u'unlawful', u'explosive', u'drunk', u'assault', u'inconsistent', u'blind']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idzhang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "# for example, here are the list of positive and negative words in a randomly selected descriptive list for petitioners\n",
    "d = descriptivesPet[0]\n",
    "pos, neg = splitDescriptivePosNeg(d, posList, negList)\n",
    "print \"These are the positive words spoken to petitioners: \\n\"\n",
    "print pos\n",
    "print \"---------------------\"\n",
    "print \"These are the negative words spoken to petitioners: \\n\"\n",
    "print neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNetPositivity(descriptive, posList, negList):\n",
    "    '''\n",
    "    Inputs:\n",
    "    descriptive: list of salient descriptive words taken from a transcript\n",
    "    posList: list of all positive words from lookup dictionary\n",
    "    negList: list of all negative words from lookup dictionary\n",
    "    \n",
    "    Returns:\n",
    "    net positivity of that descriptive, using sentiment analysis techniques\n",
    "    '''\n",
    "    pos, neg = [], []\n",
    "    for word in descriptive:\n",
    "        if word in posList:\n",
    "            pos.append(word)\n",
    "        elif word in negList:\n",
    "            neg.append(word)\n",
    "    return len(pos) - len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netPosPet = map(lambda x: findNetPositivity(x, posList, negList), mergedJudges.descriptivesPet.values)\n",
    "netPosRes = map(lambda x: findNetPositivity(x, posList, negList), mergedJudges.descriptivesRes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# append netPosPet and netPosRes to merged as a column as NEW FEATURES\n",
    "mergedJudges['netPosPet'] = pd.Series(netPosPet, index=mergedJudges.index)\n",
    "mergedJudges['netPosRes'] = pd.Series(netPosRes, index=mergedJudges.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Running classifier on combined features #1, #2, #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sneak peek at what our completed dataframe looks like now!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combining all features from #1, #2, #3 in a consolidated predictor space\n",
    "X = mergedJudges[[\"netPosPet\", \"netPosRes\", \"allInterruptionsPet\", \"allInterruptionsRes\", \"allNumWordsPet\", \"allNumWordsRes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = mergedJudges[\"partyWinning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome!!! We have all our pain-stakingly selected features in a predictor matrix and our target values - now it's time to run our classifier! Let's simply run Logistic Regression because we are trying to predict binary outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clflog2 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain, yTrain, xTest, yTest = splitTrainTest(X, y)\n",
    "clflogopt2 = cv_optimize(clflog, {\"C\": [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, xTrain, yTrain, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is our optimal classifier after cross-validation\n",
    "clflogopt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695059625213 0.636363636364\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = clflogopt2.score(xTrain, yTrain)\n",
    "test_accuracy = clflogopt2.score(xTest, yTest)\n",
    "print training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Justice-Centered Data Analysis without NLP incorporated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach with the CSV from http://supremecourtdatabase.org/ is to understand how the supreme court dataset can be manipulated to understand how Justices vote. A justice centered approach allows us to understand decisions This process is mainly exploratory and knowledge gaining as it will help us come in with a better understanding of the Supreme Court when performing NLP analysis.\n",
    "\n",
    "The most important aspect of classifications is determing which variable will return proper output. These variables were chosen after consulting many webpages on the supreme court, reading the documentation, using domain knowledge, and implementing our own intuition. The following variable were found to be signifigant the interpretation of how it will help us predict follow the explanation of the variable itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Feature/variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* term: This variable identifies the term in which the Court handed down its decision. The term itself is essentially a timeline of when the case was heard. This would be the same as using the date. We found with the political landscape of the us being very variable with time and court hearings dealing with many hearings of modern culture, the term was an important aspect to work with. \n",
    "* naturalCourt: A natural court is a period during which no personnel change occurs. Scholars have subdivided them into \"strong\" and \"weak\" natural courts, but no convention exists as to the dates on which they begin and end. Options include 1) date of confirmation, 2) date of seating, 3) cases decided after seating, and 4) cases argued and decided after seating.  Natural courts would tend to favor certain decisions because they have grown accoustomed to working together so we found it necessary to classify with them as they can indicate certain behavior within the court itself.\n",
    "* petitioner: Petitioner\" refers to the party who petitioned the Supreme Court to review the case. This party is variously known as the petitioner or the appellant. The petitioner can come from many sources and at times the source can be the same so depending on where the source comes from it can help to know if they consistently go to the supereme court and win or consistenly go and lose. \n",
    "* respondent: Respondent\" refers to the party being sued or tried and is also known as the appellee. The reasoning for choosing this variable is the same as peitioner some respondents may be more success with the court while others are not. \n",
    "* caseOrigin: The focus of this variable is the court in which the case originated. We foudn this variable to be important because certain courts bringing cases to the court can show a consistency in how the Supreme court replies. \n",
    "* caseSource: This variable identifies the court whose decision the Supreme Court reviewed. The supreme court viewing the decision can show what will happen when they view the case. \n",
    "* lcDisposition: This variable specifies the treatment the court whose decision the Supreme Court reviewed accorded the decision of the court it reviewed; e.g., whether the court below the Supreme Court---typically a federal court of appeals or a state supreme court---affirmed, reversed, remanded, etc. the decision of the court it reviewed---typically a trial court. This is an important variable because the decisions of the smaller court can create a bias for the supreme court. \n",
    "* issueArea: This variable simply separates the issues identified in the preceding variable (issue) into the following larger categories: criminal procedure, civil rights, First Amendment, due process, privacy , attorneys' or governmental officials' fees or compensation, unions, economic activity, judicial power, federalism, interstate relation, federal taxation, miscellaneous, and private law. The issueArea is also important because the Supreme court can have a tendency to uphold certain juridictions such supporting respondents in patent cases. \n",
    "* winningParty: Variable designating who the winning part was. (Respondent, Peititioner)\n",
    "* majority: a variable designating if a justice voted with the majority. We used this to extract how the justice voted as outlined below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Overview of process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for the above was arduous as the domain knowledge for the group was initially low. We used supplements such as http://www.supremecourt.gov/, and https://en.wikipedia.org/wiki/Supreme_Court_of_the_United_States to learn more about the court and the proceedings that occur. Moreover, the process of running models over again with different variables was avoided after realizing the dataset contained many variable that were outcome dependent. Moreover, many variables werevary consistent among data that we removed it as a new change in those variables can ruin accuracy on our models. For instance, the chiefJustice variable is a variable that can be used but ought not to be as a new the chief justice can be hard to account for in modern times. We needed to carefully craft what we would use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following function is used to convert any noninteger value to an integer if it needed to be regressed on. \n",
    "def encode_variable(df, target_column):\n",
    "    \"\"\"Encode variable into a number.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    df_mod[target_column] = df_mod[target_column].replace(map_to_int)\n",
    "\n",
    "    return (df_mod, targets)\n",
    "\n",
    "#from graphviz documentation, creates a png of a decision tree\n",
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62929, 10)\n"
     ]
    }
   ],
   "source": [
    "#create the target, and train\n",
    "#read  the csv and make a big df\n",
    "bigdf=pd.read_csv(\"SCDB_2015_01_justiceCentered_Citation.csv\")\n",
    "\n",
    "#create an empty small df\n",
    "smalldf = pd.DataFrame()\n",
    "\n",
    "#select the variables to run the classifier on \n",
    "#casedisposition is our target\n",
    "train_vars = ['term', 'naturalCourt', 'petitioner',\n",
    "                'respondent', 'caseOrigin', 'caseSource', 'lcDisposition', 'issueArea', 'partyWinning', 'majority']\n",
    "\n",
    "# [\"term\", \"naturalCourt \", \"petitioner\", \"respondent\", \"caseOrigin\", \"caseSource\", \"lcDisposition\", \"issueArea\"]\n",
    "#add train columns to smalldf\n",
    "smalldf = bigdf[train_vars]\n",
    "\n",
    "#drop row if any values are NAN - maxmimum  22% of original data \n",
    "smalldf=smalldf.dropna(axis=0,how='any')\n",
    "\n",
    "print smalldf.shape\n",
    "\n",
    "# smalldf, _ = encode_variable(smalldf,'chief')\n",
    "# smalldf, _ = encode_variable(smalldf, 'dateDecision')\n",
    "\n",
    "# smalldf.majority refers to whether justice voted with the majority (1 for dissent, 2 for majority)\n",
    "# smalldf.partyWinning indicates winning party (0 for responding party, 1 for petitioning party, 2 for unclear)\n",
    "# We use the above 2 features to infer which party the individual justice voted for\n",
    "# NOTE: majority has around 4000 NaNs that we filtered out\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, x in smalldf.iterrows(): \n",
    "    if x.partyWinning == 2:\n",
    "        results.append(2)\n",
    "        \n",
    "    if x.partyWinning == 1 and x.majority == 2: \n",
    "        results.append(1)\n",
    "        \n",
    "    if x.partyWinning == 0 and x.majority == 2: \n",
    "        results.append(0)\n",
    "        \n",
    "    if x.partyWinning == 1 and x.majority == 1: \n",
    "        results.append(0)\n",
    "        \n",
    "    if x.partyWinning == 0 and x.majority == 1:\n",
    "        results.append(1)\n",
    "\n",
    "smalldf['justiceVote'] = results\n",
    "smalldf['is_train'] = np.random.uniform(0, 1, len(smalldf)) <= .75\n",
    "train, test = smalldf[smalldf['is_train']==True], smalldf[smalldf['is_train']==False]\n",
    "features = list(smalldf.columns[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from homework!\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val,  n_folds=5):\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.2f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.2f\" % (test_accuracy)\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a classifier using LinearSVC, implementing a linear SVM. Since our data is fairly large, a kernelized SVM training time will be too long. Linear SVM is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.386957075532\n",
      "CPU times: user 12.3 s, sys: 142 ms, total: 12.5 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.LinearSVC(loss=\"hinge\")\n",
    "svc_classifier = svc.fit(train[features], train['justiceVote'])\n",
    "print svc_classifier.score(test[features], test['justiceVote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST {'C': 0.1} 0.614182565058 [mean: 0.52284, std: 0.11188, params: {'C': 0.001}, mean: 0.56851, std: 0.09135, params: {'C': 0.01}, mean: 0.61418, std: 0.00001, params: {'C': 0.1}, mean: 0.52284, std: 0.11187, params: {'C': 1.0}, mean: 0.47716, std: 0.11188, params: {'C': 10.0}, mean: 0.52284, std: 0.11187, params: {'C': 100.0}]\n"
     ]
    }
   ],
   "source": [
    "Cs=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "gs=GridSearchCV(svc, param_grid={'C':Cs}, cv=5)\n",
    "gs.fit(train[features], train['justiceVote'])\n",
    "print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61304292446822062"
      ]
     },
     "execution_count": 1109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = gs.best_estimator_\n",
    "best.fit(train[features], train['justiceVote'])\n",
    "best.score(test[features], test['justiceVote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linearsvm returned decent results at 61.2 percent accuracy on the test set. This result is decent as it is better than a random coin toss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Using Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an intuitive understanding of the meanings behind the variables is important and leads us to our idea of usign the decision tree classifier. A distinct advantage of using decisiontrees is that the decision at each node has an intuitive meaning and corresponds to querying along one feature axis at a time (e.g. is the petitioner an attorney general of the United States?). \n",
    "\n",
    "Furthermore, trees are easy to understand and interpret. We can look at the top node and figure out which feature it corresponds to, and conclude that this feature contributes the most information gain, i.e. is the most important/predictive feature. This makes it easy to verify whether our results make intuitive sense.\n",
    "\n",
    "We will show the process of running decision trees on each justice, before aggregating the votes now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our assumptions rely on the idea that certain classifications will have consistencies when appearing in front of the Supreme Court. We can visualize this with a decision tree, which makes buckets and provides instructions. This analysis helps us understand how the variables our working on a more micro level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75429881543752386"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=99)\n",
    "dt.fit(train[features],train[\"justiceVote\"])\n",
    "visualize_tree(dt, features)\n",
    "dt.score(test[features],test['justiceVote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Analysis ** \n",
    "\n",
    "The decision tree yields great results with 75% accuracy. Our understanding of using classification in this manner has turned out to be fruitful. This makes sense as each variable's foundation comes from a legal structure and some cases from certain courts are more likely to return a specific decision. (Note that these is a png created with the decision tree and photoshop is the preferred program to open it as it is big.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With great results from the decision tree, we want to explore a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75913896318940266"
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfrf = RandomForestClassifier(n_estimators=100, min_samples_split=2)\n",
    "clfrf = clfrf.fit(train[features], train['justiceVote'])\n",
    "clfrf.score(test[features], test['justiceVote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the gains we made in the predictions was not that significant. This makes sense as the variables were selected so the decisions tree that underly the random forests should not change greatly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "              \"min_samples_split\": [2, 10, 20],\n",
    "              \"max_depth\": [None, 2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 5, 10],\n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20],\n",
    "              }\n",
    "\n",
    "clfsvm, Xtrain, ytrain, Xtest, ytest = do_classify(DecisionTreeClassifier(random_state=99), param_grid, \n",
    "                                                   smalldf,train_vars[:8], 'justiceVote',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would optimize for parameterization as per the code above but due to time constraints we had to abandon this optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to website :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
