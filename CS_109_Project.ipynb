{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109 Project - The Court Rules In Favor Of...\n",
    "## Aidi Adnan Brian John (Team AABJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "The purpose of this project is to predict votes of Supreme Court justices using oral argument transcripts. Studies in linguistics and psychology, as well as common sense, dictates that the word choices that people make convey crucial information about their beliefs and intentions with regard to issues. Rather than use precedents or formal analysis of the law to predict Supreme Court decisions, we attempt to extract essential emotional features of oral arguments made by justices and advocates in the court. Using aggregate data from 1946 to present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Oral Argument Transcripts - obtained from http://www.supremecourt.gov/oral_arguments/argument_transcript.aspx. Transcripts are made available on the day of court hearing.\n",
    "Justice Vote Counts/Case Information - obtained from the Supreme Court Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Brian/Adnan can you fill this in with a description of what you did in parser.py/convert.py\n",
    "First, we took the all the PDF files of ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reads in text file, replace path of \"wut.txt\" to relevant txt; only processes one text file currently\n",
    "text_file = open(\"wut.txt\", \"r\")\n",
    "text = text_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a parser to extract the names of the petitioner and respondant attorneys from the first 2 pages of the converted text document. An example of list of petitioner and respondant speakers, taken from the example case in 2014 of Johnson v United States (docket number 13-7120) which shall be henceforth used as the recurring example in this process book, is:\n",
    "\n",
    "Katherine M. Menendez, ESQ., Minneapolis, Minn.; on behalf of Petitioner\n",
    "Michael R. Dreeben, ESQ., Deputy Solicitor General, Department of Justice, Washington D.C.; on behalf of Respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_petitioners_and_respondents(text):\n",
    "    '''\n",
    "    This function takes in input text file as string and outputs 2 lists of speakers speaking for petitioners and\n",
    "    respondents sides.\n",
    "    '''\n",
    "    #get portion of transcript between APPEARANCES and CONTENTS that specifies speakers for petitioners/respondents\n",
    "    start = text.find('APPEARANCES:') + len('APPEARNACES')\n",
    "    end = text.find('C O N T E N T S')\n",
    "    speakers_text = text[start:end]\n",
    "    split_speakers_text = re.split('\\.[ ]*\\n', speakers_text)\n",
    "    #for each speaker, get name (capitalized) and side (Pet/Res) he/she is speaking for\n",
    "    pet_speakers, res_speakers, other_speakers = [], [], []\n",
    "    for speaker in split_speakers_text:\n",
    "        name = speaker.strip().split(',')[0]\n",
    "        #search for first index of capitalized word (which will be start of speaker name)\n",
    "        start = 0\n",
    "        for idx, char in enumerate(name):\n",
    "            if str.isupper(char):\n",
    "                start = idx\n",
    "                break\n",
    "        #actual name to be appended to correct list\n",
    "        name = name[start:]\n",
    "        #print name\n",
    "        \n",
    "        #if words Petition, Plaintiff, etc occur in speaker blurb, speaker belongs to Pet\n",
    "        if any(x in speaker for x in ['etition' , 'ppellant', 'emand', 'evers', 'laintiff']):\n",
    "            pet_speakers.append(name)\n",
    "        #otherwise if words Respondent, Defendant, etc occur, speaker belongs to Res\n",
    "        elif any(x in speaker for x in ['espond' , 'ppellee', 'efendant']):\n",
    "            res_speakers.append(name)\n",
    "        #otherwise if neither side is specified in blurb, speaking belongs to Other\n",
    "        elif 'neither' in speaker:\n",
    "            other_speakers.append(name)\n",
    "    return pet_speakers, res_speakers, other_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MR. H. BARTOW FARR'],\n",
       " ['MR. ROY L. REARDON', 'MS. BARBARA D. UNDERWOOD'],\n",
       " [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, for wut.txt, there's 1 petitioner and 2 respondents\n",
    "pet_speakers, res_speakers, other_speakers = get_petitioners_and_respondents(text)\n",
    "pet_speakers, res_speakers, other_speakers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general flow of court proceedings is that the Petitioner attornies make their oral argument, followed by the Respondent attornies, before we hear the rebuttal argument of the Petitioners again. Throughout all proceedings, Justices are free to interject with questions and statements of their own. The below function extracts the main argument portion of the oral transcripts, which is the meat of the proceedings that we are interested in conducting analysis on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_argument_portion(text):\n",
    "    '''\n",
    "    This function gets just the argument portion of the text.\n",
    "    '''\n",
    "    #start and end defines bounds of argument portion of text\n",
    "    start = text.find('P R O C E E D')\n",
    "    end = text.rfind('Whereupon')\n",
    "    return text[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P R O C E E D I N G S\\n\\n2\\n\\n[10:13 a.m.]\\n\\n3\\n4\\n\\nCHIEF JUSTICE REHNQUIST:\\n\\nWe'll hear argument on\\n\\nNumber 00-24, PGA Tour, Inc. vs. Casey Martin.\\n\\n5\\n\\nORAL ARGUMENT OF H. BARTOW FARR, III\\n\\n6\\n\\nON BEHALF OF THE PETITIONER\\n\\n7\\n\\nMR. FARR:\\n\\nMr. Farr?\\n\\nMr. Chief Justice and may it please\\n\\n8\\n\\nthe Court:\\n\\nThe Ninth Circuit in our view made two\\n\\n9\\n\\ncritical mistakes in applying the Disabilities Act to this\\n\\n10\\n\\ntype of claim by a professional athlete. First it failed\\n\\n11\\n\\nto recognize that Title 3 of the act, \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argument_portion = get_argument_portion(text)\n",
    "argument_portion[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(s):\n",
    "    '''\n",
    "    This function counts number of proper English words in a string s (not non-words like - or --)\n",
    "    '''\n",
    "    s = s.split()\n",
    "    non_words = ['-', '--']\n",
    "    return sum([x not in non_words for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_speaker_names(speakers):\n",
    "    '''\n",
    "    This function modifies speaker names like 'QUESTION' to 'QUESTION: ', for word count parsing later on\n",
    "    '''\n",
    "    return map(lambda x: x+': ', speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    This function takes in the portions of text, and gets rid of the \\n and the line numbers. \n",
    "    '''\n",
    "    text_arr=text.splitlines()\n",
    "    text_arr.remove('')\n",
    "    text_clean=[]\n",
    "    for each in text_arr:\n",
    "        if each != '':\n",
    "            try:\n",
    "                int(each)\n",
    "            except ValueError: #assummption: if the item only has integers, it is a line number.\n",
    "                text_clean.append(each)\n",
    "    out_text=' '.join(text_clean)\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P R O C E E D I N G S [10:13 a.m.] CHIEF JUSTICE REHNQUIST: We'll hear argument on Number 00-24, PGA Tour, Inc. vs. Casey Martin. ORAL ARGUMENT OF H. BARTOW FARR, III ON BEHALF OF THE PETITIONER MR. FARR: Mr. Farr? Mr. Chief Justice and may it please the Court: The Ninth Circuit in our view made two critical mistakes in applying the Disabilities Act to this type of claim by a professional athlete. First it failed to recognize that Title 3 of the act, the public accommodations provision, apply on\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_argument=clean_text(argument_portion)\n",
    "clean_argument[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_wordcount(text):\n",
    "    '''\n",
    "    POSSIBLE FEATURE 1:\n",
    "    This function returns a dictionary with key: name of speaker/justice and value: total number of words they\n",
    "    spoke in total throughout argument.\n",
    "    '''\n",
    "    arg_text = get_argument_portion(text)\n",
    "    #keeps track of current speaker\n",
    "    current_speaker = 'N/A'\n",
    "    clean_argument = clean_text(arg_text)\n",
    "    \n",
    "    #clean argument text split by instances where speakers change\n",
    "    #TODO: cleanup - these should not be hardcorded and instead be result of \n",
    "    #modify_speaker_names(pet_speakers + res_speakers + other_speakers)!!!\n",
    "    #this is currently kept this way cuz of QUESTION: ..........ugh\n",
    "    split_argument = re.split('(MR. FARR: |QUESTION: |MR. REARDON: |CHIEF JUSTICE REHNQUIST: )', clean_argument)\n",
    "    all_speakers = ['MR. FARR: ', 'QUESTION: ', 'MR. REARDON: ', 'CHIEF JUSTICE REHNQUIST: ']\n",
    "    \n",
    "    #num_words is a dictionary that maps all speaker names to number of words they spoke\n",
    "    num_words = dict(zip(all_speakers + [current_speaker], [0] * (len(all_speakers)+1)))\n",
    "    \n",
    "    #iterate through split argument, accumulating word counts for all speakers\n",
    "    for s in split_argument:\n",
    "        #if split chunk signifies change in speaker\n",
    "        if s in all_speakers:\n",
    "            current_speaker = s\n",
    "        #if split chunk is part of speech of current speaker, append to word count\n",
    "        else:\n",
    "            num_words[current_speaker] = num_words[current_speaker] + count_words(s)\n",
    "    \n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHIEF JUSTICE REHNQUIST: ': 24,\n",
       " 'MR. FARR: ': 3433,\n",
       " 'MR. REARDON: ': 1480,\n",
       " 'N/A': 13,\n",
       " 'QUESTION: ': 5170}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for example, this gives us total number of words uttered by each speaker\n",
    "#we just need to find list of all speakers in the form they're referred to in the argument, \"JUSTICE SCALIA: \" for ex.\n",
    "total_wordcount(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a natural first choice for a model since our target value can be viewed as a probability between 0 or 1 for any individual justice to vote For or Against, with a higher probability representing a higher confidence of that justice voting in favor of the arguing party. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(penalty='l2',C=1.0, fit_intercept=True, class_weight='auto')\n",
    "log_model = LR.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in SCDB data from file\n",
    "bigdf=pd.read_csv(\"supremeCourtDb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'caseId', u'docketId', u'caseIssuesId', u'voteId', u'dateDecision',\n",
       "       u'decisionType', u'usCite', u'sctCite', u'ledCite', u'lexisCite',\n",
       "       u'term', u'naturalCourt', u'chief', u'docket', u'caseName',\n",
       "       u'dateArgument', u'dateRearg', u'petitioner', u'petitionerState',\n",
       "       u'respondent', u'respondentState', u'jurisdiction', u'adminAction',\n",
       "       u'adminActionState', u'threeJudgeFdc', u'caseOrigin',\n",
       "       u'caseOriginState', u'caseSource', u'caseSourceState',\n",
       "       u'lcDisagreement', u'certReason', u'lcDisposition',\n",
       "       u'lcDispositionDirection', u'declarationUncon', u'caseDisposition',\n",
       "       u'caseDispositionUnusual', u'partyWinning', u'precedentAlteration',\n",
       "       u'voteUnclear', u'issue', u'issueArea', u'decisionDirection',\n",
       "       u'decisionDirectionDissent', u'authorityDecision1',\n",
       "       u'authorityDecision2', u'lawType', u'lawSupp', u'lawMinor',\n",
       "       u'majOpinWriter', u'majOpinAssigner', u'splitVote', u'majVotes',\n",
       "       u'minVotes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = bigdf[\"docketId\", \"dateDecision\", \"case\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C=1.0, kernel='linear', probability=True, class_weight='auto')\n",
    "svm_model = my_svm.fit(X, y)\n",
    "svm_pred = svm_fit.predict(W)\n",
    "# Class probabilities, based on log regression on distance to hyperplane.\n",
    "svm_prob = svm_fit.predict_proba(W)\n",
    "svm_dist = svm_fit.decision_function(W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
